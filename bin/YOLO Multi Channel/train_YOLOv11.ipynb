{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>YOLO11 Training with Privileged Information</h1>\n",
    "<h2>Matthias Bartolo</h2>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Package Imports</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aFbKjvakDnZE",
    "outputId": "dbe28130-1112-4141-e265-65d3f6b06acc"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade roboflow ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rasterio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWZ2DvvEDnZF"
   },
   "source": [
    "**<h3>Required libraries.</h3>**\n",
    "https://github.com/ultralytics/ultralytics/issues/2154\n",
    "\n",
    "Steps to Change the code:\n",
    "1. Generate the other images (privileged information) and save them in tiff format.\n",
    "2. Download yolo11 yaml and add to it `ch: 4 # number of channels in the input images`\n",
    "3. Download ultralytics library and perform the following changes:\n",
    "4. Code Change 1: tasks.py: create a variable `ch= 4` and from there search for `3,` and where necessary change it to `ch,`\n",
    "5. Code Change 2: exporter.py: create a variable `ch= 4` and from there search for `3,` and where necessary change it to `ch,`\n",
    "6. Code Change 3: validator.py: create a variable `ch= 4` and from there search for `3,` and where necessary change it to `ch,`\n",
    "7. Code Change 4: predictor.py: create a variable `ch= 4` and from there search for `3,` and where necessary change it to `ch,`\n",
    "8. Code Change 5: checks.py: modify the `amp_allcloseamp_allclose` function and comment the check which utilises the `jpg` image\n",
    "9. Code Change 6: base.py: Change all the image loading functions to load the tiff images through the `rasterio` library (search for `cv2.imread` and replace it with `rasterio.open`)\n",
    "10. Code Change 7: base.py: Change the `cv2.imwrite` function to save the tiff images through the `rasterio` library (search for `cv2.imwrite` and replace it with `rasterio.open`)\n",
    "11. Code Change 8: dataset.py: Change the `cv2.imwrite` function to save the tiff images through the `rasterio` library (search for `cv2.imwrite` and replace it with `rasterio.open`)\n",
    "12. Code Change 9: plotting.py: Change the plotting functions to plot the rgb images, therefore you need to filter the extra channels from the loaded tiff images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9Zbf3zFkDnZG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "# import ultralytics\n",
    "import locale\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image\n",
    "from roboflow import Roboflow\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyRdDYkqAKN4"
   },
   "source": [
    "**<h3>Using GPU if one is available.</h3>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 28 09:23:15 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 566.36                 Driver Version: 566.36         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070      WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "|  0%   36C    P8              8W /  200W |    1532MiB /  12282MiB |      9%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      5348    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A      7812    C+G   ...Google\\NearbyShare\\nearby_share.exe      N/A      |\n",
      "|    0   N/A  N/A     10032    C+G   ...5.0_x64__cv1g1gvanyjgm\\WhatsApp.exe      N/A      |\n",
      "|    0   N/A  N/A     10664    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     11876    C+G   ...n\\132.0.2957.127\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     14076    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     16340    C+G   ..._8wekyb3d8bbwe\\Microsoft.Photos.exe      N/A      |\n",
      "|    0   N/A  N/A     16360    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     16844    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     19624    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A     19916    C+G   ...35.0_x64__zpdnekdrzrea0\\Spotify.exe      N/A      |\n",
      "|    0   N/A  N/A     21016    C+G   ...010.0_x64__8wekyb3d8bbwe\\Photos.exe      N/A      |\n",
      "|    0   N/A  N/A     21712    C+G   ...n\\132.0.2957.127\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     22708    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     22940    C+G   ...64__v826wp6bftszj\\TranslucentTB.exe      N/A      |\n",
      "|    0   N/A  N/A     24248    C+G   ...x64__97hta09mmv6hy\\Build\\Lively.exe      N/A      |\n",
      "|    0   N/A  N/A     29016    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A     29028    C+G   ...n\\NVIDIA app\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A     29340    C+G   ...1.0_x64__8wekyb3d8bbwe\\Video.UI.exe      N/A      |\n",
      "|    0   N/A  N/A     29724    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A     30628    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     31476    C+G   ...n\\NVIDIA app\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A     33452    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     34864    C+G   ...a09mmv6hy\\Build\\Plugins\\Mpv\\mpv.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dgNdkO48DnZG",
    "outputId": "11ca77d8-7b09-4d02-a899-0dfdc5aaac2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CjpPg4mGKc1v",
    "outputId": "7035eeee-b2d1-438c-bb57-188bd34ea8eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\Testing\n"
     ]
    }
   ],
   "source": [
    "# Retrieving the current working directory\n",
    "HOME = os.getcwd()\n",
    "print(HOME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3C3EO_2zNChu"
   },
   "source": [
    "**<h3>Downloading the Roboflow dataset.</h3>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BSd93ZJzZZKt",
    "outputId": "cd51037c-4df9-415e-ecd8-540168fb544a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(os.path.join(HOME, 'datasets')):\n",
    "    os.mkdir(os.path.join(HOME, 'datasets'))\n",
    "os.chdir(os.path.join(HOME, 'datasets'))\n",
    "\n",
    "\n",
    "# Zoo Animals Dataset\n",
    "rf = Roboflow(api_key=\"nyynHs3oneLLx01D04rC\")\n",
    "project = rf.workspace(\"soda-dataset\").project(\"01m-all\")\n",
    "version = project.version(1)\n",
    "dataset = version.download(\"yolov11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Training the YOLO11 model.</h3>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Added Code for Custom 4 Channel YOLO Model:\n",
    "# import torch.nn as nn\n",
    "\n",
    "# # Define the custom YOLO model class (Not working)\n",
    "# class CustomYOLO(YOLO):\n",
    "#     def __init__(self, model_path, num_channels):\n",
    "#         \"\"\"\n",
    "#         Initialize the custom YOLO model by adding a custom beginning layer for multi-channel input.\n",
    "        \n",
    "#         Args:\n",
    "#             model_path (str): Path to the pre-trained YOLO model.\n",
    "#             num_channels (int): Number of input channels (e.g., 3 for RGB, 4 for RGB + edge).\n",
    "#         \"\"\"\n",
    "#         # Load the original YOLO model from the given path\n",
    "#         super(CustomYOLO, self).__init__(model_path)\n",
    "        \n",
    "#         # Access the first convolutional layer of the YOLO model\n",
    "#         # self.model.model[0] is the first Conv2d layer (backbone structure)\n",
    "#         # You need to directly assign the custom convolutional layer to this part of the model.\n",
    "#         # Layer is the same as the normal YOLO model, but input has more channels\n",
    "#         self.model.model[0] = nn.Sequential(\n",
    "#             nn.Conv2d(num_channels, 16, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "#             nn.BatchNorm2d(16),\n",
    "#             nn.SiLU(inplace=True)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass through the model.\n",
    "        \n",
    "#         Args:\n",
    "#             x (torch.Tensor): Input tensor with multi-channel data.\n",
    "        \n",
    "#         Returns:\n",
    "#             torch.Tensor: The model output after passing through the custom and YOLO layers.\n",
    "#         \"\"\"\n",
    "#         return super(CustomYOLO, self).forward(x)\n",
    "\n",
    "# # Specify the model path and number of input channels\n",
    "# model_path = 'yolo11n.pt'  # Path to your YOLO model file\n",
    "# num_channels = 4  # Example for multi-channel input (e.g., RGB + edge detection)\n",
    "\n",
    "# # UltraLytics YOLO Model\n",
    "# # from ultralytics.nn.modules.conv import Conv  # Import the Conv layer from YOLO's common module\n",
    "\n",
    "# class CustomYOLO(YOLO):\n",
    "#     def __init__(self, model_path, num_channels):\n",
    "#         \"\"\"\n",
    "#         Initialize the custom YOLO model by adding a custom beginning layer for multi-channel input.\n",
    "        \n",
    "#         Args:\n",
    "#             model_path (str): Path to the pre-trained YOLO model.\n",
    "#             num_channels (int): Number of input channels (e.g., 3 for RGB, 4 for RGB + edge).\n",
    "#         \"\"\"\n",
    "#         # Load the original YOLO model from the given path\n",
    "#         super(CustomYOLO, self).__init__(model_path)\n",
    "        \n",
    "#         # Modify the first convolution to handle multi-channel input (num_channels)\n",
    "#         self.model.model[0].conv = nn.Conv2d(num_channels, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "\n",
    "# # Initialize the custom YOLO model\n",
    "# model = CustomYOLO(model_path, num_channels)\n",
    "\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiSpectralYOLO(nn.Module):\n",
    "#     def __init__(self, model_path, num_channels, num_classes):\n",
    "#         super(MultiSpectralYOLO, self).__init__()\n",
    "#         self.yolo = YOLO(model_path).model.model\n",
    "        \n",
    "#         # Change the first convolutional layer to accept 4-channel input\n",
    "#         self.yolo[0].conv = nn.Conv2d(num_channels, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        \n",
    "        \n",
    "#         # Change the last layer to output the correct number of classes\n",
    "#         # self.yolo[-1].nc = num_classes\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.yolo(x)\n",
    "    \n",
    "# model = MultiSpectralYOLO(model_path, num_channels, 3)\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO(\n",
      "  (model): DetectionModel(\n",
      "    (model): Sequential(\n",
      "      (0): Conv(\n",
      "        (conv): Conv2d(4, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv(\n",
      "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (2): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(8, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): Conv(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (4): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): Conv(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (6): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): C3k(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv3): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (m): Sequential(\n",
      "              (0): Bottleneck(\n",
      "                (cv1): Conv(\n",
      "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "                (cv2): Conv(\n",
      "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "              (1): Bottleneck(\n",
      "                (cv1): Conv(\n",
      "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "                (cv2): Conv(\n",
      "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): Conv(\n",
      "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (8): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): C3k(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv3): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (m): Sequential(\n",
      "              (0): Bottleneck(\n",
      "                (cv1): Conv(\n",
      "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "                (cv2): Conv(\n",
      "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "              (1): Bottleneck(\n",
      "                (cv1): Conv(\n",
      "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "                (cv2): Conv(\n",
      "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): SPPF(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (10): C2PSA(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): Sequential(\n",
      "          (0): PSABlock(\n",
      "            (attn): Attention(\n",
      "              (qkv): Conv(\n",
      "                (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): Identity()\n",
      "              )\n",
      "              (proj): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): Identity()\n",
      "              )\n",
      "              (pe): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): Identity()\n",
      "              )\n",
      "            )\n",
      "            (ffn): Sequential(\n",
      "              (0): Conv(\n",
      "                (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv(\n",
      "                (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): Identity()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): Upsample(scale_factor=2.0, mode='nearest')\n",
      "      (12): Concat()\n",
      "      (13): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): Upsample(scale_factor=2.0, mode='nearest')\n",
      "      (15): Concat()\n",
      "      (16): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (17): Conv(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (18): Concat()\n",
      "      (19): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (20): Conv(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (21): Concat()\n",
      "      (22): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): C3k(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv3): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (m): Sequential(\n",
      "              (0): Bottleneck(\n",
      "                (cv1): Conv(\n",
      "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "                (cv2): Conv(\n",
      "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "              (1): Bottleneck(\n",
      "                (cv1): Conv(\n",
      "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "                (cv2): Conv(\n",
      "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (23): Detect(\n",
      "        (cv2): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (cv3): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): DWConv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv(\n",
      "                (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): DWConv(\n",
      "                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv(\n",
      "                (conv): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): DWConv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv(\n",
      "                (conv): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): DWConv(\n",
      "                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv(\n",
      "                (conv): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): DWConv(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv(\n",
      "                (conv): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): DWConv(\n",
      "                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv(\n",
      "                (conv): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (dfl): DFL(\n",
      "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "model = YOLO(\"4-channel-yolo11n.yaml\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming you have a 3-channel input tensor\n",
    "# sample_input = torch.randn(1, 3, 640, 640)  # RGB (3 channels)\n",
    "# sample_input = torch.cat([sample_input, torch.zeros(1, 1, 640, 640)], dim=1)  # Add an extra channel\n",
    "# sample_input/=255\n",
    "\n",
    "# print(sample_input.shape)\n",
    "# output = model(sample_input, save=True)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_WRhnBXjDnZH",
    "outputId": "09bcb11a-5656-4c53-8564-51b29cc08d6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.68  Python-3.9.19 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4070, 12282MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=4-channel-yolo11n.yaml, data=e:\\Testing\\datasets\\01m-All-1/data.yaml, epochs=100, time=None, patience=15, batch=16, imgsz=640, save=True, save_period=-1, cache=True, device=0, workers=8, project=None, name=train6, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train6\n",
      "Overriding model.yaml nc=80 with nc=6\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       608  ultralytics.nn.modules.conv.Conv             [4, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    431842  ultralytics.nn.modules.head.Detect           [6, [64, 128, 256]]           \n",
      "4-channel-YOLO11n summary: 319 layers, 2,591,154 parameters, 2,591,138 gradients, 6.5 GFLOPs\n",
      "\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning E:\\Testing\\datasets\\01m-All-1\\train\\labels.cache... 316 images, 0 backgrounds, 0 corrupt: 100%|| 316/316 [00:00<?, ?it/s]\n",
      "Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.4GB RAM): 100%|| 316/316 [00:04<00:00, 71.51it/s] \n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Testing\\datasets\\01m-All-1\\valid\\labels.cache... 46 images, 0 backgrounds, 0 corrupt: 100%|| 46/46 [00:00<?, ?it/s]\n",
      "Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/46 [00:00<?, ?it/s]Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB RAM): 100%|| 46/46 [00:00<00:00, 76.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train6\\labels.jpg... \n",
      "'Image' object is not subscriptable\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train6\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      1/100       2.5G      3.989      5.387      4.187         36        640: 100%|| 20/20 [00:03<00:00,  5.48it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  6.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      2/100      2.51G      3.046      3.334      3.056         52        640: 100%|| 20/20 [00:02<00:00,  8.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 13.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      3/100      2.52G      2.698      2.871      2.783         34        640: 100%|| 20/20 [00:02<00:00,  8.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      4/100      2.54G      2.589      2.588       2.69         40        640: 100%|| 20/20 [00:02<00:00,  8.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.028     0.0485     0.0439     0.0209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      5/100      2.54G      2.417      2.373      2.524         37        640: 100%|| 20/20 [00:02<00:00,  8.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84    0.00529     0.0661    0.00299    0.00166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      6/100      2.54G      2.352      2.371      2.462         23        640: 100%|| 20/20 [00:02<00:00,  8.52it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  4.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84   5.44e-05     0.0104   2.97e-05   2.97e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      7/100      2.54G      2.304      2.268       2.44         35        640: 100%|| 20/20 [00:02<00:00,  8.55it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84     0.0605     0.0854     0.0185    0.00528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      8/100      2.51G      2.233      2.258      2.372         20        640: 100%|| 20/20 [00:02<00:00,  8.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  8.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84     0.0632      0.105     0.0308    0.00859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      9/100      2.53G      2.199      2.117      2.339         44        640: 100%|| 20/20 [00:02<00:00,  8.55it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.434      0.186      0.149     0.0533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     10/100      2.54G      2.157      2.081      2.263         29        640: 100%|| 20/20 [00:02<00:00,  8.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.607     0.0952      0.114     0.0353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     11/100      2.54G      2.053      1.927      2.218         37        640: 100%|| 20/20 [00:02<00:00,  8.68it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  9.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.333      0.222      0.123      0.055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     12/100      2.51G      2.024      1.941      2.158         35        640: 100%|| 20/20 [00:02<00:00,  8.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  9.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.902       0.12      0.138      0.058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     13/100      2.54G      1.955      1.834      2.102         34        640: 100%|| 20/20 [00:02<00:00,  8.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  8.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.608      0.233       0.23      0.103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     14/100      2.54G      1.944      1.857      2.123         39        640: 100%|| 20/20 [00:02<00:00,  8.66it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  9.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.569      0.201      0.239     0.0966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     15/100      2.54G      1.867       1.77      2.105         43        640: 100%|| 20/20 [00:02<00:00,  8.75it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  9.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.561      0.248      0.243      0.114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     16/100      2.53G      1.833      1.672      2.026         38        640: 100%|| 20/20 [00:02<00:00,  8.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  7.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.484      0.154      0.102     0.0441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     17/100      2.51G      1.809      1.634      2.012         38        640: 100%|| 20/20 [00:02<00:00,  8.49it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84       0.16       0.27      0.173     0.0966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     18/100      2.51G      1.861      1.711      2.074         40        640: 100%|| 20/20 [00:02<00:00,  8.39it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.106        0.3      0.164     0.0749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     19/100      2.54G      1.782      1.653      1.967         36        640: 100%|| 20/20 [00:02<00:00,  8.51it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  9.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.587      0.254      0.244      0.124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     20/100      2.54G      1.721      1.552      1.897         30        640: 100%|| 20/20 [00:02<00:00,  8.53it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84       0.65      0.204      0.236      0.119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     21/100      2.53G      1.724       1.51      1.889         38        640: 100%|| 20/20 [00:02<00:00,  8.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.535       0.32      0.304      0.159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     22/100      2.51G       1.71      1.586      1.934         46        640: 100%|| 20/20 [00:02<00:00,  8.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.413      0.392      0.333      0.172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     23/100      2.54G      1.692      1.552      1.906         47        640: 100%|| 20/20 [00:02<00:00,  8.74it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.182      0.465      0.319      0.142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     24/100      2.54G      1.711      1.497      1.882         28        640: 100%|| 20/20 [00:02<00:00,  8.56it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  9.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.637      0.257      0.318      0.159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     25/100      2.54G      1.699      1.568      1.916         40        640: 100%|| 20/20 [00:02<00:00,  8.82it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.489      0.352      0.319      0.158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     26/100      2.51G      1.616      1.468      1.797         39        640: 100%|| 20/20 [00:02<00:00,  8.67it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  9.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.293      0.214      0.129     0.0705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     27/100      2.53G      1.647      1.481      1.891         36        640: 100%|| 20/20 [00:02<00:00,  8.50it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  9.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.357      0.374      0.331       0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     28/100      2.54G      1.615      1.396      1.851         46        640: 100%|| 20/20 [00:02<00:00,  8.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.633      0.359      0.372        0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     29/100      2.51G      1.543      1.419      1.831         46        640: 100%|| 20/20 [00:02<00:00,  8.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.388      0.396      0.353      0.189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     30/100      2.51G      1.576      1.386      1.796         46        640: 100%|| 20/20 [00:02<00:00,  8.53it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.681      0.337      0.389      0.207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     31/100      2.54G      1.535      1.336      1.746         34        640: 100%|| 20/20 [00:02<00:00,  8.56it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  9.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.574      0.381      0.345      0.184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     32/100      2.54G      1.565      1.351      1.822         34        640: 100%|| 20/20 [00:02<00:00,  8.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  9.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.515      0.301      0.385      0.221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     33/100      2.53G      1.519      1.321      1.774         51        640: 100%|| 20/20 [00:02<00:00,  8.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  9.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.379       0.48      0.376      0.193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     34/100      2.51G      1.505      1.272      1.737         45        640: 100%|| 20/20 [00:02<00:00,  8.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.492      0.405      0.336      0.192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     35/100      2.53G      1.496      1.264      1.715         37        640: 100%|| 20/20 [00:02<00:00,  8.62it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.633      0.299      0.323      0.172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     36/100      2.53G      1.432      1.222      1.683         26        640: 100%|| 20/20 [00:02<00:00,  8.68it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.383      0.464      0.409       0.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     37/100      2.51G      1.426      1.199      1.704         36        640: 100%|| 20/20 [00:02<00:00,  8.78it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.562      0.373      0.427      0.238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     38/100      2.54G      1.455      1.275      1.672         35        640: 100%|| 20/20 [00:02<00:00,  8.73it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.585      0.459      0.432      0.244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     39/100      2.53G      1.422      1.189      1.659         45        640: 100%|| 20/20 [00:02<00:00,  8.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.546      0.456      0.446      0.252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     40/100      2.51G      1.474      1.213      1.701         29        640: 100%|| 20/20 [00:02<00:00,  8.77it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84       0.38      0.466      0.405      0.225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     41/100      2.51G      1.438      1.161      1.694         29        640: 100%|| 20/20 [00:02<00:00,  8.84it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.553      0.398      0.434      0.267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     42/100      2.51G       1.41      1.163      1.651         27        640: 100%|| 20/20 [00:02<00:00,  8.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.369      0.426      0.339      0.213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     43/100      2.52G      1.418      1.152      1.707         33        640: 100%|| 20/20 [00:02<00:00,  8.82it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.445      0.516      0.457      0.277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     44/100      2.53G      1.354      1.118      1.612         38        640: 100%|| 20/20 [00:02<00:00,  8.87it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.407      0.418      0.401       0.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     45/100      2.51G      1.357      1.171      1.632         39        640: 100%|| 20/20 [00:02<00:00,  8.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84       0.35      0.478      0.385      0.244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     46/100      2.51G      1.364      1.146      1.657         40        640: 100%|| 20/20 [00:02<00:00,  8.84it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  9.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.405      0.546      0.438      0.252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     47/100      2.54G      1.363      1.087      1.632         27        640: 100%|| 20/20 [00:02<00:00,  8.90it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.604      0.499      0.475      0.275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     48/100      2.51G       1.37      1.122      1.615         44        640: 100%|| 20/20 [00:02<00:00,  8.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.636       0.37      0.423      0.254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     49/100      2.54G      1.365      1.082      1.629         35        640: 100%|| 20/20 [00:02<00:00,  8.77it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.405      0.449      0.412      0.235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     50/100      2.53G      1.312      1.101      1.584         40        640: 100%|| 20/20 [00:02<00:00,  8.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.482      0.392      0.385      0.228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     51/100      2.54G      1.279      1.067      1.578         31        640: 100%|| 20/20 [00:02<00:00,  8.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.436      0.322      0.278      0.173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     52/100      2.51G      1.328      1.091      1.597         42        640: 100%|| 20/20 [00:02<00:00,  8.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  9.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.393      0.379      0.394      0.213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     53/100      2.54G      1.352       1.07      1.617         47        640: 100%|| 20/20 [00:02<00:00,  8.85it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.476       0.44      0.439       0.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     54/100      2.53G      1.341      1.038      1.582         33        640: 100%|| 20/20 [00:02<00:00,  8.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.368      0.451      0.421       0.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     55/100      2.54G      1.338      1.065      1.577         40        640: 100%|| 20/20 [00:02<00:00,  8.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  9.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.433      0.466      0.424      0.251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     56/100      2.53G      1.319      1.095      1.618         44        640: 100%|| 20/20 [00:02<00:00,  8.52it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  8.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84       0.51      0.377      0.417      0.271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     57/100      2.53G      1.264     0.9888      1.549         34        640: 100%|| 20/20 [00:02<00:00,  8.59it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  8.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84       0.39      0.486      0.428      0.254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     58/100      2.51G      1.283      1.005      1.564         29        640: 100%|| 20/20 [00:02<00:00,  8.28it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  8.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.636      0.417      0.482      0.299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     59/100      2.54G      1.328      1.041       1.57         38        640: 100%|| 20/20 [00:02<00:00,  8.40it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  9.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.535       0.46      0.475      0.318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     60/100      2.54G      1.279      1.028      1.576         26        640: 100%|| 20/20 [00:02<00:00,  8.74it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.536      0.451      0.476       0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     61/100      2.51G      1.274     0.9926      1.555         41        640: 100%|| 20/20 [00:02<00:00,  8.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.426      0.488      0.457      0.282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     62/100      2.51G       1.28     0.9797       1.54         35        640: 100%|| 20/20 [00:02<00:00,  8.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.512      0.517      0.433      0.276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     63/100      2.51G      1.237     0.9661      1.514         44        640: 100%|| 20/20 [00:02<00:00,  8.70it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.579      0.426      0.471      0.301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     64/100      2.52G       1.27      1.009      1.555         24        640: 100%|| 20/20 [00:02<00:00,  8.50it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.582      0.468      0.473      0.307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     65/100      2.51G      1.266     0.9549      1.543         38        640: 100%|| 20/20 [00:02<00:00,  8.83it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.597      0.452      0.476      0.309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     66/100      2.53G      1.248     0.9545      1.478         34        640: 100%|| 20/20 [00:02<00:00,  8.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.639      0.465        0.5      0.311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     67/100      2.54G      1.186     0.9309      1.474         59        640: 100%|| 20/20 [00:02<00:00,  8.82it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.662      0.483      0.463      0.295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     68/100      2.53G      1.173     0.9095      1.454         32        640: 100%|| 20/20 [00:02<00:00,  8.90it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.522      0.523      0.479      0.291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     69/100      2.53G      1.159     0.9012      1.485         39        640: 100%|| 20/20 [00:02<00:00,  8.73it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.528      0.507      0.505       0.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     70/100      2.53G      1.179     0.9248      1.494         43        640: 100%|| 20/20 [00:02<00:00,  8.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.542      0.503      0.521      0.324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     71/100      2.53G      1.144     0.9056      1.483         31        640: 100%|| 20/20 [00:02<00:00,  8.48it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  9.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.696      0.502      0.545      0.347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     72/100      2.53G       1.14     0.9155      1.465         44        640: 100%|| 20/20 [00:02<00:00,  8.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  8.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.595      0.524      0.506      0.305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     73/100      2.53G      1.137     0.8869      1.474         49        640: 100%|| 20/20 [00:02<00:00,  8.40it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  8.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.658      0.444      0.545      0.339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     74/100      2.54G      1.177      0.901      1.491         40        640: 100%|| 20/20 [00:02<00:00,  8.55it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84       0.54      0.496       0.48      0.315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     75/100      2.53G       1.12     0.8645      1.448         32        640: 100%|| 20/20 [00:02<00:00,  8.53it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.585      0.557      0.567      0.361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     76/100      2.51G      1.108      0.845      1.442         35        640: 100%|| 20/20 [00:02<00:00,  8.92it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.688      0.435      0.491      0.341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     77/100      2.53G      1.118     0.8462      1.413         47        640: 100%|| 20/20 [00:02<00:00,  8.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.636      0.551      0.561      0.356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     78/100      2.54G      1.097     0.8418       1.41         46        640: 100%|| 20/20 [00:02<00:00,  8.78it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.491        0.5      0.482      0.321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     79/100      2.54G      1.082     0.8471      1.389         38        640: 100%|| 20/20 [00:02<00:00,  8.73it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  9.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.634      0.513      0.532      0.337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     80/100      2.53G      1.119     0.8559      1.467         31        640: 100%|| 20/20 [00:02<00:00,  8.39it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  9.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.567      0.511      0.546      0.357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     81/100      2.51G      1.059      0.812      1.374         32        640: 100%|| 20/20 [00:02<00:00,  8.88it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.582      0.556      0.534      0.361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     82/100      2.53G      1.104      0.829      1.385         43        640: 100%|| 20/20 [00:02<00:00,  8.77it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  9.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84       0.58       0.51      0.513       0.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     83/100      2.51G       1.13     0.8514      1.437         30        640: 100%|| 20/20 [00:02<00:00,  8.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.511      0.508      0.513      0.345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     84/100      2.54G      1.051     0.8118      1.381         42        640: 100%|| 20/20 [00:02<00:00,  8.83it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.517      0.587      0.535      0.357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     85/100      2.53G      1.072     0.8076      1.427         37        640: 100%|| 20/20 [00:02<00:00,  8.83it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.698      0.492      0.562      0.378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     86/100      2.51G      1.062     0.8007      1.404         39        640: 100%|| 20/20 [00:02<00:00,  8.73it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84        0.6      0.554      0.546      0.359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     87/100      2.53G      1.025     0.8005       1.35         38        640: 100%|| 20/20 [00:02<00:00,  8.73it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.506      0.515      0.504      0.344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     88/100      2.53G      1.046     0.8164      1.389         43        640: 100%|| 20/20 [00:02<00:00,  8.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.619      0.553      0.552      0.371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     89/100      2.51G      1.042     0.7983      1.369         31        640: 100%|| 20/20 [00:02<00:00,  8.70it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.775      0.478      0.574      0.385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     90/100      2.54G       1.03     0.7719      1.359         36        640: 100%|| 20/20 [00:02<00:00,  8.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.705      0.512      0.557      0.365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     91/100      2.52G      1.336      1.278      1.839         15        640: 100%|| 20/20 [00:02<00:00,  8.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.719      0.402      0.498      0.297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     92/100      2.52G      1.244      1.093      1.705         16        640: 100%|| 20/20 [00:02<00:00,  8.59it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  9.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.523      0.475      0.513      0.319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     93/100       2.5G      1.207      1.073      1.683         18        640: 100%|| 20/20 [00:02<00:00,  8.63it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.542      0.509       0.55      0.329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     94/100      2.52G      1.136      1.003      1.671         21        640: 100%|| 20/20 [00:02<00:00,  8.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  9.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.757      0.475      0.589       0.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     95/100      2.52G       1.12     0.9902      1.619         14        640: 100%|| 20/20 [00:02<00:00,  8.75it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  9.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.727      0.434      0.544      0.331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     96/100      2.52G      1.121     0.9495       1.62         15        640: 100%|| 20/20 [00:02<00:00,  8.59it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  9.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84       0.52      0.518      0.538      0.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     97/100      2.52G      1.114     0.9609      1.604         20        640: 100%|| 20/20 [00:02<00:00,  8.68it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  9.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.624      0.563      0.563      0.352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     98/100      2.52G      1.112     0.9552      1.592         22        640: 100%|| 20/20 [00:02<00:00,  8.64it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.542      0.548      0.528      0.351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     99/100       2.5G      1.085     0.9282      1.587         20        640: 100%|| 20/20 [00:02<00:00,  8.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  8.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.522      0.554      0.528      0.345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    100/100      2.52G      1.052     0.9175      1.564         16        640: 100%|| 20/20 [00:02<00:00,  8.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 10.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.604      0.551      0.546      0.348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100 epochs completed in 0.083 hours.\n",
      "Optimizer stripped from runs\\detect\\train6\\weights\\last.pt, 5.5MB\n",
      "Optimizer stripped from runs\\detect\\train6\\weights\\best.pt, 5.5MB\n",
      "\n",
      "Validating runs\\detect\\train6\\weights\\best.pt...\n",
      "Ultralytics 8.3.68  Python-3.9.19 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4070, 12282MiB)\n",
      "4-channel-YOLO11n summary (fused): 238 layers, 2,583,466 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  7.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.777      0.478      0.574      0.387\n",
      "  clear_plastic_bottle         12         16      0.917      0.375      0.661      0.428\n",
      "             drink_can         25         35      0.951      0.886      0.937       0.69\n",
      "          drink_carton          5          6      0.648       0.62       0.59      0.431\n",
      "          glass_bottle         11         13      0.548      0.231      0.381      0.208\n",
      "             glass_jar          5          5      0.596        0.6       0.55      0.371\n",
      "  other_plastic_bottle          5          9          1      0.155      0.327      0.194\n",
      "Speed: 0.2ms preprocess, 3.1ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train6\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([0, 1, 2, 3, 4, 5])\n",
       "box: ultralytics.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x000001CB6CA3CA90>\n",
       "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
       "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,  0.00021192,  0.00010596,           0],\n",
       "       [          1,           1,           1, ...,      0.4023,      0.4023,           0],\n",
       "       [          1,           1,           1, ...,        0.24,        0.24,           0],\n",
       "       [          1,           1,           1, ...,  0.00036451,  0.00018226,           0],\n",
       "       [          1,           1,           1, ...,  0.00013288,  6.6438e-05,           0],\n",
       "       [          1,           1,           1, ...,   0.0005148,   0.0002574,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.038748,    0.038748,     0.11253, ...,           0,           0,           0],\n",
       "       [    0.37838,     0.37838,     0.45263, ...,           0,           0,           0],\n",
       "       [    0.16667,     0.16667,     0.19345, ...,           0,           0,           0],\n",
       "       [   0.079681,    0.079681,      0.1117, ...,           0,           0,           0],\n",
       "       [   0.050847,    0.050847,    0.065971, ...,           0,           0,           0],\n",
       "       [     0.1519,      0.1519,     0.18789, ...,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.019847,    0.019847,    0.060452, ...,           1,           1,           1],\n",
       "       [    0.23333,     0.23333,     0.29252, ...,           1,           1,           1],\n",
       "       [   0.090909,    0.090909,     0.10708, ...,           1,           1,           1],\n",
       "       [   0.042017,    0.042017,    0.060224, ...,           1,           1,           1],\n",
       "       [   0.026549,    0.026549,    0.034904, ...,           1,           1,           1],\n",
       "       [   0.085714,    0.085714,     0.11306, ...,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[     0.8125,      0.8125,      0.8125, ...,           0,           0,           0],\n",
       "       [          1,           1,           1, ...,           0,           0,           0],\n",
       "       [          1,           1,           1, ...,           0,           0,           0],\n",
       "       [    0.76923,     0.76923,     0.76923, ...,           0,           0,           0],\n",
       "       [        0.6,         0.6,         0.6, ...,           0,           0,           0],\n",
       "       [    0.66667,     0.66667,     0.55556, ...,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
       "fitness: np.float64(0.4055766453791211)\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
       "maps: array([    0.42757,     0.68996,     0.43053,     0.20835,     0.37062,     0.19392])\n",
       "names: {0: 'clear_plastic_bottle', 1: 'drink_can', 2: 'drink_carton', 3: 'glass_bottle', 4: 'glass_jar', 5: 'other_plastic_bottle'}\n",
       "plot: True\n",
       "results_dict: {'metrics/precision(B)': np.float64(0.7766727371965422), 'metrics/recall(B)': np.float64(0.47763658413452664), 'metrics/mAP50(B)': np.float64(0.574331132128463), 'metrics/mAP50-95(B)': np.float64(0.38682614685141636), 'fitness': np.float64(0.4055766453791211)}\n",
       "save_dir: WindowsPath('runs/detect/train6')\n",
       "speed: {'preprocess': 0.2199566882589589, 'inference': 3.075620402460513, 'loss': 0.0, 'postprocess': 0.5937814712524414}\n",
       "task: 'detect'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specifying the paths\n",
    "yaml_path  = dataset.location+\"/data.yaml\"\n",
    "\n",
    "# Specifying the model path\n",
    "# model_path = 'yolo11n.pt'\n",
    "\n",
    "# # Creating YOLO object\n",
    "# model = YOLO(model_path)\n",
    "\n",
    "# Specifying training parameters\n",
    "num_epochs = 100  # Number of epochs\n",
    "batch_size = 16 #8 # Adjust based on GPU memory\n",
    "image_size = 640  # Decrease for faster training\n",
    "\n",
    "# Training configuration\n",
    "train_config = {\n",
    "    'data': yaml_path,\n",
    "    'imgsz': image_size,\n",
    "    'batch': batch_size,\n",
    "    'epochs': num_epochs,\n",
    "    'device': 0,  # Use GPU 0\n",
    "    # 'workers': 1,  # Number of data loading workers\n",
    "    'optimizer': 'Adam',  # Use Adam optimizer\n",
    "    'cache': True,  # Cache images for faster training\n",
    "    'patience': 15,  # epochs to wait before decreasing LR\n",
    "    'val': True,  # Run validation during training\n",
    "    'plots': True,  # Run plots during training\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "model.train(**train_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ODk1VTlevxn"
   },
   "source": [
    "**<h3>Validating the YOLO11 model on the Validation subset.</h3>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IkrxsRHoV67H",
    "outputId": "ecb3eac0-3a4f-4df6-fdb0-9db829ad7a76"
   },
   "outputs": [],
   "source": [
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "# !pip install aspose-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YpyuwrNlXc1P",
    "outputId": "f4718557-cd29-4208-d5fa-ad0776a893b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.68  Python-3.9.19 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4070, 12282MiB)\n",
      "4-channel-YOLO11n summary (fused): 238 layers, 2,583,466 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Testing\\datasets\\01m-All-1\\valid\\labels.cache... 46 images, 0 backgrounds, 0 corrupt: 100%|| 46/46 [00:00<?, ?it/s]\n",
      "Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/46 [00:00<?, ?it/s]Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB RAM): 100%|| 46/46 [00:00<00:00, 82.69it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.779      0.477      0.573       0.39\n",
      "  clear_plastic_bottle         12         16       0.92      0.375      0.661      0.427\n",
      "             drink_can         25         35      0.955      0.886      0.936      0.694\n",
      "          drink_carton          5          6      0.647      0.617      0.588      0.429\n",
      "          glass_bottle         11         13      0.554      0.231      0.373      0.222\n",
      "             glass_jar          5          5      0.598        0.6       0.55      0.371\n",
      "  other_plastic_bottle          5          9          1      0.153       0.33      0.196\n",
      "Speed: 2.7ms preprocess, 10.2ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train62\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([0, 1, 2, 3, 4, 5])\n",
       "box: ultralytics.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x000001CB9EE9F880>\n",
       "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
       "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,  0.00019305,  9.6527e-05,           0],\n",
       "       [          1,           1,           1, ...,     0.38889,     0.38889,           0],\n",
       "       [          1,           1,           1, ...,     0.23077,     0.23077,           0],\n",
       "       [          1,           1,           1, ...,  0.00037394,  0.00018697,           0],\n",
       "       [          1,           1,           1, ...,  0.00013171,  6.5855e-05,           0],\n",
       "       [          1,           1,           1, ...,   0.0005148,   0.0002574,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.035374,    0.035483,     0.11601, ...,           0,           0,           0],\n",
       "       [    0.36842,     0.36842,     0.45039, ...,           0,           0,           0],\n",
       "       [    0.16438,     0.16438,      0.1866, ...,           0,           0,           0],\n",
       "       [   0.081633,    0.081633,     0.11085, ...,           0,           0,           0],\n",
       "       [    0.05042,     0.05042,    0.066277, ...,           0,           0,           0],\n",
       "       [     0.1519,      0.1519,     0.18785, ...,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.018081,    0.018138,    0.062466, ...,           1,           1,           1],\n",
       "       [    0.22581,     0.22581,     0.29065, ...,           1,           1,           1],\n",
       "       [   0.089552,    0.089552,      0.1029, ...,           1,           1,           1],\n",
       "       [   0.043103,    0.043103,    0.059728, ...,           1,           1,           1],\n",
       "       [   0.026316,    0.026316,    0.035076, ...,           1,           1,           1],\n",
       "       [   0.085714,    0.085714,     0.11303, ...,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[     0.8125,      0.8125,      0.8125, ...,           0,           0,           0],\n",
       "       [          1,           1,           1, ...,           0,           0,           0],\n",
       "       [          1,           1,           1, ...,           0,           0,           0],\n",
       "       [    0.76923,     0.76923,     0.76923, ...,           0,           0,           0],\n",
       "       [        0.6,         0.6,         0.6, ...,           0,           0,           0],\n",
       "       [    0.66667,     0.66667,     0.55556, ...,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
       "fitness: np.float64(0.40817662322444126)\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
       "maps: array([    0.42739,     0.69439,      0.4294,      0.2216,     0.37058,     0.19576])\n",
       "names: {0: 'clear_plastic_bottle', 1: 'drink_can', 2: 'drink_carton', 3: 'glass_bottle', 4: 'glass_jar', 5: 'other_plastic_bottle'}\n",
       "plot: True\n",
       "results_dict: {'metrics/precision(B)': np.float64(0.779178744617926), 'metrics/recall(B)': np.float64(0.4769806350292862), 'metrics/mAP50(B)': np.float64(0.5730958179214122), 'metrics/mAP50-95(B)': np.float64(0.38985226825811115), 'fitness': np.float64(0.40817662322444126)}\n",
       "save_dir: WindowsPath('runs/detect/train62')\n",
       "speed: {'preprocess': 2.6874542236328125, 'inference': 10.160612023395041, 'loss': 0.0, 'postprocess': 1.5025657156239385}\n",
       "task: 'detect'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.val() #This will output a train file however it will be on the validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Validating the YOLO11 model on the Testing subset.</h3>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.68  Python-3.9.19 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4070, 12282MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Testing\\datasets\\01m-All-1\\test\\labels.cache... 90 images, 0 backgrounds, 0 corrupt: 100%|| 90/90 [00:00<?, ?it/s]\n",
      "Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/90 [00:00<?, ?it/s]Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB RAM): 100%|| 90/90 [00:01<00:00, 80.91it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 6/6 [00:01<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         90        141      0.673      0.708      0.716       0.49\n",
      "  clear_plastic_bottle         29         33      0.801      0.697       0.85      0.549\n",
      "             drink_can         38         53       0.83      0.925      0.957      0.673\n",
      "          drink_carton          8          8      0.369          1      0.475       0.38\n",
      "          glass_bottle         16         18      0.412      0.556      0.516      0.226\n",
      "             glass_jar         11         11      0.628      0.545      0.673      0.473\n",
      "  other_plastic_bottle         15         18          1      0.525      0.824      0.637\n",
      "Speed: 1.5ms preprocess, 6.1ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train63\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([0, 1, 2, 3, 4, 5])\n",
       "box: ultralytics.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x000001CB9F5CCCD0>\n",
       "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
       "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,        0.55,        0.55,           0],\n",
       "       [          1,           1,           1, ...,     0.30286,     0.30286,           0],\n",
       "       [        0.5,         0.5,         0.5, ...,     0.38095,     0.38095,           0],\n",
       "       [          1,           1,           1, ...,   0.0014051,  0.00070254,           0],\n",
       "       [          1,           1,           1, ...,     0.25581,     0.25581,           0],\n",
       "       [          1,           1,           1, ...,        0.45,        0.45,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.056075,    0.056326,     0.15086, ...,           0,           0,           0],\n",
       "       [    0.36177,     0.36177,     0.44078, ...,           0,           0,           0],\n",
       "       [    0.10256,     0.10256,     0.11725, ...,           0,           0,           0],\n",
       "       [    0.07489,    0.074923,     0.10738, ...,           0,           0,           0],\n",
       "       [    0.09607,     0.09607,     0.11807, ...,           0,           0,           0],\n",
       "       [    0.21818,     0.21818,     0.27273, ...,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.028846,    0.028979,    0.081586, ...,           1,           1,           1],\n",
       "       [    0.22083,     0.22083,     0.28269, ...,           1,           1,           1],\n",
       "       [   0.054054,    0.054054,    0.062276, ...,           1,           1,           1],\n",
       "       [   0.038991,    0.039009,    0.056925, ...,           1,           1,           1],\n",
       "       [   0.050459,    0.050459,    0.062738, ...,           1,           1,           1],\n",
       "       [    0.12245,     0.12245,      0.1579, ...,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,           0,           0,           0],\n",
       "       [          1,           1,           1, ...,           0,           0,           0],\n",
       "       [          1,           1,           1, ...,           0,           0,           0],\n",
       "       [    0.94444,     0.94444,     0.94444, ...,           0,           0,           0],\n",
       "       [          1,           1,           1, ...,           0,           0,           0],\n",
       "       [          1,           1,           1, ...,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
       "fitness: np.float64(0.5122410825824938)\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
       "maps: array([    0.54892,      0.6731,     0.38035,     0.22596,     0.47257,     0.63677])\n",
       "names: {0: 'clear_plastic_bottle', 1: 'drink_can', 2: 'drink_carton', 3: 'glass_bottle', 4: 'glass_jar', 5: 'other_plastic_bottle'}\n",
       "plot: True\n",
       "results_dict: {'metrics/precision(B)': np.float64(0.673327258133455), 'metrics/recall(B)': np.float64(0.7079711215189666), 'metrics/mAP50(B)': np.float64(0.7159081200099754), 'metrics/mAP50-95(B)': np.float64(0.48961141175721806), 'fitness': np.float64(0.5122410825824938)}\n",
       "save_dir: WindowsPath('runs/detect/train63')\n",
       "speed: {'preprocess': 1.4736255009969075, 'inference': 6.140780448913574, 'loss': 0.0, 'postprocess': 0.8451355828179253}\n",
       "task: 'detect'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.val(split='test') #This will output a train file however it will be on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Testing the YOLO11 model on the Testing subset.</h3>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.67  Python-3.9.19 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4070, 12282MiB)\n",
      "4-channel-YOLO11n summary (fused): 238 layers, 2,583,466 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\yolov10\\Scripts\\yolo.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\ultralytics\\cfg\\__init__.py\", line 986, in entrypoint\n",
      "    getattr(model, mode)(**overrides)  # default args from model\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\ultralytics\\engine\\model.py\", line 558, in predict\n",
      "    return self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\ultralytics\\engine\\predictor.py\", line 190, in predict_cli\n",
      "    for _ in gen:  # sourcery skip: remove-empty-nested-block, noqa\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 36, in generator_context\n",
      "    response = gen.send(None)\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\ultralytics\\engine\\predictor.py\", line 241, in stream_inference\n",
      "    self.model.warmup(imgsz=(1 if self.model.pt or self.model.triton else self.dataset.bs, 3, *self.imgsz))\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\ultralytics\\nn\\autobackend.py\", line 768, in warmup\n",
      "    self.forward(im)  # warmup\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\ultralytics\\nn\\autobackend.py\", line 552, in forward\n",
      "    y = self.model(im, augment=augment, visualize=visualize, embed=embed)\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 110, in forward\n",
      "    return self.predict(x, *args, **kwargs)\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 128, in predict\n",
      "    return self._predict_once(x, profile, visualize, embed)\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 149, in _predict_once\n",
      "    x = m(x)  # run\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\ultralytics\\nn\\modules\\conv.py\", line 55, in forward_fuse\n",
      "    return self.act(self.conv(x))\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 554, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 549, in _conv_forward\n",
      "    return F.conv2d(\n",
      "RuntimeError: Given groups=1, weight of size [16, 4, 3, 3], expected input[1, 3, 640, 640] to have 4 channels, but got 3 channels instead\n"
     ]
    }
   ],
   "source": [
    "!yolo task=detect \\\n",
    "mode=predict \\\n",
    "model=E:/Testing/datasets/runs/detect/train5/weights/best.pt \\\n",
    "conf=0.2 \\\n",
    "source=E:/Testing/datasets/01m-All-1/test/images \\\n",
    "save=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO(\n",
      "  (model): DetectionModel(\n",
      "    (model): Sequential(\n",
      "      (0): Conv(\n",
      "        (conv): Conv2d(4, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv(\n",
      "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (2): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(8, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): Conv(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (4): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): Conv(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (6): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): C3k(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv3): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (m): Sequential(\n",
      "              (0): Bottleneck(\n",
      "                (cv1): Conv(\n",
      "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "                (cv2): Conv(\n",
      "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "              (1): Bottleneck(\n",
      "                (cv1): Conv(\n",
      "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "                (cv2): Conv(\n",
      "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): Conv(\n",
      "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (8): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): C3k(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv3): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (m): Sequential(\n",
      "              (0): Bottleneck(\n",
      "                (cv1): Conv(\n",
      "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "                (cv2): Conv(\n",
      "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "              (1): Bottleneck(\n",
      "                (cv1): Conv(\n",
      "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "                (cv2): Conv(\n",
      "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): SPPF(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (10): C2PSA(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): Sequential(\n",
      "          (0): PSABlock(\n",
      "            (attn): Attention(\n",
      "              (qkv): Conv(\n",
      "                (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): Identity()\n",
      "              )\n",
      "              (proj): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): Identity()\n",
      "              )\n",
      "              (pe): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): Identity()\n",
      "              )\n",
      "            )\n",
      "            (ffn): Sequential(\n",
      "              (0): Conv(\n",
      "                (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv(\n",
      "                (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): Identity()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): Upsample(scale_factor=2.0, mode='nearest')\n",
      "      (12): Concat()\n",
      "      (13): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): Upsample(scale_factor=2.0, mode='nearest')\n",
      "      (15): Concat()\n",
      "      (16): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (17): Conv(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (18): Concat()\n",
      "      (19): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (20): Conv(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (21): Concat()\n",
      "      (22): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): C3k(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv3): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (m): Sequential(\n",
      "              (0): Bottleneck(\n",
      "                (cv1): Conv(\n",
      "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "                (cv2): Conv(\n",
      "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "              (1): Bottleneck(\n",
      "                (cv1): Conv(\n",
      "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "                (cv2): Conv(\n",
      "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (23): Detect(\n",
      "        (cv2): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (cv3): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): DWConv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): DWConv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Conv2d(64, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): DWConv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv(\n",
      "                (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): DWConv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Conv2d(64, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): DWConv(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv(\n",
      "                (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): DWConv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Conv2d(64, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (dfl): DFL(\n",
      "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_path = 'E:/Testing/datasets/runs/detect/train5/weights/best.pt'\n",
    "model = YOLO(model_path)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBkrV5y5X9CH"
   },
   "source": [
    "**<h3>Training Results.</h3>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "mWCxLBpMbKoQ",
    "outputId": "722f7b87-4d71-4e44-85aa-0c6a33f08362"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Replace 'path/to/your/data.csv' with the actual path to your CSV file\n",
    "file_path = 'E:/Testing/datasets/runs/detect/train4/results.csv'\n",
    "\n",
    "column_name = 'epoch'\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Remove leading spaces from column names\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plotting the training losses\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(data['epoch'], data['train/box_loss'], label='Box Loss')\n",
    "plt.plot(data['epoch'], data['train/cls_loss'], label='Class Loss')\n",
    "plt.title('Training Losses')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eA9tRvyOf-cP"
   },
   "source": [
    "**<h3>Validation Results.</h3>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "jw8qLLSef-lX",
    "outputId": "c4e37ba4-ec04-45b1-f055-9adac023570e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plotting the training losses\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(data['epoch'], data['val/box_loss'], label='Box Loss')\n",
    "plt.plot(data['epoch'], data['val/cls_loss'], label='Class Loss')\n",
    "plt.title('Validation Losses')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6EZwLBNfjKP"
   },
   "source": [
    "**<h3>Testing Resultant Images.</h3>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "mzkcnDekgUWf"
   },
   "outputs": [],
   "source": [
    "counter =0\n",
    "limit = 10\n",
    "for image_path in glob.glob(f'{HOME}/datasets/runs/detect/predict/*.jpg'):\n",
    "      display(Image(filename=image_path))\n",
    "      print(\"\\n\")\n",
    "      counter += 1\n",
    "      if counter == limit:\n",
    "          break"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "yolov10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
