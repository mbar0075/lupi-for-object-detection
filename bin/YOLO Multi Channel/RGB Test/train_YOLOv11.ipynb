{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>YOLO11 Training with Privileged Information</h1>\n",
    "<h2>Matthias Bartolo</h2>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Package Imports</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aFbKjvakDnZE",
    "outputId": "dbe28130-1112-4141-e265-65d3f6b06acc"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade roboflow ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rasterio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWZ2DvvEDnZF"
   },
   "source": [
    "**<h3>Required libraries.</h3>**\n",
    "https://github.com/ultralytics/ultralytics/issues/2154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9Zbf3zFkDnZG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "# import ultralytics\n",
    "import locale\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image\n",
    "from roboflow import Roboflow\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyRdDYkqAKN4"
   },
   "source": [
    "**<h3>Using GPU if one is available.</h3>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan 29 08:15:38 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 566.36                 Driver Version: 566.36         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070      WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "|  0%   30C    P8              8W /  200W |    1183MiB /  12282MiB |      3%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      7080    C+G   ...n\\NVIDIA app\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A      7568    C+G   ...Google\\NearbyShare\\nearby_share.exe      N/A      |\n",
      "|    0   N/A  N/A      9336    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     10320    C+G   ...5.0_x64__cv1g1gvanyjgm\\WhatsApp.exe      N/A      |\n",
      "|    0   N/A  N/A     12588    C+G   ...n\\NVIDIA app\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A     14480    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     14880    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     17016    C+G   ...x64__97hta09mmv6hy\\Build\\Lively.exe      N/A      |\n",
      "|    0   N/A  N/A     17988    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     21204    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     21520    C+G   ...64__v826wp6bftszj\\TranslucentTB.exe      N/A      |\n",
      "|    0   N/A  N/A     21696    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     21812    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     26932    C+G   ...n\\132.0.2957.127\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     27832    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A     29188    C+G   ...a09mmv6hy\\Build\\Plugins\\Mpv\\mpv.exe      N/A      |\n",
      "|    0   N/A  N/A     29444    C+G   ...n\\132.0.2957.127\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     30504    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     34576    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dgNdkO48DnZG",
    "outputId": "11ca77d8-7b09-4d02-a899-0dfdc5aaac2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CjpPg4mGKc1v",
    "outputId": "7035eeee-b2d1-438c-bb57-188bd34ea8eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\Testing\\RGB Test\n"
     ]
    }
   ],
   "source": [
    "# Retrieving the current working directory\n",
    "HOME = os.getcwd()\n",
    "print(HOME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3C3EO_2zNChu"
   },
   "source": [
    "**<h3>Downloading the Roboflow dataset.</h3>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BSd93ZJzZZKt",
    "outputId": "cd51037c-4df9-415e-ecd8-540168fb544a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in 01m-All-1 to yolov11::   3%|â–Ž         | 33295/1048346 [00:04<02:08, 7924.57it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m project \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mworkspace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoda-dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mproject(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m01m-all\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m version \u001b[38;5;241m=\u001b[39m project\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mversion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myolov11\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\roboflow\\core\\version.py:233\u001b[0m, in \u001b[0;36mVersion.download\u001b[1;34m(self, model_format, location, overwrite)\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError:\n\u001b[0;32m    231\u001b[0m             response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m--> 233\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__download_zip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__extract_zip(location, model_format)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reformat_yaml(location, model_format)  \u001b[38;5;66;03m# TODO: is roboflow-python a place to be munging yaml files?\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\roboflow\\core\\version.py:837\u001b[0m, in \u001b[0;36mVersion.__download_zip\u001b[1;34m(self, link, location, format)\u001b[0m\n\u001b[0;32m    835\u001b[0m total_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(response\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-length\u001b[39m\u001b[38;5;124m\"\u001b[39m))  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    836\u001b[0m desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m TQDM_DISABLE \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading Dataset Version Zip in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mformat\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 837\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[0;32m    838\u001b[0m     response\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m),\n\u001b[0;32m    839\u001b[0m     desc\u001b[38;5;241m=\u001b[39mdesc,\n\u001b[0;32m    840\u001b[0m     total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(total_length \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    841\u001b[0m ):\n\u001b[0;32m    842\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk:\n\u001b[0;32m    843\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(chunk)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\urllib3\\response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp):\n\u001b[1;32m--> 628\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m    631\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\urllib3\\response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    564\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 567\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    569\u001b[0m         flush_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\urllib3\\response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\http\\client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;66;03m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 463\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\http\\client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    502\u001b[0m         b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength]\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\ssl.py:1275\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1272\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1273\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1274\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\ssl.py:1133\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(os.path.join(HOME, 'datasets')):\n",
    "    os.mkdir(os.path.join(HOME, 'datasets'))\n",
    "os.chdir(os.path.join(HOME, 'datasets'))\n",
    "\n",
    "\n",
    "# Zoo Animals Dataset\n",
    "rf = Roboflow(api_key=\"nyynHs3oneLLx01D04rC\")\n",
    "project = rf.workspace(\"soda-dataset\").project(\"01m-all\")\n",
    "version = project.version(1)\n",
    "dataset = version.download(\"yolov11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomYOLO(\n",
      "  (model): DetectionModel(\n",
      "    (model): Sequential(\n",
      "      (0): Conv(\n",
      "        (conv): Conv2d(4, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv(\n",
      "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (2): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(8, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): Conv(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (4): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): Conv(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (6): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): C3k(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv3): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (m): Sequential(\n",
      "              (0): Bottleneck(\n",
      "                (cv1): Conv(\n",
      "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "                (cv2): Conv(\n",
      "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "              (1): Bottleneck(\n",
      "                (cv1): Conv(\n",
      "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "                (cv2): Conv(\n",
      "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): Conv(\n",
      "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (8): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): C3k(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv3): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (m): Sequential(\n",
      "              (0): Bottleneck(\n",
      "                (cv1): Conv(\n",
      "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "                (cv2): Conv(\n",
      "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "              (1): Bottleneck(\n",
      "                (cv1): Conv(\n",
      "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "                (cv2): Conv(\n",
      "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): SPPF(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (10): C2PSA(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): Sequential(\n",
      "          (0): PSABlock(\n",
      "            (attn): Attention(\n",
      "              (qkv): Conv(\n",
      "                (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): Identity()\n",
      "              )\n",
      "              (proj): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): Identity()\n",
      "              )\n",
      "              (pe): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): Identity()\n",
      "              )\n",
      "            )\n",
      "            (ffn): Sequential(\n",
      "              (0): Conv(\n",
      "                (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv(\n",
      "                (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): Identity()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): Upsample(scale_factor=2.0, mode='nearest')\n",
      "      (12): Concat()\n",
      "      (13): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): Upsample(scale_factor=2.0, mode='nearest')\n",
      "      (15): Concat()\n",
      "      (16): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (17): Conv(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (18): Concat()\n",
      "      (19): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (20): Conv(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (21): Concat()\n",
      "      (22): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): C3k(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv3): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (m): Sequential(\n",
      "              (0): Bottleneck(\n",
      "                (cv1): Conv(\n",
      "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "                (cv2): Conv(\n",
      "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "              (1): Bottleneck(\n",
      "                (cv1): Conv(\n",
      "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "                (cv2): Conv(\n",
      "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (23): Detect(\n",
      "        (cv2): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (cv3): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): DWConv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): DWConv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Conv2d(64, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): DWConv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv(\n",
      "                (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): DWConv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Conv2d(64, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): DWConv(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv(\n",
      "                (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): DWConv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Conv2d(64, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (dfl): DFL(\n",
      "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Added Code for Custom 4 Channel YOLO Model:\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Specify the model path and number of input channels\n",
    "model_path = 'yolo11.pt'  # Path to your YOLO model file\n",
    "num_channels = 4  # Example for multi-channel input (e.g., RGB + edge detection)\n",
    "\n",
    "# UltraLytics YOLO Model implementation through pytorch\n",
    "\n",
    "# class CustomYOLO(nn.Module):\n",
    "#     def __init__(self, model_path, num_channels):\n",
    "#         \"\"\"\n",
    "#         Initialize the custom YOLO model by adding a custom beginning layer for multi-channel input.\n",
    "\n",
    "#         Args:\n",
    "#             model_path (str): Path to the pre-trained YOLO model checkpoint.\n",
    "#             num_channels (int): Number of input channels (e.g., 3 for RGB, 4 for RGB + edge).\n",
    "#         \"\"\"\n",
    "#         super(CustomYOLO, self).__init__()\n",
    "        \n",
    "#         # Load the checkpoint\n",
    "#         checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "        \n",
    "#         # Extract the model from the checkpoint if it's a dictionary\n",
    "#         if isinstance(checkpoint, dict) and \"model_state_dict\" in checkpoint:\n",
    "#             model = YOLO()  # Assuming YOLOModel is your architecture\n",
    "#             model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "#             self.model = model\n",
    "#         else:\n",
    "#             # Assume the checkpoint itself is the model\n",
    "#             self.model = checkpoint\n",
    "            \n",
    "#         print(self.model)\n",
    "        \n",
    "#         # Modify the first convolution to handle multi-channel input\n",
    "#         original_conv = self.model.model.model[0].conv\n",
    "#         self.model.model.model[0].conv = nn.Conv2d(\n",
    "#             num_channels, \n",
    "#             original_conv.out_channels, \n",
    "#             kernel_size=original_conv.kernel_size, \n",
    "#             stride=original_conv.stride, \n",
    "#             padding=original_conv.padding, \n",
    "#             bias=original_conv.bias is not None\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "# from ultralytics.nn.modules.conv import Conv  # Import the Conv layer from YOLO's common module\n",
    "\n",
    "class CustomYOLO(YOLO):\n",
    "    def __init__(self, model_path, num_channels):\n",
    "        \"\"\"\n",
    "        Initialize the custom YOLO model by adding a custom beginning layer for multi-channel input.\n",
    "        \n",
    "        Args:\n",
    "            model_path (str): Path to the pre-trained YOLO model.\n",
    "            num_channels (int): Number of input channels (e.g., 3 for RGB, 4 for RGB + edge).\n",
    "        \"\"\"\n",
    "        # Load the original YOLO model from the given path\n",
    "        super(CustomYOLO, self).__init__(model_path)\n",
    "        \n",
    "        # Modify the first convolution to handle multi-channel input (num_channels)\n",
    "        self.model.model[0].conv = nn.Conv2d(num_channels, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "    \n",
    "\n",
    "# Create the custom YOLO model\n",
    "model = CustomYOLO(model_path, num_channels)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 640, 640])\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [16, 4, 3, 3], expected input[1, 3, 640, 640] to have 4 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m sample_input\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(sample_input\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 7\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\ultralytics\\engine\\model.py:180\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    153\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    154\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    156\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    157\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(source, stream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\ultralytics\\engine\\model.py:558\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 558\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\ultralytics\\engine\\predictor.py:175\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\torch\\utils\\_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\ultralytics\\engine\\predictor.py:241\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# Warmup model\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone_warmup:\n\u001b[1;32m--> 241\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarmup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtriton\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimgsz\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone_warmup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindows, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, [], \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\ultralytics\\nn\\autobackend.py:768\u001b[0m, in \u001b[0;36mAutoBackend.warmup\u001b[1;34m(self, imgsz)\u001b[0m\n\u001b[0;32m    766\u001b[0m im \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mimgsz, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mhalf \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# input\u001b[39;00m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\ultralytics\\nn\\autobackend.py:552\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[1;32m--> 552\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\ultralytics\\nn\\tasks.py:110\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\ultralytics\\nn\\tasks.py:128\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\ultralytics\\nn\\tasks.py:149\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 149\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    150\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:55\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply convolution and activation without batch normalization.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\yolov10\\lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [16, 4, 3, 3], expected input[1, 3, 640, 640] to have 4 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "# Assuming you have a 3-channel input tensor\n",
    "sample_input = torch.randn(1, 3, 640, 640)  # RGB (3 channels)\n",
    "sample_input = torch.cat([sample_input, torch.zeros(1, 1, 640, 640)], dim=1)  # Add an extra channel\n",
    "sample_input/=255\n",
    "\n",
    "print(sample_input.shape)\n",
    "output = model(sample_input, save=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Training the YOLO11 model.</h3>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_WRhnBXjDnZH",
    "outputId": "09bcb11a-5656-4c53-8564-51b29cc08d6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.68 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.67  Python-3.9.19 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4070, 12282MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.pt, data=e:\\Testing\\RGB Test\\datasets\\01m-All-1/data.yaml, epochs=100, time=None, patience=15, batch=16, imgsz=640, save=True, save_period=-1, cache=True, device=0, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train\n",
      "Overriding model.yaml nc=80 with nc=6\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    431842  ultralytics.nn.modules.head.Detect           [6, [64, 128, 256]]           \n",
      "YOLO11n summary: 319 layers, 2,591,010 parameters, 2,590,994 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning E:\\Testing\\RGB Test\\datasets\\01m-All-1\\train\\labels.cache... 316 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 316/316 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.3GB RAM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 316/316 [00:01<00:00, 194.26it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Testing\\RGB Test\\datasets\\01m-All-1\\valid\\labels.cache... 46 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB RAM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:00<00:00, 187.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      1/100      2.44G      1.732      3.473       1.47         35        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  7.82it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  8.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84   0.000389     0.0256   0.000337   6.99e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      2/100      2.48G      1.667      2.447      1.491         47        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 11.57it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84    0.00042     0.0464    0.00027   6.67e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      3/100      2.37G      1.682      2.201      1.505         44        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 11.84it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84   8.12e-05     0.0382   4.68e-05   1.29e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      4/100      2.39G      1.684      2.079      1.497         43        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.52it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84   8.12e-05     0.0382   4.68e-05   1.29e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      5/100      2.37G       1.65      2.005      1.439         26        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.167     0.0289   5.22e-05    1.2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      6/100      2.37G      1.561      1.892      1.393         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.45it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84   3.73e-05    0.00952   1.77e-05   4.38e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      7/100      2.37G      1.512      1.744      1.361         26        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.45it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 10.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84    0.00018     0.0519   0.000125   3.86e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      8/100      2.37G      1.483       1.71      1.354         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.64it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 12.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.176      0.112     0.0117    0.00351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      9/100      2.37G      1.417       1.57      1.322         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.586      0.121      0.101     0.0499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     10/100      2.37G      1.425      1.619      1.316         26        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.63it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.794      0.114      0.124     0.0738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     11/100      2.37G      1.323      1.523      1.271         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 12.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84       0.58     0.0905     0.0965     0.0535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     12/100      2.37G      1.361      1.448      1.273         39        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.82it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  9.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84       0.56     0.0446     0.0229     0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     13/100      2.37G      1.295      1.414      1.228         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.92it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 10.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84       0.38      0.127     0.0544     0.0317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     14/100      2.37G      1.296      1.393      1.244         35        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.86it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 11.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.625       0.11      0.141     0.0875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     15/100      2.37G      1.287       1.37      1.242         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 13.05it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.862      0.159      0.193      0.129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     16/100      2.39G       1.29      1.377      1.201         39        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 13.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.698      0.165      0.211      0.128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     17/100      2.37G      1.246       1.32       1.19         40        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  9.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84       0.52      0.222       0.22      0.134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     18/100      2.37G      1.268      1.283      1.225         40        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 12.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.253      0.427      0.269      0.144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     19/100      2.37G      1.286       1.34       1.22         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.50it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 12.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.224      0.393      0.221      0.122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     20/100      2.39G      1.295      1.336      1.213         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.53it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.231      0.305       0.24      0.147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     21/100      2.37G       1.22      1.248      1.179         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.428      0.273      0.317      0.206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     22/100      2.37G       1.24      1.265      1.189         45        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84       0.61      0.237      0.359      0.201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     23/100      2.37G      1.213      1.221      1.184         46        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 10.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.338      0.373      0.235      0.148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     24/100      2.37G      1.209      1.185      1.177         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.50it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 11.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.236      0.391      0.322       0.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     25/100      2.37G      1.189      1.212      1.173         40        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.92it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.202      0.453      0.298      0.195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     26/100      2.37G      1.166      1.191      1.161         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.67it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.392      0.287      0.296       0.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     27/100      2.37G      1.097      1.115      1.108         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.47it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.382      0.339      0.275      0.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     28/100      2.37G      1.116       1.13      1.129         44        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.241      0.419      0.297        0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     29/100      2.37G      1.145      1.113      1.142         52        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 12.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.548      0.381      0.376      0.265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     30/100      2.37G      1.163      1.128      1.154         44        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.59it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.295      0.485      0.356      0.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     31/100      2.37G      1.112      1.081      1.112         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.82it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.358      0.308      0.296      0.191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     32/100      2.37G      1.101      1.038       1.11         39        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.289      0.534       0.38      0.241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     33/100      2.37G      1.115      1.031      1.125         56        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 13.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.445      0.381      0.451      0.314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     34/100      2.37G       1.05       1.02      1.099         44        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.49it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.432      0.407      0.348       0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     35/100      2.39G       1.07     0.9942      1.105         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.73it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.221      0.445      0.291      0.203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     36/100      2.37G       1.03      1.021      1.084         33        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.59it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.344       0.41      0.358      0.221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     37/100      2.37G      1.032     0.9613      1.066         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.69it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.288       0.45      0.317      0.197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     38/100      2.37G      1.059     0.9555      1.081         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 12.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.261      0.386      0.296       0.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     39/100      2.37G      1.029     0.9174      1.068         50        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.93it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.337      0.417      0.359      0.222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     40/100      2.37G      1.044     0.9226      1.074         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.49it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.609      0.391       0.47      0.315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     41/100      2.37G      1.028     0.9288      1.083         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.49it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.408      0.521      0.413      0.269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     42/100      2.37G      1.045     0.9455      1.083         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.73it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.475      0.343      0.369      0.263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     43/100      2.37G       1.08     0.9685      1.108         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.48it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84       0.46      0.404      0.417      0.257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     44/100      2.37G      1.041     0.8778      1.081         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 11.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84       0.48      0.418       0.47      0.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     45/100      2.37G      1.034     0.9125      1.075         40        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.41it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 12.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.377      0.648      0.439      0.313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     46/100      2.37G      1.026     0.8968      1.075         39        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.64it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 11.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84       0.62      0.322       0.41       0.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     47/100      2.37G     0.9958     0.9064      1.065         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.57it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.482       0.45      0.474      0.318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     48/100      2.37G     0.9733     0.8459      1.044         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.527      0.476      0.482       0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     49/100      2.39G      1.006      0.866      1.058         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.41it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.517      0.447       0.51      0.356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     50/100      2.37G      1.005     0.8826      1.057         40        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.39it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.656      0.555      0.575      0.413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     51/100      2.37G     0.9951       0.82      1.068         33        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.82it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.581      0.468      0.554      0.394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     52/100      2.37G     0.9888     0.8193      1.053         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.58it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.438      0.534      0.515      0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     53/100      2.37G     0.9341     0.8098      1.035         48        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.48it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84       0.51      0.485      0.488      0.332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     54/100      2.37G     0.9589     0.8053      1.041         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 13.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.543      0.605      0.545      0.368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     55/100      2.39G     0.9841     0.7847      1.047         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.69it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.548      0.541      0.542      0.378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     56/100      2.37G     0.9587     0.8092      1.038         48        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.47it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.524      0.527      0.508      0.366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     57/100      2.37G     0.9308     0.7621      1.023         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.75it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.809       0.43      0.551      0.408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     58/100      2.39G     0.9457     0.7635      1.035         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.525      0.396      0.527      0.404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     59/100      2.39G     0.9559     0.7814      1.051         44        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.40it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.587      0.488      0.495      0.361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     60/100      2.37G     0.9314     0.7921      1.032         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.43it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.547      0.467      0.514      0.366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     61/100      2.37G     0.9308     0.7717      1.026         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.74it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.529      0.476      0.518      0.348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     62/100      2.39G     0.9135     0.7392      1.024         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.69it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 12.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.505      0.432      0.431      0.288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     63/100      2.37G      0.876     0.7107     0.9986         50        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.58it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.533      0.485      0.473      0.287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     64/100      2.37G      0.905     0.7258      1.014         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.93it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 11.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.332      0.561       0.45       0.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     65/100      2.37G     0.9034     0.7557      1.021         45        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.78it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.608       0.55      0.605      0.417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     66/100      2.39G     0.9056     0.7446      1.004         35        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 13.05it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.614      0.494       0.56      0.366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     67/100      2.37G     0.8797     0.7131     0.9929         53        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.595      0.569      0.571       0.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     68/100      2.39G     0.8823     0.7176     0.9965         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.578      0.662      0.632      0.465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     69/100      2.39G     0.8899     0.6808      0.994         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.39it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.583      0.525      0.548      0.376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     70/100      2.39G     0.8731     0.6941      1.001         45        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.86it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.521      0.637       0.56      0.386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     71/100      2.37G     0.8884     0.6975      1.019         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 13.04it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.597      0.513      0.552      0.396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     72/100      2.37G     0.8999     0.7412       1.01         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 13.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.611      0.538      0.581      0.373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     73/100      2.37G     0.8825     0.6925          1         42        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.86it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.514      0.461      0.503      0.352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     74/100      2.39G     0.8931     0.6788      1.014         40        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 13.04it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.485      0.538       0.56      0.369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     75/100      2.39G     0.8794      0.686      1.001         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 13.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.531       0.51      0.571      0.407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     76/100      2.37G     0.8315     0.6346     0.9905         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84       0.61      0.592      0.601      0.407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     77/100      2.37G     0.8366     0.6573     0.9831         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.631      0.563      0.582      0.399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     78/100      2.39G     0.8198     0.6287     0.9728         48        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.601      0.587      0.578      0.416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     79/100      2.37G      0.798     0.6152     0.9581         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.514      0.484      0.513       0.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     80/100      2.37G     0.8032      0.643     0.9686         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.92it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.615       0.44      0.512      0.345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     81/100      2.37G      0.775     0.6191     0.9527         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.87it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.608      0.572      0.589      0.425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     82/100      2.39G     0.8027     0.6362     0.9592         39        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.90it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.592      0.522      0.567       0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     83/100      2.37G     0.8283     0.6365      0.983         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.625      0.487      0.555      0.383\n",
      "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 15 epochs. Best results observed at epoch 68, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=15) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "83 epochs completed in 0.047 hours.\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\last.pt, 5.5MB\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\best.pt, 5.5MB\n",
      "\n",
      "Validating runs\\detect\\train\\weights\\best.pt...\n",
      "Ultralytics 8.3.67  Python-3.9.19 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4070, 12282MiB)\n",
      "YOLO11n summary (fused): 238 layers, 2,583,322 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.579      0.662      0.632      0.464\n",
      "  clear_plastic_bottle         12         16      0.796      0.731       0.78      0.531\n",
      "             drink_can         25         35      0.829      0.968      0.973      0.735\n",
      "          drink_carton          5          6       0.54      0.833      0.664      0.543\n",
      "          glass_bottle         11         13      0.545      0.615      0.591      0.443\n",
      "             glass_jar          5          5      0.451        0.6      0.609      0.404\n",
      "  other_plastic_bottle          5          9      0.313      0.222      0.174      0.131\n",
      "Speed: 0.1ms preprocess, 0.8ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([0, 1, 2, 3, 4, 5])\n",
       "box: ultralytics.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x000002149895BC10>\n",
       "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
       "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,   0.0012972,  0.00064862,           0],\n",
       "       [          1,           1,           1, ...,     0.77778,     0.77778,           0],\n",
       "       [          1,           1,           1, ...,     0.26087,     0.26087,           0],\n",
       "       [          1,           1,           1, ...,  0.00061527,  0.00030764,           0],\n",
       "       [          1,           1,           1, ...,  0.00042596,  0.00021298,           0],\n",
       "       [        0.4,         0.4,         0.4, ...,  0.00077856,  0.00038928,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.21138,     0.21138,      0.2574, ...,           0,           0,           0],\n",
       "       [    0.45161,     0.45161,     0.50041, ...,           0,           0,           0],\n",
       "       [    0.16901,     0.16901,     0.22416, ...,           0,           0,           0],\n",
       "       [    0.12987,     0.12987,     0.15777, ...,           0,           0,           0],\n",
       "       [   0.080808,    0.080808,    0.071419, ...,           0,           0,           0],\n",
       "       [    0.15556,     0.15556,     0.15181, ...,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[     0.1215,      0.1215,     0.15292, ...,           1,           1,           1],\n",
       "       [    0.29167,     0.29167,      0.3337, ...,           1,           1,           1],\n",
       "       [   0.092308,    0.092308,     0.12623, ...,           1,           1,           1],\n",
       "       [   0.070922,    0.070922,      0.0879, ...,           1,           1,           1],\n",
       "       [   0.042553,    0.042553,    0.037969, ...,           1,           1,           1],\n",
       "       [    0.08642,     0.08642,     0.08566, ...,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[     0.8125,      0.8125,      0.8125, ...,           0,           0,           0],\n",
       "       [          1,           1,           1, ...,           0,           0,           0],\n",
       "       [          1,           1,           1, ...,           0,           0,           0],\n",
       "       [    0.76923,     0.76923,     0.76923, ...,           0,           0,           0],\n",
       "       [        0.8,         0.8,         0.6, ...,           0,           0,           0],\n",
       "       [    0.77778,     0.77778,     0.66667, ...,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
       "fitness: np.float64(0.48093756087787687)\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
       "maps: array([    0.53051,     0.73454,     0.54271,     0.44281,     0.40381,     0.13051])\n",
       "names: {0: 'clear_plastic_bottle', 1: 'drink_can', 2: 'drink_carton', 3: 'glass_bottle', 4: 'glass_jar', 5: 'other_plastic_bottle'}\n",
       "plot: True\n",
       "results_dict: {'metrics/precision(B)': np.float64(0.5789644688046252), 'metrics/recall(B)': np.float64(0.6617232797963676), 'metrics/mAP50(B)': np.float64(0.6320460151339574), 'metrics/mAP50-95(B)': np.float64(0.4641477326272012), 'fitness': np.float64(0.48093756087787687)}\n",
       "save_dir: WindowsPath('runs/detect/train')\n",
       "speed: {'preprocess': 0.13218755307404892, 'inference': 0.8265712986821714, 'loss': 0.0, 'postprocess': 0.60644357100777}\n",
       "task: 'detect'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specifying the paths\n",
    "yaml_path  = dataset.location+\"/data.yaml\"\n",
    "\n",
    "# Specifying the model path\n",
    "model_path = 'yolo11n.pt'\n",
    "\n",
    "# Creating YOLO object\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# Specifying training parameters\n",
    "num_epochs = 100  # Number of epochs\n",
    "batch_size = 16 #8 # Adjust based on GPU memory\n",
    "image_size = 640  # Decrease for faster training\n",
    "\n",
    "# Training configuration\n",
    "train_config = {\n",
    "    'data': yaml_path,\n",
    "    'imgsz': image_size,\n",
    "    'batch': batch_size,\n",
    "    'epochs': num_epochs,\n",
    "    'device': 0,  # Use GPU 0\n",
    "    # 'workers': 1,  # Number of data loading workers\n",
    "    'optimizer': 'Adam',  # Use Adam optimizer\n",
    "    'cache': True,  # Cache images for faster training\n",
    "    'patience': 15,  # epochs to wait before decreasing LR\n",
    "    'val': True,  # Run validation during training\n",
    "    'plots': True,  # Run plots during training\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "model.train(**train_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ODk1VTlevxn"
   },
   "source": [
    "**<h3>Validating the YOLO11 model on the Validation subset.</h3>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IkrxsRHoV67H",
    "outputId": "ecb3eac0-3a4f-4df6-fdb0-9db829ad7a76"
   },
   "outputs": [],
   "source": [
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "# !pip install aspose-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YpyuwrNlXc1P",
    "outputId": "f4718557-cd29-4208-d5fa-ad0776a893b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.67  Python-3.9.19 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4070, 12282MiB)\n",
      "YOLO11n summary (fused): 238 layers, 2,583,322 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Testing\\RGB Test\\datasets\\01m-All-1\\valid\\labels.cache... 46 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB RAM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:00<00:00, 191.58it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46         84      0.564      0.641      0.628      0.463\n",
      "  clear_plastic_bottle         12         16      0.792      0.716       0.78      0.541\n",
      "             drink_can         25         35      0.829      0.968       0.97      0.735\n",
      "          drink_carton          5          6      0.529      0.833      0.658      0.534\n",
      "          glass_bottle         11         13      0.567      0.615      0.599       0.46\n",
      "             glass_jar          5          5      0.453        0.6      0.609      0.404\n",
      "  other_plastic_bottle          5          9      0.214      0.111      0.149      0.107\n",
      "Speed: 1.1ms preprocess, 6.8ms inference, 0.0ms loss, 2.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train2\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([0, 1, 2, 3, 4, 5])\n",
       "box: ultralytics.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x000002149830C220>\n",
       "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
       "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,   0.0013608,  0.00068042,           0],\n",
       "       [          1,           1,           1, ...,     0.76087,     0.76087,           0],\n",
       "       [          1,           1,           1, ...,        0.25,        0.25,           0],\n",
       "       [          1,           1,           1, ...,  0.00063789,  0.00031895,           0],\n",
       "       [          1,           1,           1, ...,  0.00041708,  0.00020854,           0],\n",
       "       [    0.33333,     0.33333,     0.33333, ...,  0.00079827,  0.00039913,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.22034,     0.22034,     0.25978, ...,           0,           0,           0],\n",
       "       [    0.46667,     0.46667,     0.51918, ...,           0,           0,           0],\n",
       "       [    0.17143,     0.17143,     0.22425, ...,           0,           0,           0],\n",
       "       [    0.13423,     0.13423,     0.16143, ...,           0,           0,           0],\n",
       "       [   0.079208,    0.079208,    0.070299, ...,           0,           0,           0],\n",
       "       [    0.15909,     0.15909,     0.16082, ...,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.12745,     0.12745,      0.1546, ...,           1,           1,           1],\n",
       "       [    0.30435,     0.30435,      0.3506, ...,           1,           1,           1],\n",
       "       [    0.09375,     0.09375,     0.12628, ...,           1,           1,           1],\n",
       "       [   0.073529,    0.073529,    0.090175, ...,           1,           1,           1],\n",
       "       [   0.041667,    0.041667,    0.037337, ...,           1,           1,           1],\n",
       "       [   0.088608,    0.088608,    0.091441, ...,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[     0.8125,      0.8125,      0.8125, ...,           0,           0,           0],\n",
       "       [          1,           1,           1, ...,           0,           0,           0],\n",
       "       [          1,           1,           1, ...,           0,           0,           0],\n",
       "       [    0.76923,     0.76923,     0.76923, ...,           0,           0,           0],\n",
       "       [        0.8,         0.8,         0.6, ...,           0,           0,           0],\n",
       "       [    0.77778,     0.77778,     0.66667, ...,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
       "fitness: np.float64(0.4798674176019251)\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
       "maps: array([    0.54053,     0.73464,     0.53436,     0.45992,     0.40381,     0.10747])\n",
       "names: {0: 'clear_plastic_bottle', 1: 'drink_can', 2: 'drink_carton', 3: 'glass_bottle', 4: 'glass_jar', 5: 'other_plastic_bottle'}\n",
       "plot: True\n",
       "results_dict: {'metrics/precision(B)': np.float64(0.5639341924504234), 'metrics/recall(B)': np.float64(0.6406284827417867), 'metrics/mAP50(B)': np.float64(0.6275791572228532), 'metrics/mAP50-95(B)': np.float64(0.4634550020884886), 'fitness': np.float64(0.4798674176019251)}\n",
       "save_dir: WindowsPath('runs/detect/train2')\n",
       "speed: {'preprocess': 1.06023705523947, 'inference': 6.824534872303839, 'loss': 0.0, 'postprocess': 2.7202212292215098}\n",
       "task: 'detect'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.val() #This will output a train file however it will be on the validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Validating the YOLO11 model on the Testing subset.</h3>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.67  Python-3.9.19 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4070, 12282MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Testing\\RGB Test\\datasets\\01m-All-1\\test\\labels.cache... 90 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB RAM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:00<00:00, 210.04it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:01<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         90        141      0.789      0.683      0.803      0.614\n",
      "  clear_plastic_bottle         29         33      0.828       0.73      0.888       0.68\n",
      "             drink_can         38         53      0.873      0.906      0.933      0.723\n",
      "          drink_carton          8          8      0.607       0.75       0.73       0.63\n",
      "          glass_bottle         16         18       0.86      0.682      0.821       0.59\n",
      "             glass_jar         11         11       0.78      0.364      0.628        0.4\n",
      "  other_plastic_bottle         15         18      0.786      0.667      0.816      0.659\n",
      "Speed: 0.6ms preprocess, 5.6ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([0, 1, 2, 3, 4, 5])\n",
       "box: ultralytics.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x0000021497F0DD90>\n",
       "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
       "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,     0.53226,     0.53226,           0],\n",
       "       [          1,           1,           1, ...,     0.43443,     0.43443,           0],\n",
       "       [          1,           1,           1, ...,     0.30769,     0.30769,           0],\n",
       "       [          1,           1,           1, ...,   0.0024407,   0.0012203,           0],\n",
       "       [        0.8,         0.8,         0.8, ...,     0.15493,     0.15493,           0],\n",
       "       [          1,           1,           1, ...,     0.32143,     0.32143,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.27848,     0.27853,      0.3352, ...,           0,           0,           0],\n",
       "       [     0.3569,      0.3569,     0.42264, ...,           0,           0,           0],\n",
       "       [    0.12903,     0.12903,     0.14894, ...,           0,           0,           0],\n",
       "       [    0.12639,     0.12639,     0.15366, ...,           0,           0,           0],\n",
       "       [    0.11957,     0.11957,     0.15434, ...,           0,           0,           0],\n",
       "       [    0.21053,     0.21053,     0.25163, ...,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.16176,      0.1618,     0.20134, ...,           1,           1,           1],\n",
       "       [    0.21721,     0.21721,     0.26794, ...,           1,           1,           1],\n",
       "       [   0.068966,    0.068966,     0.08046, ...,           1,           1,           1],\n",
       "       [   0.067729,    0.067729,    0.083635, ...,           1,           1,           1],\n",
       "       [   0.063584,    0.063584,    0.083624, ...,           1,           1,           1],\n",
       "       [    0.11765,     0.11765,     0.14392, ...,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,           0,           0,           0],\n",
       "       [          1,           1,           1, ...,           0,           0,           0],\n",
       "       [          1,           1,           1, ...,           0,           0,           0],\n",
       "       [    0.94444,     0.94444,     0.94444, ...,           0,           0,           0],\n",
       "       [          1,           1,           1, ...,           0,           0,           0],\n",
       "       [          1,           1,           1, ...,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
       "fitness: np.float64(0.6327091137759948)\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
       "maps: array([    0.68032,     0.72305,     0.63033,     0.59017,     0.40021,     0.65885])\n",
       "names: {0: 'clear_plastic_bottle', 1: 'drink_can', 2: 'drink_carton', 3: 'glass_bottle', 4: 'glass_jar', 5: 'other_plastic_bottle'}\n",
       "plot: True\n",
       "results_dict: {'metrics/precision(B)': np.float64(0.7891241071897884), 'metrics/recall(B)': np.float64(0.6830103359832074), 'metrics/mAP50(B)': np.float64(0.8027016018502682), 'metrics/mAP50-95(B)': np.float64(0.6138210595455199), 'fitness': np.float64(0.6327091137759948)}\n",
       "save_dir: WindowsPath('runs/detect/train3')\n",
       "speed: {'preprocess': 0.5503998862372503, 'inference': 5.562644534640842, 'loss': 0.0, 'postprocess': 1.3065046734280057}\n",
       "task: 'detect'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.val(split='test') #This will output a train file however it will be on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBkrV5y5X9CH"
   },
   "source": [
    "**<h3>Training Results.</h3>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "mWCxLBpMbKoQ",
    "outputId": "722f7b87-4d71-4e44-85aa-0c6a33f08362"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Replace 'path/to/your/data.csv' with the actual path to your CSV file\n",
    "file_path = 'E:/Testing/RGB Test/datasets/runs/detect/train/results.csv'\n",
    "\n",
    "column_name = 'epoch'\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Remove leading spaces from column names\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plotting the training losses\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(data['epoch'], data['train/box_loss'], label='Box Loss')\n",
    "plt.plot(data['epoch'], data['train/cls_loss'], label='Class Loss')\n",
    "plt.title('Training Losses')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eA9tRvyOf-cP"
   },
   "source": [
    "**<h3>Validation Results.</h3>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "jw8qLLSef-lX",
    "outputId": "c4e37ba4-ec04-45b1-f055-9adac023570e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plotting the training losses\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(data['epoch'], data['val/box_loss'], label='Box Loss')\n",
    "plt.plot(data['epoch'], data['val/cls_loss'], label='Class Loss')\n",
    "plt.title('Validation Losses')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "yolov10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
