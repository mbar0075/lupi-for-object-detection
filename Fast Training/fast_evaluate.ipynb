{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Fast Evaluate Multiple Pytorch Models Notebook</h1>\n",
    "<h2>Matthias Bartolo</h2>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package Imports and Constants - Remain the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.retinanet import RetinaNetClassificationHead\n",
    "from torchvision.models.detection.fcos import FCOSClassificationHead\n",
    "from torchvision.models.detection.ssd import SSDClassificationHead\n",
    "from torchvision.models.detection.ssdlite import SSDLiteClassificationHead\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_INPUT = '../datasets/SODA-Litter-Dataset-All-2' # COCO-Dataset # BDW-Dataset-1 #UAVVASTE-1\n",
    "MODEL_NAME = 'SODA_Dataset_Tiled_Multi' #'SODA_Dataset_Tiled_Single' # 'BDW_Dataset_Test_Single' # 'BDW_Dataset_Test2_Single' # 'UAVVASTE_Dataset_Test_Single'\n",
    "DIR_TRAIN = f'{DIR_INPUT}/train'\n",
    "DIR_VALID = f'{DIR_INPUT}/valid'\n",
    "DIR_TEST = f'{DIR_INPUT}/test'\n",
    "DIR_IMAGES = 'images'\n",
    "DIR_ANNOTATIONS = '_annotations.coco.json'\n",
    "IMG_RESIZE = (800, 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes - Remain the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For COCO Dataset\n",
    "# # To get the classes from the annotation file\n",
    "# classes_annotation_path = f'{DIR_TRAIN}/{DIR_ANNOTATIONS}'\n",
    "\n",
    "# # Load the annotation file\n",
    "# with open(classes_annotation_path, 'r') as f:\n",
    "#     coco_data = json.load(f)\n",
    "\n",
    "# # Extract class categories\n",
    "# CLASSES = [\n",
    "#     {\n",
    "#         \"id\": category[\"id\"],\n",
    "#         \"name\": category[\"name\"],\n",
    "#         \"supercategory\": category.get(\"supercategory\", \"None\"),\n",
    "#         \"color\": (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)),\n",
    "#     }\n",
    "#     for category in coco_data[\"categories\"]\n",
    "# ]\n",
    "\n",
    "# # Change id = 0 to Background\n",
    "# CLASSES[0]['name'] = 'Background'\n",
    "\n",
    "# For Litter Multi-Class:\n",
    "CLASSES = [\n",
    "    {\n",
    "        \"id\": 0,\n",
    "        \"name\": \"Background\",\n",
    "        \"supercategory\": \"Background\",\n",
    "        \"color\": [0, 0, 0],  # Black for background\n",
    "    },  # Background Class which was needed for FasterRCNN\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"name\": \"Clear Plastic Bottle\",\n",
    "        \"supercategory\": \"Bottle\",\n",
    "        \"color\": [180, 240, 240],  # Muted green for clear plastic bottle\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"name\": \"Drink Can\",\n",
    "        \"supercategory\": \"Can\",\n",
    "        \"color\": [60, 60, 220],  # Muted red for drink can\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"name\": \"Drink Carton\",\n",
    "        \"supercategory\": \"Carton\",\n",
    "        \"color\": [60, 180, 255],  # Muted orange for drink carton\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"name\": \"Glass Bottle\",\n",
    "        \"supercategory\": \"Bottle\",\n",
    "        \"color\": [0, 130, 0],  # Muted blue for glass bottle\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"name\": \"Glass Jar\",\n",
    "        \"supercategory\": \"Glass Jar\",\n",
    "        \"color\": [100, 70, 50],  # Muted brownish-red for glass jar\n",
    "    },\n",
    "    {\n",
    "        \"id\": 6,\n",
    "        \"name\": \"Other Plastic Bottle\",\n",
    "        \"supercategory\": \"Bottle\",\n",
    "        \"color\": [200, 200, 100],  # Muted cyan for other plastic bottle\n",
    "    }\n",
    "]\n",
    "\n",
    "# For Litter Binary:\n",
    "# CLASSES = [\n",
    "#     {\n",
    "#         \"id\": 0,\n",
    "#         \"name\": \"Background\",\n",
    "#         \"supercategory\": \"Background\",\n",
    "#         \"color\": [0, 0, 0],  # Black for background\n",
    "#     },  # Background Class which was needed for FasterRCNN\n",
    "#     {\n",
    "#         \"id\": 1,\n",
    "#         \"name\": \"Litter\",\n",
    "#         \"supercategory\": \"Litter\",\n",
    "#         \"color\": [80, 150, 80],  # Muted green for clear plastic bottle\n",
    "#     },\n",
    "# ]# Also labels.append(1)\n",
    "\n",
    "\n",
    "NUM_CLASSES = len(CLASSES) # Number of classes in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Means and Stds - Remain the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the image mean and standard deviation\n",
    "\"\"\"\n",
    "From Pytorch documentation:\n",
    "    - mean (sequence) – Sequence of means for each channel.\n",
    "    - std (sequence) – Sequence of standard deviations for each channel.\n",
    "    https://pytorch.org/vision/0.9/_modules/torchvision/models/detection/faster_rcnn.html\n",
    "\n",
    "    Varies based on the dataset used. For COCO dataset, the mean and standard deviation are:\n",
    "        - mean = [0.485, 0.456, 0.406]\n",
    "        - std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    IMPORTANT: CHANGE THESE FOR FINAL DATASET\n",
    "\"\"\"\n",
    "# COCO dataset\n",
    "# img_means = [\n",
    "#     0.338, 0.320, 0.292, 0.077\n",
    "# ]\n",
    "\n",
    "# img_stds = [\n",
    "#     0.314, 0.304, 0.302, 0.126\n",
    "# ]\n",
    "\n",
    "# Pascal VOC dataset\n",
    "# img_means = [0.452, 0.431, 0.399, 0.142]\n",
    "# img_stds = [0.275, 0.273, 0.284, 0.216]\n",
    "\n",
    "# SODA Litter dataset\n",
    "img_means = [0.467, 0.43, 0.357, 0.021]\n",
    "img_stds = [0.255, 0.24, 0.233, 0.129]\n",
    "\n",
    "# BDW dataset\n",
    "# img_means = [0.534, 0.554, 0.496, 0.041]\n",
    "# img_stds = [0.183, 0.159, 0.205, 0.182]\n",
    "\n",
    "# UAVVASTE dataset\n",
    "# img_means = [0.522, 0.52, 0.446, 0.006]\n",
    "# img_stds = [0.168, 0.169, 0.178, 0.073]\n",
    "\n",
    "\n",
    "ALL_PRIVILEGED_INFORMATION_DIRS = [\n",
    "    \"Box_Mask\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Size - Remain the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8#4\n",
    "num_workers = 4#0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifications - Change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RetinaNet Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory Inputs\n",
    "# SAVE_DIR = f'../runs/RetinaNet/RetinaNet_{MODEL_NAME}_baseline1'\n",
    "\n",
    "# # Privileged Information Paths\n",
    "# PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# # Special Constructed Features\n",
    "#     # \"Box_Mask\",\n",
    "# ]\n",
    "\n",
    "# # Number of input image channels RGB + Extras\n",
    "# NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# # Defining Model\n",
    "\n",
    "# # Set the device to CUDA or CPU\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # Load the RetinaNet model with pretrained weights\n",
    "# weights = torchvision.models.detection.RetinaNet_ResNet50_FPN_Weights.DEFAULT\n",
    "# pre_trained_model = torchvision.models.detection.retinanet_resnet50_fpn(weights=weights)\n",
    "\n",
    "# model = pre_trained_model.to(device)\n",
    "\n",
    "# # Modify the first convolutional layer for Multi-channel input\n",
    "# model.backbone.body.conv1 = torch.nn.Conv2d(NUM_CHANNELS, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "# # Initialize the first convolutional layer's weights\n",
    "# torch.nn.init.kaiming_normal_(model.backbone.body.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# # Get the number of input features for the classification head\n",
    "# in_features = model.head.classification_head.cls_logits.in_channels\n",
    "# num_anchors = model.head.classification_head.num_anchors\n",
    "\n",
    "# # Modify classification head to match the number of classes for your task\n",
    "# # RetinaNetClassificationHead is redefined to include the correct number of classes\n",
    "# model.head.classification_head = RetinaNetClassificationHead(\n",
    "#     in_channels=in_features,\n",
    "#     num_classes=NUM_CLASSES,\n",
    "#     num_anchors=num_anchors,\n",
    "#     norm_layer=partial(torch.nn.GroupNorm, 32)\n",
    "# )\n",
    "\n",
    "# # Move the model to the correct device (e.g., CUDA or CPU)\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Verify the model structure\n",
    "# print(model)\n",
    "\n",
    "# import evaluate\n",
    "\n",
    "# evaluate.main_function(\n",
    "#     DIR_TEST=DIR_TEST,\n",
    "#     DIR_IMAGES=DIR_IMAGES,\n",
    "#     DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "#     IMG_RESIZE=IMG_RESIZE,\n",
    "#     SAVE_DIR=SAVE_DIR,\n",
    "#     CLASSES=CLASSES,\n",
    "#     NUM_CLASSES=NUM_CLASSES,\n",
    "#     PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "#     NUM_CHANNELS=NUM_CHANNELS,\n",
    "#     img_means=img_means,\n",
    "#     img_stds=img_stds,\n",
    "#     ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "#     model=model,\n",
    "#     BATCH_SIZE=batch_size,\n",
    "#     NUM_WORKERS=num_workers,\n",
    "#     model_name='RetinaNet',\n",
    "# )\n",
    "\n",
    "# del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RetinaNet Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory Inputs\n",
    "# SAVE_DIR = f'../runs/RetinaNet/RetinaNet_{MODEL_NAME}_teacher1'\n",
    "\n",
    "# # Privileged Information Paths\n",
    "# PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# # Special Constructed Features\n",
    "#     \"Box_Mask\",\n",
    "# ]\n",
    "\n",
    "# # Number of input image channels RGB + Extras\n",
    "# NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# # Defining Model\n",
    "\n",
    "# # Set the device to CUDA or CPU\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # Load the RetinaNet model with pretrained weights\n",
    "# weights = torchvision.models.detection.RetinaNet_ResNet50_FPN_Weights.DEFAULT\n",
    "# pre_trained_model = torchvision.models.detection.retinanet_resnet50_fpn(weights=weights)\n",
    "\n",
    "# model = pre_trained_model.to(device)\n",
    "\n",
    "# # Modify the first convolutional layer for Multi-channel input\n",
    "# model.backbone.body.conv1 = torch.nn.Conv2d(NUM_CHANNELS, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "# # Initialize the first convolutional layer's weights\n",
    "# torch.nn.init.kaiming_normal_(model.backbone.body.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# # Get the number of input features for the classification head\n",
    "# in_features = model.head.classification_head.cls_logits.in_channels\n",
    "# num_anchors = model.head.classification_head.num_anchors\n",
    "\n",
    "# # Modify classification head to match the number of classes for your task\n",
    "# # RetinaNetClassificationHead is redefined to include the correct number of classes\n",
    "# model.head.classification_head = RetinaNetClassificationHead(\n",
    "#     in_channels=in_features,\n",
    "#     num_classes=NUM_CLASSES,\n",
    "#     num_anchors=num_anchors,\n",
    "#     norm_layer=partial(torch.nn.GroupNorm, 32)\n",
    "# )\n",
    "\n",
    "# # Move the model to the correct device (e.g., CUDA or CPU)\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Verify the model structure\n",
    "# print(model)\n",
    "\n",
    "# import evaluate\n",
    "\n",
    "# evaluate.main_function(\n",
    "#     DIR_TEST=DIR_TEST,\n",
    "#     DIR_IMAGES=DIR_IMAGES,\n",
    "#     DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "#     IMG_RESIZE=IMG_RESIZE,\n",
    "#     SAVE_DIR=SAVE_DIR,\n",
    "#     CLASSES=CLASSES,\n",
    "#     NUM_CLASSES=NUM_CLASSES,\n",
    "#     PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "#     NUM_CHANNELS=NUM_CHANNELS,\n",
    "#     img_means=img_means,\n",
    "#     img_stds=img_stds,\n",
    "#     ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "#     model=model,\n",
    "#     BATCH_SIZE=batch_size,\n",
    "#     NUM_WORKERS=num_workers,\n",
    "#     model_name='RetinaNet',\n",
    "# )\n",
    "\n",
    "# del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RetinaNet Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory Inputs\n",
    "# SAVE_DIR = f'../runs/RetinaNet/RetinaNet_{MODEL_NAME}_student1'\n",
    "\n",
    "# # Privileged Information Paths\n",
    "# PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# # Special Constructed Features\n",
    "#     # \"Box_Mask\",\n",
    "# ]\n",
    "\n",
    "# # Number of input image channels RGB + Extras\n",
    "# NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# # Defining Student Model\n",
    "# # Set the device to CUDA or CPU\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # Load the RetinaNet model with pretrained weights\n",
    "# weights = torchvision.models.detection.RetinaNet_ResNet50_FPN_Weights.DEFAULT\n",
    "# pre_trained_model = torchvision.models.detection.retinanet_resnet50_fpn(weights=weights)\n",
    "\n",
    "# model = pre_trained_model.to(device)\n",
    "\n",
    "# # Get the number of input features for the classification head\n",
    "# in_features = model.head.classification_head.cls_logits.in_channels\n",
    "# num_anchors = model.head.classification_head.num_anchors\n",
    "\n",
    "# # Modify classification head to match the number of classes for your task\n",
    "# # RetinaNetClassificationHead is redefined to include the correct number of classes\n",
    "# model.head.classification_head = RetinaNetClassificationHead(\n",
    "#     in_channels=in_features,\n",
    "#     num_classes=NUM_CLASSES,\n",
    "#     num_anchors=num_anchors,\n",
    "#     norm_layer=partial(torch.nn.GroupNorm, 32)\n",
    "# )\n",
    "\n",
    "# # Move the model to the correct device (e.g., CUDA or CPU)\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Verify the model structure\n",
    "# print(model)\n",
    "\n",
    "# import evaluate\n",
    "\n",
    "# evaluate.main_function(\n",
    "#     DIR_TEST=DIR_TEST,\n",
    "#     DIR_IMAGES=DIR_IMAGES,\n",
    "#     DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "#     IMG_RESIZE=IMG_RESIZE,\n",
    "#     SAVE_DIR=SAVE_DIR,\n",
    "#     CLASSES=CLASSES,\n",
    "#     NUM_CLASSES=NUM_CLASSES,\n",
    "#     PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "#     NUM_CHANNELS=NUM_CHANNELS,\n",
    "#     img_means=img_means,\n",
    "#     img_stds=img_stds,\n",
    "#     ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "#     model=model,\n",
    "#     BATCH_SIZE=batch_size,\n",
    "#     NUM_WORKERS=num_workers,\n",
    "#     model_name='RetinaNet',\n",
    "# )\n",
    "\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory Inputs\n",
    "# SAVE_DIR = f'../runs/RetinaNet/RetinaNet_{MODEL_NAME}_student2'\n",
    "\n",
    "# # Privileged Information Paths\n",
    "# PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# # Special Constructed Features\n",
    "#     # \"Box_Mask\",\n",
    "# ]\n",
    "\n",
    "# # Number of input image channels RGB + Extras\n",
    "# NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# # Defining Student Model\n",
    "# # Set the device to CUDA or CPU\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # Load the RetinaNet model with pretrained weights\n",
    "# weights = torchvision.models.detection.RetinaNet_ResNet50_FPN_Weights.DEFAULT\n",
    "# pre_trained_model = torchvision.models.detection.retinanet_resnet50_fpn(weights=weights)\n",
    "\n",
    "# model = pre_trained_model.to(device)\n",
    "\n",
    "# # Get the number of input features for the classification head\n",
    "# in_features = model.head.classification_head.cls_logits.in_channels\n",
    "# num_anchors = model.head.classification_head.num_anchors\n",
    "\n",
    "# # Modify classification head to match the number of classes for your task\n",
    "# # RetinaNetClassificationHead is redefined to include the correct number of classes\n",
    "# model.head.classification_head = RetinaNetClassificationHead(\n",
    "#     in_channels=in_features,\n",
    "#     num_classes=NUM_CLASSES,\n",
    "#     num_anchors=num_anchors,\n",
    "#     norm_layer=partial(torch.nn.GroupNorm, 32)\n",
    "# )\n",
    "\n",
    "# # Move the model to the correct device (e.g., CUDA or CPU)\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Verify the model structure\n",
    "# print(model)\n",
    "\n",
    "# import evaluate\n",
    "\n",
    "# evaluate.main_function(\n",
    "#     DIR_TEST=DIR_TEST,\n",
    "#     DIR_IMAGES=DIR_IMAGES,\n",
    "#     DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "#     IMG_RESIZE=IMG_RESIZE,\n",
    "#     SAVE_DIR=SAVE_DIR,\n",
    "#     CLASSES=CLASSES,\n",
    "#     NUM_CLASSES=NUM_CLASSES,\n",
    "#     PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "#     NUM_CHANNELS=NUM_CHANNELS,\n",
    "#     img_means=img_means,\n",
    "#     img_stds=img_stds,\n",
    "#     ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "#     model=model,\n",
    "#     BATCH_SIZE=batch_size,\n",
    "#     NUM_WORKERS=num_workers,\n",
    "#     model_name='RetinaNet',\n",
    "# )\n",
    "\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory Inputs\n",
    "SAVE_DIR = f'../runs/RetinaNet/RetinaNet_{MODEL_NAME}_student3'\n",
    "\n",
    "# Privileged Information Paths\n",
    "PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# Special Constructed Features\n",
    "    # \"Box_Mask\",\n",
    "]\n",
    "\n",
    "# Number of input image channels RGB + Extras\n",
    "NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# Defining Student Model\n",
    "# Set the device to CUDA or CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Load the RetinaNet model with pretrained weights\n",
    "weights = torchvision.models.detection.RetinaNet_ResNet50_FPN_Weights.DEFAULT\n",
    "pre_trained_model = torchvision.models.detection.retinanet_resnet50_fpn(weights=weights)\n",
    "\n",
    "model = pre_trained_model.to(device)\n",
    "\n",
    "# Get the number of input features for the classification head\n",
    "in_features = model.head.classification_head.cls_logits.in_channels\n",
    "num_anchors = model.head.classification_head.num_anchors\n",
    "\n",
    "# Modify classification head to match the number of classes for your task\n",
    "# RetinaNetClassificationHead is redefined to include the correct number of classes\n",
    "model.head.classification_head = RetinaNetClassificationHead(\n",
    "    in_channels=in_features,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_anchors=num_anchors,\n",
    "    norm_layer=partial(torch.nn.GroupNorm, 32)\n",
    ")\n",
    "\n",
    "# Move the model to the correct device (e.g., CUDA or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Verify the model structure\n",
    "print(model)\n",
    "\n",
    "import evaluate\n",
    "\n",
    "evaluate.main_function(\n",
    "    DIR_TEST=DIR_TEST,\n",
    "    DIR_IMAGES=DIR_IMAGES,\n",
    "    DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "    IMG_RESIZE=IMG_RESIZE,\n",
    "    SAVE_DIR=SAVE_DIR,\n",
    "    CLASSES=CLASSES,\n",
    "    NUM_CLASSES=NUM_CLASSES,\n",
    "    PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "    NUM_CHANNELS=NUM_CHANNELS,\n",
    "    img_means=img_means,\n",
    "    img_stds=img_stds,\n",
    "    ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "    model=model,\n",
    "    BATCH_SIZE=batch_size,\n",
    "    NUM_WORKERS=num_workers,\n",
    "    model_name='RetinaNet',\n",
    ")\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory Inputs\n",
    "SAVE_DIR = f'../runs/RetinaNet/RetinaNet_{MODEL_NAME}_student4'\n",
    "\n",
    "# Privileged Information Paths\n",
    "PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# Special Constructed Features\n",
    "    # \"Box_Mask\",\n",
    "]\n",
    "\n",
    "# Number of input image channels RGB + Extras\n",
    "NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# Defining Student Model\n",
    "# Set the device to CUDA or CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Load the RetinaNet model with pretrained weights\n",
    "weights = torchvision.models.detection.RetinaNet_ResNet50_FPN_Weights.DEFAULT\n",
    "pre_trained_model = torchvision.models.detection.retinanet_resnet50_fpn(weights=weights)\n",
    "\n",
    "model = pre_trained_model.to(device)\n",
    "\n",
    "# Get the number of input features for the classification head\n",
    "in_features = model.head.classification_head.cls_logits.in_channels\n",
    "num_anchors = model.head.classification_head.num_anchors\n",
    "\n",
    "# Modify classification head to match the number of classes for your task\n",
    "# RetinaNetClassificationHead is redefined to include the correct number of classes\n",
    "model.head.classification_head = RetinaNetClassificationHead(\n",
    "    in_channels=in_features,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_anchors=num_anchors,\n",
    "    norm_layer=partial(torch.nn.GroupNorm, 32)\n",
    ")\n",
    "\n",
    "# Move the model to the correct device (e.g., CUDA or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Verify the model structure\n",
    "print(model)\n",
    "\n",
    "import evaluate\n",
    "\n",
    "evaluate.main_function(\n",
    "    DIR_TEST=DIR_TEST,\n",
    "    DIR_IMAGES=DIR_IMAGES,\n",
    "    DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "    IMG_RESIZE=IMG_RESIZE,\n",
    "    SAVE_DIR=SAVE_DIR,\n",
    "    CLASSES=CLASSES,\n",
    "    NUM_CLASSES=NUM_CLASSES,\n",
    "    PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "    NUM_CHANNELS=NUM_CHANNELS,\n",
    "    img_means=img_means,\n",
    "    img_stds=img_stds,\n",
    "    ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "    model=model,\n",
    "    BATCH_SIZE=batch_size,\n",
    "    NUM_WORKERS=num_workers,\n",
    "    model_name='RetinaNet',\n",
    ")\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FCOS Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory Inputs\n",
    "# SAVE_DIR = f'../runs/FCOS/FCOS_{MODEL_NAME}_baseline1'\n",
    "\n",
    "# # Privileged Information Paths\n",
    "# PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# # Special Constructed Features\n",
    "#     # \"Box_Mask\",\n",
    "# ]\n",
    "\n",
    "# # Number of input image channels RGB + Extras\n",
    "# NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# # Defining Model\n",
    "\n",
    "# # Set the device to CUDA or CPU\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # Load the FCOS model with pretrained weights\n",
    "# weights = torchvision.models.detection.FCOS_ResNet50_FPN_Weights.DEFAULT\n",
    "# pre_trained_model = torchvision.models.detection.fcos_resnet50_fpn(weights=weights)\n",
    "\n",
    "# model = pre_trained_model.to(device)\n",
    "\n",
    "# # Modify the first convolutional layer for 4-channel input\n",
    "# model.backbone.body.conv1 = torch.nn.Conv2d(NUM_CHANNELS, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "# # Initialize the first convolutional layer's weights\n",
    "# torch.nn.init.kaiming_normal_(model.backbone.body.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# # Get the correct number of input features for the classifier\n",
    "# # Get the number of input channels from the classification head\n",
    "# in_features = model.head.classification_head.cls_logits.in_channels\n",
    "# num_anchors = model.head.classification_head.num_anchors\n",
    "\n",
    "# # Modify classification head to match the number of classes for your task\n",
    "# # FCOSClassificationHead is redefined to include the correct number of classes\n",
    "# model.head.classification_head = FCOSClassificationHead(\n",
    "#     in_channels=in_features,\n",
    "#     num_classes=NUM_CLASSES,\n",
    "#     num_anchors=num_anchors,\n",
    "#     norm_layer=partial(torch.nn.GroupNorm, 32)\n",
    "# )\n",
    "\n",
    "# # Move the model to the correct device (e.g., CUDA or CPU)\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Verify the model structure\n",
    "# print(model)\n",
    "\n",
    "# import evaluate\n",
    "\n",
    "# evaluate.main_function(\n",
    "#     DIR_TEST=DIR_TEST,\n",
    "#     DIR_IMAGES=DIR_IMAGES,\n",
    "#     DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "#     IMG_RESIZE=IMG_RESIZE,\n",
    "#     SAVE_DIR=SAVE_DIR,\n",
    "#     CLASSES=CLASSES,\n",
    "#     NUM_CLASSES=NUM_CLASSES,\n",
    "#     PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "#     NUM_CHANNELS=NUM_CHANNELS,\n",
    "#     img_means=img_means,\n",
    "#     img_stds=img_stds,\n",
    "#     ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "#     model=model,\n",
    "#     BATCH_SIZE=batch_size,\n",
    "#     NUM_WORKERS=num_workers,\n",
    "#     model_name='FCOS',\n",
    "# )\n",
    "\n",
    "# del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FCOS Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory Inputs\n",
    "# SAVE_DIR = f'../runs/FCOS/FCOS_{MODEL_NAME}_teacher1'\n",
    "\n",
    "# # Privileged Information Paths\n",
    "# PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# # Special Constructed Features\n",
    "#     \"Box_Mask\",\n",
    "# ]\n",
    "\n",
    "# # Number of input image channels RGB + Extras\n",
    "# NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# # Defining Model\n",
    "\n",
    "# # Set the device to CUDA or CPU\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # Load the FCOS model with pretrained weights\n",
    "# weights = torchvision.models.detection.FCOS_ResNet50_FPN_Weights.DEFAULT\n",
    "# pre_trained_model = torchvision.models.detection.fcos_resnet50_fpn(weights=weights)\n",
    "\n",
    "# model = pre_trained_model.to(device)\n",
    "\n",
    "# # Modify the first convolutional layer for 4-channel input\n",
    "# model.backbone.body.conv1 = torch.nn.Conv2d(NUM_CHANNELS, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "# # Initialize the first convolutional layer's weights\n",
    "# torch.nn.init.kaiming_normal_(model.backbone.body.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# # Get the correct number of input features for the classifier\n",
    "# # Get the number of input channels from the classification head\n",
    "# in_features = model.head.classification_head.cls_logits.in_channels\n",
    "# num_anchors = model.head.classification_head.num_anchors\n",
    "\n",
    "# # Modify classification head to match the number of classes for your task\n",
    "# # FCOSClassificationHead is redefined to include the correct number of classes\n",
    "# model.head.classification_head = FCOSClassificationHead(\n",
    "#     in_channels=in_features,\n",
    "#     num_classes=NUM_CLASSES,\n",
    "#     num_anchors=num_anchors,\n",
    "#     norm_layer=partial(torch.nn.GroupNorm, 32)\n",
    "# )\n",
    "\n",
    "# # Move the model to the correct device (e.g., CUDA or CPU)\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Verify the model structure\n",
    "# print(model)\n",
    "\n",
    "# import evaluate\n",
    "\n",
    "# evaluate.main_function(\n",
    "#     DIR_TEST=DIR_TEST,\n",
    "#     DIR_IMAGES=DIR_IMAGES,\n",
    "#     DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "#     IMG_RESIZE=IMG_RESIZE,\n",
    "#     SAVE_DIR=SAVE_DIR,\n",
    "#     CLASSES=CLASSES,\n",
    "#     NUM_CLASSES=NUM_CLASSES,\n",
    "#     PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "#     NUM_CHANNELS=NUM_CHANNELS,\n",
    "#     img_means=img_means,\n",
    "#     img_stds=img_stds,\n",
    "#     ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "#     model=model,\n",
    "#     BATCH_SIZE=batch_size,\n",
    "#     NUM_WORKERS=num_workers,\n",
    "#     model_name='FCOS',\n",
    "# )\n",
    "\n",
    "# del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FCOS Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory Inputs\n",
    "# SAVE_DIR = f'../runs/FCOS/FCOS_{MODEL_NAME}_student1'\n",
    "\n",
    "# # Privileged Information Paths\n",
    "# PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# # Special Constructed Features\n",
    "#     # \"Box_Mask\",\n",
    "# ]\n",
    "\n",
    "# # Number of input image channels RGB + Extras\n",
    "# NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# # Defining Model\n",
    "\n",
    "# # Set the device to CUDA or CPU\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # Load the FCOS model with pretrained weights\n",
    "# weights = torchvision.models.detection.FCOS_ResNet50_FPN_Weights.DEFAULT\n",
    "# pre_trained_model = torchvision.models.detection.fcos_resnet50_fpn(weights=weights)\n",
    "\n",
    "# model = pre_trained_model.to(device)\n",
    "\n",
    "# # Modify the first convolutional layer for 4-channel input\n",
    "# model.backbone.body.conv1 = torch.nn.Conv2d(NUM_CHANNELS, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "# # Initialize the first convolutional layer's weights\n",
    "# torch.nn.init.kaiming_normal_(model.backbone.body.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# # Get the correct number of input features for the classifier\n",
    "# # Get the number of input channels from the classification head\n",
    "# in_features = model.head.classification_head.cls_logits.in_channels\n",
    "# num_anchors = model.head.classification_head.num_anchors\n",
    "\n",
    "# # Modify classification head to match the number of classes for your task\n",
    "# # FCOSClassificationHead is redefined to include the correct number of classes\n",
    "# model.head.classification_head = FCOSClassificationHead(\n",
    "#     in_channels=in_features,\n",
    "#     num_classes=NUM_CLASSES,\n",
    "#     num_anchors=num_anchors,\n",
    "#     norm_layer=partial(torch.nn.GroupNorm, 32)\n",
    "# )\n",
    "\n",
    "# # Move the model to the correct device (e.g., CUDA or CPU)\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Verify the model structure\n",
    "# print(model)\n",
    "\n",
    "# import evaluate\n",
    "\n",
    "# evaluate.main_function(\n",
    "#     DIR_TEST=DIR_TEST,\n",
    "#     DIR_IMAGES=DIR_IMAGES,\n",
    "#     DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "#     IMG_RESIZE=IMG_RESIZE,\n",
    "#     SAVE_DIR=SAVE_DIR,\n",
    "#     CLASSES=CLASSES,\n",
    "#     NUM_CLASSES=NUM_CLASSES,\n",
    "#     PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "#     NUM_CHANNELS=NUM_CHANNELS,\n",
    "#     img_means=img_means,\n",
    "#     img_stds=img_stds,\n",
    "#     ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "#     model=model,\n",
    "#     BATCH_SIZE=batch_size,\n",
    "#     NUM_WORKERS=num_workers,\n",
    "#     model_name='FCOS',\n",
    "# )\n",
    "\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory Inputs\n",
    "# SAVE_DIR = f'../runs/FCOS/FCOS_{MODEL_NAME}_student2'\n",
    "\n",
    "# # Privileged Information Paths\n",
    "# PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# # Special Constructed Features\n",
    "#     # \"Box_Mask\",\n",
    "# ]\n",
    "\n",
    "# # Number of input image channels RGB + Extras\n",
    "# NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# # Defining Model\n",
    "\n",
    "# # Set the device to CUDA or CPU\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # Load the FCOS model with pretrained weights\n",
    "# weights = torchvision.models.detection.FCOS_ResNet50_FPN_Weights.DEFAULT\n",
    "# pre_trained_model = torchvision.models.detection.fcos_resnet50_fpn(weights=weights)\n",
    "\n",
    "# model = pre_trained_model.to(device)\n",
    "\n",
    "# # Modify the first convolutional layer for 4-channel input\n",
    "# model.backbone.body.conv1 = torch.nn.Conv2d(NUM_CHANNELS, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "# # Initialize the first convolutional layer's weights\n",
    "# torch.nn.init.kaiming_normal_(model.backbone.body.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# # Get the correct number of input features for the classifier\n",
    "# # Get the number of input channels from the classification head\n",
    "# in_features = model.head.classification_head.cls_logits.in_channels\n",
    "# num_anchors = model.head.classification_head.num_anchors\n",
    "\n",
    "# # Modify classification head to match the number of classes for your task\n",
    "# # FCOSClassificationHead is redefined to include the correct number of classes\n",
    "# model.head.classification_head = FCOSClassificationHead(\n",
    "#     in_channels=in_features,\n",
    "#     num_classes=NUM_CLASSES,\n",
    "#     num_anchors=num_anchors,\n",
    "#     norm_layer=partial(torch.nn.GroupNorm, 32)\n",
    "# )\n",
    "\n",
    "# # Move the model to the correct device (e.g., CUDA or CPU)\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Verify the model structure\n",
    "# print(model)\n",
    "\n",
    "# import evaluate\n",
    "\n",
    "# evaluate.main_function(\n",
    "#     DIR_TEST=DIR_TEST,\n",
    "#     DIR_IMAGES=DIR_IMAGES,\n",
    "#     DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "#     IMG_RESIZE=IMG_RESIZE,\n",
    "#     SAVE_DIR=SAVE_DIR,\n",
    "#     CLASSES=CLASSES,\n",
    "#     NUM_CLASSES=NUM_CLASSES,\n",
    "#     PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "#     NUM_CHANNELS=NUM_CHANNELS,\n",
    "#     img_means=img_means,\n",
    "#     img_stds=img_stds,\n",
    "#     ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "#     model=model,\n",
    "#     BATCH_SIZE=batch_size,\n",
    "#     NUM_WORKERS=num_workers,\n",
    "#     model_name='FCOS',\n",
    "# )\n",
    "\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory Inputs\n",
    "SAVE_DIR = f'../runs/FCOS/FCOS_{MODEL_NAME}_student3'\n",
    "\n",
    "# Privileged Information Paths\n",
    "PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# Special Constructed Features\n",
    "    # \"Box_Mask\",\n",
    "]\n",
    "\n",
    "# Number of input image channels RGB + Extras\n",
    "NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# Defining Model\n",
    "\n",
    "# Set the device to CUDA or CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Load the FCOS model with pretrained weights\n",
    "weights = torchvision.models.detection.FCOS_ResNet50_FPN_Weights.DEFAULT\n",
    "pre_trained_model = torchvision.models.detection.fcos_resnet50_fpn(weights=weights)\n",
    "\n",
    "model = pre_trained_model.to(device)\n",
    "\n",
    "# Modify the first convolutional layer for 4-channel input\n",
    "model.backbone.body.conv1 = torch.nn.Conv2d(NUM_CHANNELS, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "# Initialize the first convolutional layer's weights\n",
    "torch.nn.init.kaiming_normal_(model.backbone.body.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# Get the correct number of input features for the classifier\n",
    "# Get the number of input channels from the classification head\n",
    "in_features = model.head.classification_head.cls_logits.in_channels\n",
    "num_anchors = model.head.classification_head.num_anchors\n",
    "\n",
    "# Modify classification head to match the number of classes for your task\n",
    "# FCOSClassificationHead is redefined to include the correct number of classes\n",
    "model.head.classification_head = FCOSClassificationHead(\n",
    "    in_channels=in_features,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_anchors=num_anchors,\n",
    "    norm_layer=partial(torch.nn.GroupNorm, 32)\n",
    ")\n",
    "\n",
    "# Move the model to the correct device (e.g., CUDA or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Verify the model structure\n",
    "print(model)\n",
    "\n",
    "import evaluate\n",
    "\n",
    "evaluate.main_function(\n",
    "    DIR_TEST=DIR_TEST,\n",
    "    DIR_IMAGES=DIR_IMAGES,\n",
    "    DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "    IMG_RESIZE=IMG_RESIZE,\n",
    "    SAVE_DIR=SAVE_DIR,\n",
    "    CLASSES=CLASSES,\n",
    "    NUM_CLASSES=NUM_CLASSES,\n",
    "    PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "    NUM_CHANNELS=NUM_CHANNELS,\n",
    "    img_means=img_means,\n",
    "    img_stds=img_stds,\n",
    "    ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "    model=model,\n",
    "    BATCH_SIZE=batch_size,\n",
    "    NUM_WORKERS=num_workers,\n",
    "    model_name='FCOS',\n",
    ")\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory Inputs\n",
    "SAVE_DIR = f'../runs/FCOS/FCOS_{MODEL_NAME}_student4'\n",
    "\n",
    "# Privileged Information Paths\n",
    "PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# Special Constructed Features\n",
    "    # \"Box_Mask\",\n",
    "]\n",
    "\n",
    "# Number of input image channels RGB + Extras\n",
    "NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# Defining Model\n",
    "\n",
    "# Set the device to CUDA or CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Load the FCOS model with pretrained weights\n",
    "weights = torchvision.models.detection.FCOS_ResNet50_FPN_Weights.DEFAULT\n",
    "pre_trained_model = torchvision.models.detection.fcos_resnet50_fpn(weights=weights)\n",
    "\n",
    "model = pre_trained_model.to(device)\n",
    "\n",
    "# Modify the first convolutional layer for 4-channel input\n",
    "model.backbone.body.conv1 = torch.nn.Conv2d(NUM_CHANNELS, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "# Initialize the first convolutional layer's weights\n",
    "torch.nn.init.kaiming_normal_(model.backbone.body.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# Get the correct number of input features for the classifier\n",
    "# Get the number of input channels from the classification head\n",
    "in_features = model.head.classification_head.cls_logits.in_channels\n",
    "num_anchors = model.head.classification_head.num_anchors\n",
    "\n",
    "# Modify classification head to match the number of classes for your task\n",
    "# FCOSClassificationHead is redefined to include the correct number of classes\n",
    "model.head.classification_head = FCOSClassificationHead(\n",
    "    in_channels=in_features,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_anchors=num_anchors,\n",
    "    norm_layer=partial(torch.nn.GroupNorm, 32)\n",
    ")\n",
    "\n",
    "# Move the model to the correct device (e.g., CUDA or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Verify the model structure\n",
    "print(model)\n",
    "\n",
    "import evaluate\n",
    "\n",
    "evaluate.main_function(\n",
    "    DIR_TEST=DIR_TEST,\n",
    "    DIR_IMAGES=DIR_IMAGES,\n",
    "    DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "    IMG_RESIZE=IMG_RESIZE,\n",
    "    SAVE_DIR=SAVE_DIR,\n",
    "    CLASSES=CLASSES,\n",
    "    NUM_CLASSES=NUM_CLASSES,\n",
    "    PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "    NUM_CHANNELS=NUM_CHANNELS,\n",
    "    img_means=img_means,\n",
    "    img_stds=img_stds,\n",
    "    ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "    model=model,\n",
    "    BATCH_SIZE=batch_size,\n",
    "    NUM_WORKERS=num_workers,\n",
    "    model_name='FCOS',\n",
    ")\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faster R-CNN Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory Inputs\n",
    "# SAVE_DIR = f'../runs/FasterRCNN/FasterRCNN_{MODEL_NAME}_baseline1'\n",
    "\n",
    "# # Privileged Information Paths\n",
    "# PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# # Special Constructed Features\n",
    "#     # \"Box_Mask\",\n",
    "# ]\n",
    "\n",
    "# # Number of input image channels RGB + Extras\n",
    "# NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# # Defining Model\n",
    "\n",
    "# # Set the device to CUDA or CPU\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # Load the FasterRCNN model with pretrained weights\n",
    "# weights = torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "# pre_trained_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights)\n",
    "\n",
    "# model = pre_trained_model.to(device)\n",
    "\n",
    "# # Modify the first convolutional layer for 4-channel input\n",
    "# model.backbone.body.conv1 = torch.nn.Conv2d(NUM_CHANNELS, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False).to(device)\n",
    "\n",
    "# # Initialize the first convolutional layer's weights (was not working with the default initialization)\n",
    "# torch.nn.init.kaiming_normal_(model.backbone.body.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# # Get number of input features for the classifier\n",
    "# in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# # Replace the pre-trained head with a new one\n",
    "# model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES).to(device)\n",
    "# # norm_layer=partial(torch.nn.GroupNorm, 32),  # Normalization layer\n",
    "\n",
    "# # Move the model to the correct device (e.g., CUDA or CPU)\n",
    "# model = model.to(device)\n",
    "# # These don't seem to be necessary, but are included for completeness\n",
    "# model.to(device)\n",
    "# model.backbone.body.conv1.to(device)\n",
    "# model.rpn.to(device)\n",
    "# model.roi_heads.to(device)\n",
    "\n",
    "# # Verify the model structure\n",
    "# print(model)\n",
    "\n",
    "# import evaluate\n",
    "\n",
    "# evaluate.main_function(\n",
    "#     DIR_TEST=DIR_TEST,\n",
    "#     DIR_IMAGES=DIR_IMAGES,\n",
    "#     DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "#     IMG_RESIZE=IMG_RESIZE,\n",
    "#     SAVE_DIR=SAVE_DIR,\n",
    "#     CLASSES=CLASSES,\n",
    "#     NUM_CLASSES=NUM_CLASSES,\n",
    "#     PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "#     NUM_CHANNELS=NUM_CHANNELS,\n",
    "#     img_means=img_means,\n",
    "#     img_stds=img_stds,\n",
    "#     ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "#     model=model,\n",
    "#     BATCH_SIZE=batch_size,\n",
    "#     NUM_WORKERS=num_workers,\n",
    "#     model_name='Faster R-CNN',\n",
    "# )\n",
    "\n",
    "# del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faster R-CNN Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory Inputs\n",
    "# SAVE_DIR = f'../runs/FasterRCNN/FasterRCNN_{MODEL_NAME}_teacher1'\n",
    "\n",
    "# # Privileged Information Paths\n",
    "# PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# # Special Constructed Features\n",
    "#     \"Box_Mask\",\n",
    "# ]\n",
    "\n",
    "# # Number of input image channels RGB + Extras\n",
    "# NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# # Defining Model\n",
    "\n",
    "# # Set the device to CUDA or CPU\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # Load the FasterRCNN model with pretrained weights\n",
    "# weights = torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "# pre_trained_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights)\n",
    "\n",
    "# model = pre_trained_model.to(device)\n",
    "\n",
    "# # Modify the first convolutional layer for 4-channel input\n",
    "# model.backbone.body.conv1 = torch.nn.Conv2d(NUM_CHANNELS, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False).to(device)\n",
    "\n",
    "# # Initialize the first convolutional layer's weights (was not working with the default initialization)\n",
    "# torch.nn.init.kaiming_normal_(model.backbone.body.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# # Get number of input features for the classifier\n",
    "# in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# # Replace the pre-trained head with a new one\n",
    "# model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES).to(device)\n",
    "# # norm_layer=partial(torch.nn.GroupNorm, 32),  # Normalization layer\n",
    "\n",
    "# # Move the model to the correct device (e.g., CUDA or CPU)\n",
    "# model = model.to(device)\n",
    "# # These don't seem to be necessary, but are included for completeness\n",
    "# model.to(device)\n",
    "# model.backbone.body.conv1.to(device)\n",
    "# model.rpn.to(device)\n",
    "# model.roi_heads.to(device)\n",
    "\n",
    "# # Verify the model structure\n",
    "# print(model)\n",
    "\n",
    "# import evaluate\n",
    "\n",
    "# evaluate.main_function(\n",
    "#     DIR_TEST=DIR_TEST,\n",
    "#     DIR_IMAGES=DIR_IMAGES,\n",
    "#     DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "#     IMG_RESIZE=IMG_RESIZE,\n",
    "#     SAVE_DIR=SAVE_DIR,\n",
    "#     CLASSES=CLASSES,\n",
    "#     NUM_CLASSES=NUM_CLASSES,\n",
    "#     PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "#     NUM_CHANNELS=NUM_CHANNELS,\n",
    "#     img_means=img_means,\n",
    "#     img_stds=img_stds,\n",
    "#     ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "#     model=model,\n",
    "#     BATCH_SIZE=batch_size,\n",
    "#     NUM_WORKERS=num_workers,\n",
    "#     model_name='Faster R-CNN',\n",
    "# )\n",
    "\n",
    "# del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faster R-CNN Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory Inputs\n",
    "# SAVE_DIR = f'../runs/FasterRCNN/FasterRCNN_{MODEL_NAME}_student1'\n",
    "\n",
    "# # Privileged Information Paths\n",
    "# PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# # Special Constructed Features\n",
    "#     # \"Box_Mask\",\n",
    "# ]\n",
    "\n",
    "# # Number of input image channels RGB + Extras\n",
    "# NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# # Defining Model\n",
    "\n",
    "# # Set the device to CUDA or CPU\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # Load the FasterRCNN model with pretrained weights\n",
    "# weights = torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "# pre_trained_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights)\n",
    "\n",
    "# model = pre_trained_model.to(device)\n",
    "\n",
    "# # Modify the first convolutional layer for 4-channel input\n",
    "# model.backbone.body.conv1 = torch.nn.Conv2d(NUM_CHANNELS, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False).to(device)\n",
    "\n",
    "# # Initialize the first convolutional layer's weights (was not working with the default initialization)\n",
    "# torch.nn.init.kaiming_normal_(model.backbone.body.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# # Get number of input features for the classifier\n",
    "# in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# # Replace the pre-trained head with a new one\n",
    "# model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES).to(device)\n",
    "# # norm_layer=partial(torch.nn.GroupNorm, 32),  # Normalization layer\n",
    "\n",
    "# # Move the model to the correct device (e.g., CUDA or CPU)\n",
    "# model = model.to(device)\n",
    "# # These don't seem to be necessary, but are included for completeness\n",
    "# model.to(device)\n",
    "# model.backbone.body.conv1.to(device)\n",
    "# model.rpn.to(device)\n",
    "# model.roi_heads.to(device)\n",
    "\n",
    "# # Verify the model structure\n",
    "# print(model)\n",
    "\n",
    "# import evaluate\n",
    "\n",
    "# evaluate.main_function(\n",
    "#     DIR_TEST=DIR_TEST,\n",
    "#     DIR_IMAGES=DIR_IMAGES,\n",
    "#     DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "#     IMG_RESIZE=IMG_RESIZE,\n",
    "#     SAVE_DIR=SAVE_DIR,\n",
    "#     CLASSES=CLASSES,\n",
    "#     NUM_CLASSES=NUM_CLASSES,\n",
    "#     PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "#     NUM_CHANNELS=NUM_CHANNELS,\n",
    "#     img_means=img_means,\n",
    "#     img_stds=img_stds,\n",
    "#     ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "#     model=model,\n",
    "#     BATCH_SIZE=batch_size,\n",
    "#     NUM_WORKERS=num_workers,\n",
    "#     model_name='Faster R-CNN',\n",
    "# )\n",
    "\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory Inputs\n",
    "# SAVE_DIR = f'../runs/FasterRCNN/FasterRCNN_{MODEL_NAME}_student2'\n",
    "\n",
    "# # Privileged Information Paths\n",
    "# PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# # Special Constructed Features\n",
    "#     # \"Box_Mask\",\n",
    "# ]\n",
    "\n",
    "# # Number of input image channels RGB + Extras\n",
    "# NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# # Defining Model\n",
    "\n",
    "# # Set the device to CUDA or CPU\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # Load the FasterRCNN model with pretrained weights\n",
    "# weights = torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "# pre_trained_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights)\n",
    "\n",
    "# model = pre_trained_model.to(device)\n",
    "\n",
    "# # Modify the first convolutional layer for 4-channel input\n",
    "# model.backbone.body.conv1 = torch.nn.Conv2d(NUM_CHANNELS, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False).to(device)\n",
    "\n",
    "# # Initialize the first convolutional layer's weights (was not working with the default initialization)\n",
    "# torch.nn.init.kaiming_normal_(model.backbone.body.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# # Get number of input features for the classifier\n",
    "# in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# # Replace the pre-trained head with a new one\n",
    "# model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES).to(device)\n",
    "# # norm_layer=partial(torch.nn.GroupNorm, 32),  # Normalization layer\n",
    "\n",
    "# # Move the model to the correct device (e.g., CUDA or CPU)\n",
    "# model = model.to(device)\n",
    "# # These don't seem to be necessary, but are included for completeness\n",
    "# model.to(device)\n",
    "# model.backbone.body.conv1.to(device)\n",
    "# model.rpn.to(device)\n",
    "# model.roi_heads.to(device)\n",
    "\n",
    "# # Verify the model structure\n",
    "# print(model)\n",
    "\n",
    "# import evaluate\n",
    "\n",
    "# evaluate.main_function(\n",
    "#     DIR_TEST=DIR_TEST,\n",
    "#     DIR_IMAGES=DIR_IMAGES,\n",
    "#     DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "#     IMG_RESIZE=IMG_RESIZE,\n",
    "#     SAVE_DIR=SAVE_DIR,\n",
    "#     CLASSES=CLASSES,\n",
    "#     NUM_CLASSES=NUM_CLASSES,\n",
    "#     PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "#     NUM_CHANNELS=NUM_CHANNELS,\n",
    "#     img_means=img_means,\n",
    "#     img_stds=img_stds,\n",
    "#     ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "#     model=model,\n",
    "#     BATCH_SIZE=batch_size,\n",
    "#     NUM_WORKERS=num_workers,\n",
    "#     model_name='Faster R-CNN',\n",
    "# )\n",
    "\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory Inputs\n",
    "SAVE_DIR = f'../runs/FasterRCNN/FasterRCNN_{MODEL_NAME}_student3'\n",
    "\n",
    "# Privileged Information Paths\n",
    "PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# Special Constructed Features\n",
    "    # \"Box_Mask\",\n",
    "]\n",
    "\n",
    "# Number of input image channels RGB + Extras\n",
    "NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# Defining Model\n",
    "\n",
    "# Set the device to CUDA or CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Load the FasterRCNN model with pretrained weights\n",
    "weights = torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "pre_trained_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights)\n",
    "\n",
    "model = pre_trained_model.to(device)\n",
    "\n",
    "# Modify the first convolutional layer for 4-channel input\n",
    "model.backbone.body.conv1 = torch.nn.Conv2d(NUM_CHANNELS, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False).to(device)\n",
    "\n",
    "# Initialize the first convolutional layer's weights (was not working with the default initialization)\n",
    "torch.nn.init.kaiming_normal_(model.backbone.body.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# Get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# Replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES).to(device)\n",
    "# norm_layer=partial(torch.nn.GroupNorm, 32),  # Normalization layer\n",
    "\n",
    "# Move the model to the correct device (e.g., CUDA or CPU)\n",
    "model = model.to(device)\n",
    "# These don't seem to be necessary, but are included for completeness\n",
    "model.to(device)\n",
    "model.backbone.body.conv1.to(device)\n",
    "model.rpn.to(device)\n",
    "model.roi_heads.to(device)\n",
    "\n",
    "# Verify the model structure\n",
    "print(model)\n",
    "\n",
    "import evaluate\n",
    "\n",
    "evaluate.main_function(\n",
    "    DIR_TEST=DIR_TEST,\n",
    "    DIR_IMAGES=DIR_IMAGES,\n",
    "    DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "    IMG_RESIZE=IMG_RESIZE,\n",
    "    SAVE_DIR=SAVE_DIR,\n",
    "    CLASSES=CLASSES,\n",
    "    NUM_CLASSES=NUM_CLASSES,\n",
    "    PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "    NUM_CHANNELS=NUM_CHANNELS,\n",
    "    img_means=img_means,\n",
    "    img_stds=img_stds,\n",
    "    ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "    model=model,\n",
    "    BATCH_SIZE=batch_size,\n",
    "    NUM_WORKERS=num_workers,\n",
    "    model_name='Faster R-CNN',\n",
    ")\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory Inputs\n",
    "SAVE_DIR = f'../runs/FasterRCNN/FasterRCNN_{MODEL_NAME}_student4'\n",
    "\n",
    "# Privileged Information Paths\n",
    "PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# Special Constructed Features\n",
    "    # \"Box_Mask\",\n",
    "]\n",
    "\n",
    "# Number of input image channels RGB + Extras\n",
    "NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# Defining Model\n",
    "\n",
    "# Set the device to CUDA or CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Load the FasterRCNN model with pretrained weights\n",
    "weights = torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "pre_trained_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights)\n",
    "\n",
    "model = pre_trained_model.to(device)\n",
    "\n",
    "# Modify the first convolutional layer for 4-channel input\n",
    "model.backbone.body.conv1 = torch.nn.Conv2d(NUM_CHANNELS, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False).to(device)\n",
    "\n",
    "# Initialize the first convolutional layer's weights (was not working with the default initialization)\n",
    "torch.nn.init.kaiming_normal_(model.backbone.body.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# Get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# Replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES).to(device)\n",
    "# norm_layer=partial(torch.nn.GroupNorm, 32),  # Normalization layer\n",
    "\n",
    "# Move the model to the correct device (e.g., CUDA or CPU)\n",
    "model = model.to(device)\n",
    "# These don't seem to be necessary, but are included for completeness\n",
    "model.to(device)\n",
    "model.backbone.body.conv1.to(device)\n",
    "model.rpn.to(device)\n",
    "model.roi_heads.to(device)\n",
    "\n",
    "# Verify the model structure\n",
    "print(model)\n",
    "\n",
    "import evaluate\n",
    "\n",
    "evaluate.main_function(\n",
    "    DIR_TEST=DIR_TEST,\n",
    "    DIR_IMAGES=DIR_IMAGES,\n",
    "    DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "    IMG_RESIZE=IMG_RESIZE,\n",
    "    SAVE_DIR=SAVE_DIR,\n",
    "    CLASSES=CLASSES,\n",
    "    NUM_CLASSES=NUM_CLASSES,\n",
    "    PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "    NUM_CHANNELS=NUM_CHANNELS,\n",
    "    img_means=img_means,\n",
    "    img_stds=img_stds,\n",
    "    ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "    model=model,\n",
    "    BATCH_SIZE=batch_size,\n",
    "    NUM_WORKERS=num_workers,\n",
    "    model_name='Faster R-CNN',\n",
    ")\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SSD Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory Inputs\n",
    "# SAVE_DIR = f'../runs/SSD/SSD_{MODEL_NAME}_baseline1'\n",
    "\n",
    "# # Privileged Information Paths\n",
    "# PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# # Special Constructed Features\n",
    "#     # \"Box_Mask\",\n",
    "# ]\n",
    "\n",
    "# # Number of input image channels RGB + Extras\n",
    "# NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# # Defining Model\n",
    "\n",
    "# # Set the device to CUDA or CPU\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # Load the pretrained SSD model (standard SSD)\n",
    "# weights = torchvision.models.detection.SSD300_VGG16_Weights.DEFAULT\n",
    "# pre_trained_model = torchvision.models.detection.ssd300_vgg16(weights=weights)\n",
    "\n",
    "# model = pre_trained_model.to(device)\n",
    "\n",
    "# # Modify the first convolutional layer for 4-channel input\n",
    "# # In SSD, the input convolution layer is part of the VGG model's backbone\n",
    "# model.backbone.features[0] = torch.nn.Conv2d(NUM_CHANNELS, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "# # Initialize the first convolutional layer's weights\n",
    "# torch.nn.init.kaiming_normal_(model.backbone.features[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# # Modify the classification head\n",
    "# # SSD uses a set of convolutional layers for the classification head, which needs to be adapted for your number of classes\n",
    "# # We need to retrieve the correct number of channels for each feature map in the SSD model\n",
    "# in_channels = [layer.in_channels for layer in model.head.classification_head.module_list]\n",
    "# num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "\n",
    "# # Redefine the classification head to match the number of classes\n",
    "# model.head.classification_head = SSDClassificationHead(\n",
    "#     in_channels=in_channels,  # List of input channels for each feature map\n",
    "#     num_anchors=num_anchors,  # List of anchors per location for each feature map\n",
    "#     num_classes=NUM_CLASSES,  # Number of classes (including background)\n",
    "#     # norm_layer=partial(torch.nn.GroupNorm, 32)\n",
    "# )\n",
    "\n",
    "# # Move the model to the correct device (e.g., CUDA or CPU)\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Verify the model structure\n",
    "# print(model)\n",
    "\n",
    "# import evaluate\n",
    "\n",
    "# evaluate.main_function(\n",
    "#     DIR_TEST=DIR_TEST,\n",
    "#     DIR_IMAGES=DIR_IMAGES,\n",
    "#     DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "#     IMG_RESIZE=IMG_RESIZE,\n",
    "#     SAVE_DIR=SAVE_DIR,\n",
    "#     CLASSES=CLASSES,\n",
    "#     NUM_CLASSES=NUM_CLASSES,\n",
    "#     PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "#     NUM_CHANNELS=NUM_CHANNELS,\n",
    "#     img_means=img_means,\n",
    "#     img_stds=img_stds,\n",
    "#     ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "#     model=model,\n",
    "#     BATCH_SIZE=batch_size,\n",
    "#     NUM_WORKERS=num_workers,\n",
    "#     model_name='SSD',\n",
    "# )\n",
    "\n",
    "# del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SSD Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory Inputs\n",
    "# SAVE_DIR = f'../runs/SSD/SSD_{MODEL_NAME}_teacher1'\n",
    "\n",
    "# # Privileged Information Paths\n",
    "# PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# # Special Constructed Features\n",
    "#     \"Box_Mask\",\n",
    "# ]\n",
    "\n",
    "# # Number of input image channels RGB + Extras\n",
    "# NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# # Defining Model\n",
    "\n",
    "# # Set the device to CUDA or CPU\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # Load the pretrained SSD model (standard SSD)\n",
    "# weights = torchvision.models.detection.SSD300_VGG16_Weights.DEFAULT\n",
    "# pre_trained_model = torchvision.models.detection.ssd300_vgg16(weights=weights)\n",
    "\n",
    "# model = pre_trained_model.to(device)\n",
    "\n",
    "# # Modify the first convolutional layer for 4-channel input\n",
    "# # In SSD, the input convolution layer is part of the VGG model's backbone\n",
    "# model.backbone.features[0] = torch.nn.Conv2d(NUM_CHANNELS, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "# # Initialize the first convolutional layer's weights\n",
    "# torch.nn.init.kaiming_normal_(model.backbone.features[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# # Modify the classification head\n",
    "# # SSD uses a set of convolutional layers for the classification head, which needs to be adapted for your number of classes\n",
    "# # We need to retrieve the correct number of channels for each feature map in the SSD model\n",
    "# in_channels = [layer.in_channels for layer in model.head.classification_head.module_list]\n",
    "# num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "\n",
    "# # Redefine the classification head to match the number of classes\n",
    "# model.head.classification_head = SSDClassificationHead(\n",
    "#     in_channels=in_channels,  # List of input channels for each feature map\n",
    "#     num_anchors=num_anchors,  # List of anchors per location for each feature map\n",
    "#     num_classes=NUM_CLASSES,  # Number of classes (including background)\n",
    "#     # norm_layer=partial(torch.nn.GroupNorm, 32)\n",
    "# )\n",
    "\n",
    "# # Move the model to the correct device (e.g., CUDA or CPU)\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Verify the model structure\n",
    "# print(model)\n",
    "\n",
    "# import evaluate\n",
    "\n",
    "# evaluate.main_function(\n",
    "#     DIR_TEST=DIR_TEST,\n",
    "#     DIR_IMAGES=DIR_IMAGES,\n",
    "#     DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "#     IMG_RESIZE=IMG_RESIZE,\n",
    "#     SAVE_DIR=SAVE_DIR,\n",
    "#     CLASSES=CLASSES,\n",
    "#     NUM_CLASSES=NUM_CLASSES,\n",
    "#     PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "#     NUM_CHANNELS=NUM_CHANNELS,\n",
    "#     img_means=img_means,\n",
    "#     img_stds=img_stds,\n",
    "#     ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "#     model=model,\n",
    "#     BATCH_SIZE=batch_size,\n",
    "#     NUM_WORKERS=num_workers,\n",
    "#     model_name='SSD',\n",
    "# )\n",
    "\n",
    "# del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SSD Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory Inputs\n",
    "# SAVE_DIR = f'../runs/SSD/SSD_{MODEL_NAME}_student1'\n",
    "\n",
    "# # Privileged Information Paths\n",
    "# PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# # Special Constructed Features\n",
    "#     # \"Box_Mask\",\n",
    "# ]\n",
    "\n",
    "# # Number of input image channels RGB + Extras\n",
    "# NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# # Defining Model\n",
    "\n",
    "# # Set the device to CUDA or CPU\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # Load the pretrained SSD model (standard SSD)\n",
    "# weights = torchvision.models.detection.SSD300_VGG16_Weights.DEFAULT\n",
    "# pre_trained_model = torchvision.models.detection.ssd300_vgg16(weights=weights)\n",
    "\n",
    "# model = pre_trained_model.to(device)\n",
    "\n",
    "# # Modify the first convolutional layer for 4-channel input\n",
    "# # In SSD, the input convolution layer is part of the VGG model's backbone\n",
    "# model.backbone.features[0] = torch.nn.Conv2d(NUM_CHANNELS, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "# # Initialize the first convolutional layer's weights\n",
    "# torch.nn.init.kaiming_normal_(model.backbone.features[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# # Modify the classification head\n",
    "# # SSD uses a set of convolutional layers for the classification head, which needs to be adapted for your number of classes\n",
    "# # We need to retrieve the correct number of channels for each feature map in the SSD model\n",
    "# in_channels = [layer.in_channels for layer in model.head.classification_head.module_list]\n",
    "# num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "\n",
    "# # Redefine the classification head to match the number of classes\n",
    "# model.head.classification_head = SSDClassificationHead(\n",
    "#     in_channels=in_channels,  # List of input channels for each feature map\n",
    "#     num_anchors=num_anchors,  # List of anchors per location for each feature map\n",
    "#     num_classes=NUM_CLASSES,  # Number of classes (including background)\n",
    "#     # norm_layer=partial(torch.nn.GroupNorm, 32)\n",
    "# )\n",
    "\n",
    "# # Move the model to the correct device (e.g., CUDA or CPU)\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Verify the model structure\n",
    "# print(model)\n",
    "\n",
    "# import evaluate\n",
    "\n",
    "# evaluate.main_function(\n",
    "#     DIR_TEST=DIR_TEST,\n",
    "#     DIR_IMAGES=DIR_IMAGES,\n",
    "#     DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "#     IMG_RESIZE=IMG_RESIZE,\n",
    "#     SAVE_DIR=SAVE_DIR,\n",
    "#     CLASSES=CLASSES,\n",
    "#     NUM_CLASSES=NUM_CLASSES,\n",
    "#     PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "#     NUM_CHANNELS=NUM_CHANNELS,\n",
    "#     img_means=img_means,\n",
    "#     img_stds=img_stds,\n",
    "#     ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "#     model=model,\n",
    "#     BATCH_SIZE=batch_size,\n",
    "#     NUM_WORKERS=num_workers,\n",
    "#     model_name='SSD',\n",
    "# )\n",
    "\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory Inputs\n",
    "# SAVE_DIR = f'../runs/SSD/SSD_{MODEL_NAME}_student2'\n",
    "\n",
    "# # Privileged Information Paths\n",
    "# PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# # Special Constructed Features\n",
    "#     # \"Box_Mask\",\n",
    "# ]\n",
    "\n",
    "# # Number of input image channels RGB + Extras\n",
    "# NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# # Defining Model\n",
    "\n",
    "# # Set the device to CUDA or CPU\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # Load the pretrained SSD model (standard SSD)\n",
    "# weights = torchvision.models.detection.SSD300_VGG16_Weights.DEFAULT\n",
    "# pre_trained_model = torchvision.models.detection.ssd300_vgg16(weights=weights)\n",
    "\n",
    "# model = pre_trained_model.to(device)\n",
    "\n",
    "# # Modify the first convolutional layer for 4-channel input\n",
    "# # In SSD, the input convolution layer is part of the VGG model's backbone\n",
    "# model.backbone.features[0] = torch.nn.Conv2d(NUM_CHANNELS, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "# # Initialize the first convolutional layer's weights\n",
    "# torch.nn.init.kaiming_normal_(model.backbone.features[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# # Modify the classification head\n",
    "# # SSD uses a set of convolutional layers for the classification head, which needs to be adapted for your number of classes\n",
    "# # We need to retrieve the correct number of channels for each feature map in the SSD model\n",
    "# in_channels = [layer.in_channels for layer in model.head.classification_head.module_list]\n",
    "# num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "\n",
    "# # Redefine the classification head to match the number of classes\n",
    "# model.head.classification_head = SSDClassificationHead(\n",
    "#     in_channels=in_channels,  # List of input channels for each feature map\n",
    "#     num_anchors=num_anchors,  # List of anchors per location for each feature map\n",
    "#     num_classes=NUM_CLASSES,  # Number of classes (including background)\n",
    "#     # norm_layer=partial(torch.nn.GroupNorm, 32)\n",
    "# )\n",
    "\n",
    "# # Move the model to the correct device (e.g., CUDA or CPU)\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Verify the model structure\n",
    "# print(model)\n",
    "\n",
    "# import evaluate\n",
    "\n",
    "# evaluate.main_function(\n",
    "#     DIR_TEST=DIR_TEST,\n",
    "#     DIR_IMAGES=DIR_IMAGES,\n",
    "#     DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "#     IMG_RESIZE=IMG_RESIZE,\n",
    "#     SAVE_DIR=SAVE_DIR,\n",
    "#     CLASSES=CLASSES,\n",
    "#     NUM_CLASSES=NUM_CLASSES,\n",
    "#     PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "#     NUM_CHANNELS=NUM_CHANNELS,\n",
    "#     img_means=img_means,\n",
    "#     img_stds=img_stds,\n",
    "#     ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "#     model=model,\n",
    "#     BATCH_SIZE=batch_size,\n",
    "#     NUM_WORKERS=num_workers,\n",
    "#     model_name='SSD',\n",
    "# )\n",
    "\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory Inputs\n",
    "SAVE_DIR = f'../runs/SSD/SSD_{MODEL_NAME}_student3'\n",
    "\n",
    "# Privileged Information Paths\n",
    "PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# Special Constructed Features\n",
    "    # \"Box_Mask\",\n",
    "]\n",
    "\n",
    "# Number of input image channels RGB + Extras\n",
    "NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# Defining Model\n",
    "\n",
    "# Set the device to CUDA or CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Load the pretrained SSD model (standard SSD)\n",
    "weights = torchvision.models.detection.SSD300_VGG16_Weights.DEFAULT\n",
    "pre_trained_model = torchvision.models.detection.ssd300_vgg16(weights=weights)\n",
    "\n",
    "model = pre_trained_model.to(device)\n",
    "\n",
    "# Modify the first convolutional layer for 4-channel input\n",
    "# In SSD, the input convolution layer is part of the VGG model's backbone\n",
    "model.backbone.features[0] = torch.nn.Conv2d(NUM_CHANNELS, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "# Initialize the first convolutional layer's weights\n",
    "torch.nn.init.kaiming_normal_(model.backbone.features[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# Modify the classification head\n",
    "# SSD uses a set of convolutional layers for the classification head, which needs to be adapted for your number of classes\n",
    "# We need to retrieve the correct number of channels for each feature map in the SSD model\n",
    "in_channels = [layer.in_channels for layer in model.head.classification_head.module_list]\n",
    "num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "\n",
    "# Redefine the classification head to match the number of classes\n",
    "model.head.classification_head = SSDClassificationHead(\n",
    "    in_channels=in_channels,  # List of input channels for each feature map\n",
    "    num_anchors=num_anchors,  # List of anchors per location for each feature map\n",
    "    num_classes=NUM_CLASSES,  # Number of classes (including background)\n",
    "    # norm_layer=partial(torch.nn.GroupNorm, 32)\n",
    ")\n",
    "\n",
    "# Move the model to the correct device (e.g., CUDA or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Verify the model structure\n",
    "print(model)\n",
    "\n",
    "import evaluate\n",
    "\n",
    "evaluate.main_function(\n",
    "    DIR_TEST=DIR_TEST,\n",
    "    DIR_IMAGES=DIR_IMAGES,\n",
    "    DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "    IMG_RESIZE=IMG_RESIZE,\n",
    "    SAVE_DIR=SAVE_DIR,\n",
    "    CLASSES=CLASSES,\n",
    "    NUM_CLASSES=NUM_CLASSES,\n",
    "    PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "    NUM_CHANNELS=NUM_CHANNELS,\n",
    "    img_means=img_means,\n",
    "    img_stds=img_stds,\n",
    "    ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "    model=model,\n",
    "    BATCH_SIZE=batch_size,\n",
    "    NUM_WORKERS=num_workers,\n",
    "    model_name='SSD',\n",
    ")\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory Inputs\n",
    "SAVE_DIR = f'../runs/SSD/SSD_{MODEL_NAME}_student4'\n",
    "\n",
    "# Privileged Information Paths\n",
    "PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# Special Constructed Features\n",
    "    # \"Box_Mask\",\n",
    "]\n",
    "\n",
    "# Number of input image channels RGB + Extras\n",
    "NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# Defining Model\n",
    "\n",
    "# Set the device to CUDA or CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Load the pretrained SSD model (standard SSD)\n",
    "weights = torchvision.models.detection.SSD300_VGG16_Weights.DEFAULT\n",
    "pre_trained_model = torchvision.models.detection.ssd300_vgg16(weights=weights)\n",
    "\n",
    "model = pre_trained_model.to(device)\n",
    "\n",
    "# Modify the first convolutional layer for 4-channel input\n",
    "# In SSD, the input convolution layer is part of the VGG model's backbone\n",
    "model.backbone.features[0] = torch.nn.Conv2d(NUM_CHANNELS, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "# Initialize the first convolutional layer's weights\n",
    "torch.nn.init.kaiming_normal_(model.backbone.features[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# Modify the classification head\n",
    "# SSD uses a set of convolutional layers for the classification head, which needs to be adapted for your number of classes\n",
    "# We need to retrieve the correct number of channels for each feature map in the SSD model\n",
    "in_channels = [layer.in_channels for layer in model.head.classification_head.module_list]\n",
    "num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "\n",
    "# Redefine the classification head to match the number of classes\n",
    "model.head.classification_head = SSDClassificationHead(\n",
    "    in_channels=in_channels,  # List of input channels for each feature map\n",
    "    num_anchors=num_anchors,  # List of anchors per location for each feature map\n",
    "    num_classes=NUM_CLASSES,  # Number of classes (including background)\n",
    "    # norm_layer=partial(torch.nn.GroupNorm, 32)\n",
    ")\n",
    "\n",
    "# Move the model to the correct device (e.g., CUDA or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Verify the model structure\n",
    "print(model)\n",
    "\n",
    "import evaluate\n",
    "\n",
    "evaluate.main_function(\n",
    "    DIR_TEST=DIR_TEST,\n",
    "    DIR_IMAGES=DIR_IMAGES,\n",
    "    DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "    IMG_RESIZE=IMG_RESIZE,\n",
    "    SAVE_DIR=SAVE_DIR,\n",
    "    CLASSES=CLASSES,\n",
    "    NUM_CLASSES=NUM_CLASSES,\n",
    "    PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "    NUM_CHANNELS=NUM_CHANNELS,\n",
    "    img_means=img_means,\n",
    "    img_stds=img_stds,\n",
    "    ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "    model=model,\n",
    "    BATCH_SIZE=batch_size,\n",
    "    NUM_WORKERS=num_workers,\n",
    "    model_name='SSD',\n",
    ")\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SSDLite Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory Inputs\n",
    "# SAVE_DIR = f'../runs/SSDLite/SSDLite_{MODEL_NAME}_baseline1'\n",
    "\n",
    "# # Privileged Information Paths\n",
    "# PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# # Special Constructed Features\n",
    "#     # \"Box_Mask\",\n",
    "# ]\n",
    "\n",
    "# # Number of input image channels RGB + Extras\n",
    "# NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# # Defining Model\n",
    "\n",
    "# # Set the device to CUDA or CPU\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # Load the pretrained SSD model (SSDLite320 MobileNetV3)\n",
    "# weights = torchvision.models.detection.SSDLite320_MobileNet_V3_Large_Weights.DEFAULT\n",
    "# pre_trained_model = torchvision.models.detection.ssdlite320_mobilenet_v3_large(weights=weights)\n",
    "\n",
    "# model = pre_trained_model.to(device)\n",
    "\n",
    "# # Modify the first convolutional layer for 4-channel input\n",
    "# # In SSD, the input convolution layer is part of the VGG model's backbone\n",
    "# model.backbone.features[0][0][0] = torch.nn.Conv2d(NUM_CHANNELS, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "# # Initialize the first convolutional layer's weights\n",
    "# torch.nn.init.kaiming_normal_(model.backbone.features[0][0][0].weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# # Modify the classification head\n",
    "# # SSD uses a set of convolutional layers for the classification head, which needs to be adapted for your number of classes\n",
    "# # https://stackoverflow.com/questions/71094251/fine-tuning-ssd-lite-in-torchvision\n",
    "# # from torchvision.models.detection import _utils as det_utils\n",
    "# # Forward a dummy image through the backbone to get output channels\n",
    "# tmp_img = torch.zeros((1, NUM_CHANNELS, 640, 640), dtype=torch.float32, device=device)\n",
    "# model.to(device)\n",
    "# with torch.no_grad():\n",
    "#     features = model.backbone(tmp_img)\n",
    "\n",
    "# # Extract feature map channels\n",
    "# if isinstance(features, torch.Tensor):\n",
    "#     in_channels = [features.shape[1]]  # Single feature map\n",
    "# else:\n",
    "#     in_channels = [f.shape[1] for f in features.values()]  # Multiple feature maps\n",
    "\n",
    "\n",
    "# num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "\n",
    "# # Redefine the classification head to match the number of classes\n",
    "# model.head.classification_head = SSDLiteClassificationHead(\n",
    "#     in_channels=in_channels,  # List of input channels for each feature map\n",
    "#     num_anchors=num_anchors,  # List of anchors per location for each feature map\n",
    "#     num_classes=NUM_CLASSES,  # Number of classes (including background)\n",
    "#     norm_layer=partial(torch.nn.GroupNorm, 32),  # Normalization layer\n",
    "# )\n",
    "\n",
    "# # Set the number of classes in the model\n",
    "# model.num_classes = NUM_CLASSES\n",
    "# model.head.num_classes = NUM_CLASSES\n",
    "\n",
    "# # Move the model to the correct device (e.g., CUDA or CPU)\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Verify the model structure\n",
    "# print(model)\n",
    "\n",
    "# import evaluate\n",
    "\n",
    "# evaluate.main_function(\n",
    "#     DIR_TEST=DIR_TEST,\n",
    "#     DIR_IMAGES=DIR_IMAGES,\n",
    "#     DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "#     IMG_RESIZE=IMG_RESIZE,\n",
    "#     SAVE_DIR=SAVE_DIR,\n",
    "#     CLASSES=CLASSES,\n",
    "#     NUM_CLASSES=NUM_CLASSES,\n",
    "#     PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "#     NUM_CHANNELS=NUM_CHANNELS,\n",
    "#     img_means=img_means,\n",
    "#     img_stds=img_stds,\n",
    "#     ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "#     model=model,\n",
    "#     BATCH_SIZE=batch_size,\n",
    "#     NUM_WORKERS=num_workers,\n",
    "#     model_name='SSDLite',\n",
    "# )\n",
    "\n",
    "# del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SSDLite Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory Inputs\n",
    "# SAVE_DIR = f'../runs/SSDLite/SSDLite_{MODEL_NAME}_teacher1'\n",
    "\n",
    "# # Privileged Information Paths\n",
    "# PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# # Special Constructed Features\n",
    "#     \"Box_Mask\",\n",
    "# ]\n",
    "\n",
    "# # Number of input image channels RGB + Extras\n",
    "# NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# # Defining Model\n",
    "\n",
    "# # Set the device to CUDA or CPU\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # Load the pretrained SSD model (SSDLite320 MobileNetV3)\n",
    "# weights = torchvision.models.detection.SSDLite320_MobileNet_V3_Large_Weights.DEFAULT\n",
    "# pre_trained_model = torchvision.models.detection.ssdlite320_mobilenet_v3_large(weights=weights)\n",
    "\n",
    "# model = pre_trained_model.to(device)\n",
    "\n",
    "# # Modify the first convolutional layer for 4-channel input\n",
    "# # In SSD, the input convolution layer is part of the VGG model's backbone\n",
    "# model.backbone.features[0][0][0] = torch.nn.Conv2d(NUM_CHANNELS, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "# # Initialize the first convolutional layer's weights\n",
    "# torch.nn.init.kaiming_normal_(model.backbone.features[0][0][0].weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# # Modify the classification head\n",
    "# # SSD uses a set of convolutional layers for the classification head, which needs to be adapted for your number of classes\n",
    "# # https://stackoverflow.com/questions/71094251/fine-tuning-ssd-lite-in-torchvision\n",
    "# # from torchvision.models.detection import _utils as det_utils\n",
    "# # Forward a dummy image through the backbone to get output channels\n",
    "# tmp_img = torch.zeros((1, NUM_CHANNELS, 640, 640), dtype=torch.float32, device=device)\n",
    "# model.to(device)\n",
    "# with torch.no_grad():\n",
    "#     features = model.backbone(tmp_img)\n",
    "\n",
    "# # Extract feature map channels\n",
    "# if isinstance(features, torch.Tensor):\n",
    "#     in_channels = [features.shape[1]]  # Single feature map\n",
    "# else:\n",
    "#     in_channels = [f.shape[1] for f in features.values()]  # Multiple feature maps\n",
    "\n",
    "\n",
    "# num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "\n",
    "# # Redefine the classification head to match the number of classes\n",
    "# model.head.classification_head = SSDLiteClassificationHead(\n",
    "#     in_channels=in_channels,  # List of input channels for each feature map\n",
    "#     num_anchors=num_anchors,  # List of anchors per location for each feature map\n",
    "#     num_classes=NUM_CLASSES,  # Number of classes (including background)\n",
    "#     norm_layer=partial(torch.nn.GroupNorm, 32),  # Normalization layer\n",
    "# )\n",
    "\n",
    "# # Set the number of classes in the model\n",
    "# model.num_classes = NUM_CLASSES\n",
    "# model.head.num_classes = NUM_CLASSES\n",
    "\n",
    "# # Move the model to the correct device (e.g., CUDA or CPU)\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Verify the model structure\n",
    "# print(model)\n",
    "\n",
    "# import evaluate\n",
    "\n",
    "# evaluate.main_function(\n",
    "#     DIR_TEST=DIR_TEST,\n",
    "#     DIR_IMAGES=DIR_IMAGES,\n",
    "#     DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "#     IMG_RESIZE=IMG_RESIZE,\n",
    "#     SAVE_DIR=SAVE_DIR,\n",
    "#     CLASSES=CLASSES,\n",
    "#     NUM_CLASSES=NUM_CLASSES,\n",
    "#     PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "#     NUM_CHANNELS=NUM_CHANNELS,\n",
    "#     img_means=img_means,\n",
    "#     img_stds=img_stds,\n",
    "#     ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "#     model=model,\n",
    "#     BATCH_SIZE=batch_size,\n",
    "#     NUM_WORKERS=num_workers,\n",
    "#     model_name='SSDLite',\n",
    "# )\n",
    "\n",
    "# del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SSDLite Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory Inputs\n",
    "# SAVE_DIR = f'../runs/SSDLite/SSDLite_{MODEL_NAME}_student1'\n",
    "\n",
    "# # Privileged Information Paths\n",
    "# PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# # Special Constructed Features\n",
    "#     # \"Box_Mask\",\n",
    "# ]\n",
    "\n",
    "# # Number of input image channels RGB + Extras\n",
    "# NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# # Defining Model\n",
    "\n",
    "# # Set the device to CUDA or CPU\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # Load the pretrained SSD model (SSDLite320 MobileNetV3)\n",
    "# weights = torchvision.models.detection.SSDLite320_MobileNet_V3_Large_Weights.DEFAULT\n",
    "# pre_trained_model = torchvision.models.detection.ssdlite320_mobilenet_v3_large(weights=weights)\n",
    "\n",
    "# model = pre_trained_model.to(device)\n",
    "\n",
    "# # Modify the first convolutional layer for 4-channel input\n",
    "# # In SSD, the input convolution layer is part of the VGG model's backbone\n",
    "# model.backbone.features[0][0][0] = torch.nn.Conv2d(NUM_CHANNELS, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "# # Initialize the first convolutional layer's weights\n",
    "# torch.nn.init.kaiming_normal_(model.backbone.features[0][0][0].weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# # Modify the classification head\n",
    "# # SSD uses a set of convolutional layers for the classification head, which needs to be adapted for your number of classes\n",
    "# # https://stackoverflow.com/questions/71094251/fine-tuning-ssd-lite-in-torchvision\n",
    "# # from torchvision.models.detection import _utils as det_utils\n",
    "# # Forward a dummy image through the backbone to get output channels\n",
    "# tmp_img = torch.zeros((1, NUM_CHANNELS, 640, 640), dtype=torch.float32, device=device)\n",
    "# model.to(device)\n",
    "# with torch.no_grad():\n",
    "#     features = model.backbone(tmp_img)\n",
    "\n",
    "# # Extract feature map channels\n",
    "# if isinstance(features, torch.Tensor):\n",
    "#     in_channels = [features.shape[1]]  # Single feature map\n",
    "# else:\n",
    "#     in_channels = [f.shape[1] for f in features.values()]  # Multiple feature maps\n",
    "\n",
    "\n",
    "# num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "\n",
    "# # Redefine the classification head to match the number of classes\n",
    "# model.head.classification_head = SSDLiteClassificationHead(\n",
    "#     in_channels=in_channels,  # List of input channels for each feature map\n",
    "#     num_anchors=num_anchors,  # List of anchors per location for each feature map\n",
    "#     num_classes=NUM_CLASSES,  # Number of classes (including background)\n",
    "#     norm_layer=partial(torch.nn.GroupNorm, 32),  # Normalization layer\n",
    "# )\n",
    "\n",
    "# # Set the number of classes in the model\n",
    "# model.num_classes = NUM_CLASSES\n",
    "# model.head.num_classes = NUM_CLASSES\n",
    "\n",
    "# # Move the model to the correct device (e.g., CUDA or CPU)\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Verify the model structure\n",
    "# print(model)\n",
    "\n",
    "# import evaluate\n",
    "\n",
    "# evaluate.main_function(\n",
    "#     DIR_TEST=DIR_TEST,\n",
    "#     DIR_IMAGES=DIR_IMAGES,\n",
    "#     DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "#     IMG_RESIZE=IMG_RESIZE,\n",
    "#     SAVE_DIR=SAVE_DIR,\n",
    "#     CLASSES=CLASSES,\n",
    "#     NUM_CLASSES=NUM_CLASSES,\n",
    "#     PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "#     NUM_CHANNELS=NUM_CHANNELS,\n",
    "#     img_means=img_means,\n",
    "#     img_stds=img_stds,\n",
    "#     ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "#     model=model,\n",
    "#     BATCH_SIZE=batch_size,\n",
    "#     NUM_WORKERS=num_workers,\n",
    "#     model_name='SSDLite',\n",
    "# )\n",
    "\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory Inputs\n",
    "# SAVE_DIR = f'../runs/SSDLite/SSDLite_{MODEL_NAME}_student2'\n",
    "\n",
    "# # Privileged Information Paths\n",
    "# PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# # Special Constructed Features\n",
    "#     # \"Box_Mask\",\n",
    "# ]\n",
    "\n",
    "# # Number of input image channels RGB + Extras\n",
    "# NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# # Defining Model\n",
    "\n",
    "# # Set the device to CUDA or CPU\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # Load the pretrained SSD model (SSDLite320 MobileNetV3)\n",
    "# weights = torchvision.models.detection.SSDLite320_MobileNet_V3_Large_Weights.DEFAULT\n",
    "# pre_trained_model = torchvision.models.detection.ssdlite320_mobilenet_v3_large(weights=weights)\n",
    "\n",
    "# model = pre_trained_model.to(device)\n",
    "\n",
    "# # Modify the first convolutional layer for 4-channel input\n",
    "# # In SSD, the input convolution layer is part of the VGG model's backbone\n",
    "# model.backbone.features[0][0][0] = torch.nn.Conv2d(NUM_CHANNELS, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "# # Initialize the first convolutional layer's weights\n",
    "# torch.nn.init.kaiming_normal_(model.backbone.features[0][0][0].weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# # Modify the classification head\n",
    "# # SSD uses a set of convolutional layers for the classification head, which needs to be adapted for your number of classes\n",
    "# # https://stackoverflow.com/questions/71094251/fine-tuning-ssd-lite-in-torchvision\n",
    "# # from torchvision.models.detection import _utils as det_utils\n",
    "# # Forward a dummy image through the backbone to get output channels\n",
    "# tmp_img = torch.zeros((1, NUM_CHANNELS, 640, 640), dtype=torch.float32, device=device)\n",
    "# model.to(device)\n",
    "# with torch.no_grad():\n",
    "#     features = model.backbone(tmp_img)\n",
    "\n",
    "# # Extract feature map channels\n",
    "# if isinstance(features, torch.Tensor):\n",
    "#     in_channels = [features.shape[1]]  # Single feature map\n",
    "# else:\n",
    "#     in_channels = [f.shape[1] for f in features.values()]  # Multiple feature maps\n",
    "\n",
    "\n",
    "# num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "\n",
    "# # Redefine the classification head to match the number of classes\n",
    "# model.head.classification_head = SSDLiteClassificationHead(\n",
    "#     in_channels=in_channels,  # List of input channels for each feature map\n",
    "#     num_anchors=num_anchors,  # List of anchors per location for each feature map\n",
    "#     num_classes=NUM_CLASSES,  # Number of classes (including background)\n",
    "#     norm_layer=partial(torch.nn.GroupNorm, 32),  # Normalization layer\n",
    "# )\n",
    "\n",
    "# # Set the number of classes in the model\n",
    "# model.num_classes = NUM_CLASSES\n",
    "# model.head.num_classes = NUM_CLASSES\n",
    "\n",
    "# # Move the model to the correct device (e.g., CUDA or CPU)\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Verify the model structure\n",
    "# print(model)\n",
    "\n",
    "# import evaluate\n",
    "\n",
    "# evaluate.main_function(\n",
    "#     DIR_TEST=DIR_TEST,\n",
    "#     DIR_IMAGES=DIR_IMAGES,\n",
    "#     DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "#     IMG_RESIZE=IMG_RESIZE,\n",
    "#     SAVE_DIR=SAVE_DIR,\n",
    "#     CLASSES=CLASSES,\n",
    "#     NUM_CLASSES=NUM_CLASSES,\n",
    "#     PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "#     NUM_CHANNELS=NUM_CHANNELS,\n",
    "#     img_means=img_means,\n",
    "#     img_stds=img_stds,\n",
    "#     ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "#     model=model,\n",
    "#     BATCH_SIZE=batch_size,\n",
    "#     NUM_WORKERS=num_workers,\n",
    "#     model_name='SSDLite',\n",
    "# )\n",
    "\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory Inputs\n",
    "SAVE_DIR = f'../runs/SSDLite/SSDLite_{MODEL_NAME}_student3'\n",
    "\n",
    "# Privileged Information Paths\n",
    "PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# Special Constructed Features\n",
    "    # \"Box_Mask\",\n",
    "]\n",
    "\n",
    "# Number of input image channels RGB + Extras\n",
    "NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# Defining Model\n",
    "\n",
    "# Set the device to CUDA or CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Load the pretrained SSD model (SSDLite320 MobileNetV3)\n",
    "weights = torchvision.models.detection.SSDLite320_MobileNet_V3_Large_Weights.DEFAULT\n",
    "pre_trained_model = torchvision.models.detection.ssdlite320_mobilenet_v3_large(weights=weights)\n",
    "\n",
    "model = pre_trained_model.to(device)\n",
    "\n",
    "# Modify the first convolutional layer for 4-channel input\n",
    "# In SSD, the input convolution layer is part of the VGG model's backbone\n",
    "model.backbone.features[0][0][0] = torch.nn.Conv2d(NUM_CHANNELS, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "# Initialize the first convolutional layer's weights\n",
    "torch.nn.init.kaiming_normal_(model.backbone.features[0][0][0].weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# Modify the classification head\n",
    "# SSD uses a set of convolutional layers for the classification head, which needs to be adapted for your number of classes\n",
    "# https://stackoverflow.com/questions/71094251/fine-tuning-ssd-lite-in-torchvision\n",
    "# from torchvision.models.detection import _utils as det_utils\n",
    "# Forward a dummy image through the backbone to get output channels\n",
    "tmp_img = torch.zeros((1, NUM_CHANNELS, 640, 640), dtype=torch.float32, device=device)\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    features = model.backbone(tmp_img)\n",
    "\n",
    "# Extract feature map channels\n",
    "if isinstance(features, torch.Tensor):\n",
    "    in_channels = [features.shape[1]]  # Single feature map\n",
    "else:\n",
    "    in_channels = [f.shape[1] for f in features.values()]  # Multiple feature maps\n",
    "\n",
    "\n",
    "num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "\n",
    "# Redefine the classification head to match the number of classes\n",
    "model.head.classification_head = SSDLiteClassificationHead(\n",
    "    in_channels=in_channels,  # List of input channels for each feature map\n",
    "    num_anchors=num_anchors,  # List of anchors per location for each feature map\n",
    "    num_classes=NUM_CLASSES,  # Number of classes (including background)\n",
    "    norm_layer=partial(torch.nn.GroupNorm, 32),  # Normalization layer\n",
    ")\n",
    "\n",
    "# Set the number of classes in the model\n",
    "model.num_classes = NUM_CLASSES\n",
    "model.head.num_classes = NUM_CLASSES\n",
    "\n",
    "# Move the model to the correct device (e.g., CUDA or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Verify the model structure\n",
    "print(model)\n",
    "\n",
    "import evaluate\n",
    "\n",
    "evaluate.main_function(\n",
    "    DIR_TEST=DIR_TEST,\n",
    "    DIR_IMAGES=DIR_IMAGES,\n",
    "    DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "    IMG_RESIZE=IMG_RESIZE,\n",
    "    SAVE_DIR=SAVE_DIR,\n",
    "    CLASSES=CLASSES,\n",
    "    NUM_CLASSES=NUM_CLASSES,\n",
    "    PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "    NUM_CHANNELS=NUM_CHANNELS,\n",
    "    img_means=img_means,\n",
    "    img_stds=img_stds,\n",
    "    ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "    model=model,\n",
    "    BATCH_SIZE=batch_size,\n",
    "    NUM_WORKERS=num_workers,\n",
    "    model_name='SSDLite',\n",
    ")\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory Inputs\n",
    "SAVE_DIR = f'../runs/SSDLite/SSDLite_{MODEL_NAME}_student4'\n",
    "\n",
    "# Privileged Information Paths\n",
    "PRIVILEGED_INFORMATION_DIRS = [\n",
    "\n",
    "# Special Constructed Features\n",
    "    # \"Box_Mask\",\n",
    "]\n",
    "\n",
    "# Number of input image channels RGB + Extras\n",
    "NUM_CHANNELS = 3 + len(PRIVILEGED_INFORMATION_DIRS)\n",
    "\n",
    "# Defining Model\n",
    "\n",
    "# Set the device to CUDA or CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Load the pretrained SSD model (SSDLite320 MobileNetV3)\n",
    "weights = torchvision.models.detection.SSDLite320_MobileNet_V3_Large_Weights.DEFAULT\n",
    "pre_trained_model = torchvision.models.detection.ssdlite320_mobilenet_v3_large(weights=weights)\n",
    "\n",
    "model = pre_trained_model.to(device)\n",
    "\n",
    "# Modify the first convolutional layer for 4-channel input\n",
    "# In SSD, the input convolution layer is part of the VGG model's backbone\n",
    "model.backbone.features[0][0][0] = torch.nn.Conv2d(NUM_CHANNELS, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "# Initialize the first convolutional layer's weights\n",
    "torch.nn.init.kaiming_normal_(model.backbone.features[0][0][0].weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# Modify the classification head\n",
    "# SSD uses a set of convolutional layers for the classification head, which needs to be adapted for your number of classes\n",
    "# https://stackoverflow.com/questions/71094251/fine-tuning-ssd-lite-in-torchvision\n",
    "# from torchvision.models.detection import _utils as det_utils\n",
    "# Forward a dummy image through the backbone to get output channels\n",
    "tmp_img = torch.zeros((1, NUM_CHANNELS, 640, 640), dtype=torch.float32, device=device)\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    features = model.backbone(tmp_img)\n",
    "\n",
    "# Extract feature map channels\n",
    "if isinstance(features, torch.Tensor):\n",
    "    in_channels = [features.shape[1]]  # Single feature map\n",
    "else:\n",
    "    in_channels = [f.shape[1] for f in features.values()]  # Multiple feature maps\n",
    "\n",
    "\n",
    "num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "\n",
    "# Redefine the classification head to match the number of classes\n",
    "model.head.classification_head = SSDLiteClassificationHead(\n",
    "    in_channels=in_channels,  # List of input channels for each feature map\n",
    "    num_anchors=num_anchors,  # List of anchors per location for each feature map\n",
    "    num_classes=NUM_CLASSES,  # Number of classes (including background)\n",
    "    norm_layer=partial(torch.nn.GroupNorm, 32),  # Normalization layer\n",
    ")\n",
    "\n",
    "# Set the number of classes in the model\n",
    "model.num_classes = NUM_CLASSES\n",
    "model.head.num_classes = NUM_CLASSES\n",
    "\n",
    "# Move the model to the correct device (e.g., CUDA or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Verify the model structure\n",
    "print(model)\n",
    "\n",
    "import evaluate\n",
    "\n",
    "evaluate.main_function(\n",
    "    DIR_TEST=DIR_TEST,\n",
    "    DIR_IMAGES=DIR_IMAGES,\n",
    "    DIR_ANNOTATIONS=DIR_ANNOTATIONS,\n",
    "    IMG_RESIZE=IMG_RESIZE,\n",
    "    SAVE_DIR=SAVE_DIR,\n",
    "    CLASSES=CLASSES,\n",
    "    NUM_CLASSES=NUM_CLASSES,\n",
    "    PRIVILEGED_INFORMATION_DIRS=PRIVILEGED_INFORMATION_DIRS,\n",
    "    NUM_CHANNELS=NUM_CHANNELS,\n",
    "    img_means=img_means,\n",
    "    img_stds=img_stds,\n",
    "    ALL_PRIVILEGED_INFORMATION_DIRS=ALL_PRIVILEGED_INFORMATION_DIRS,\n",
    "    model=model,\n",
    "    BATCH_SIZE=batch_size,\n",
    "    NUM_WORKERS=num_workers,\n",
    "    model_name='SSDLite',\n",
    ")\n",
    "\n",
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
