\graphicspath{{content/chapters/4_methodology/figures/}}

\chapter{Methodology}%
\label{chp:methodology}
\rule{\textwidth}{1pt} \\[1ex]

\epigraph{\textit{``We cannot solve problems with the same thinking we used when we created them.''}}{\textbf{-- Albert Einstein}}

\section{Introduction}
\label{sec:4_introduction}
% Methodology
% \begin{itemize}
%     \item Problem Definition (Mathematically) - Done
%     \item Theoretical Basis, describing the problem mathematically - Done
%     \item Privileged Information Generation - Done
%     \item Model Architectures (model selection), explain (don't have to say anything about yolo, used architectures open source, torchvision footnotes, pretrained models trained on COCO just mention) subsection for each one one page for each, - Done
%     \item Teacher creation, and student creation: LUPI framework, explain how you implemented the teacher modified the input, and the student distillation process, refer to the equation in the problem definition, we focus on the last feature representation layers of the backbone,  put the table 2 cols more details on the architecture . . . - Done
%     \item LUPI for OD Architecture + explanation (Teacher and student) - Done
%     \item Training Parameters (To be kept equal across experiments) - Done
%     \item Architecture parameters + Preprocessing + post processing + mention of tiling (pre and post processing different sections) - Done
%     \item Datasets SODA dataset without Tiling and With Tiling, include figures (a) and (b), BDW and UAVVaste we just write the number of examples, Pascal VOC, the same no figures (subsection for each)
%     \item Include Metrics here in methodology
% \end{itemize}

% This chapter opens with a formal mathematical formulation and theoretical context for the problem under investigation, specifically addressing how \gls{lupi} may be incorporated into object detection. This initial section establishes the conceptual basis before proceeding to define the role of privileged information within this context.
% Subsequently, the chapter presents an overview of the object detection models selected for this study, outlining their structural components and architectural configurations. This is followed by a detailed account of the modifications made to these models to facilitate the construction of the teacher and student networks. An architectural overview is provided to clarify the interrelation between these components and the general configuration adopted.
% The subsequent section outlines the implementation details, including the training parameters employed during the experiments outlined in the next chapter. This section also covers the pre-processing and post-processing steps applied uniformly across all models.
% The discussion then turns to the datasets employed in this study. Both \gls{uav}-based litter detection datasets and the Pascal VOC 2012 dataset are examined, highlighting the dataset pre-processing techniques required for each. The structure and nature of the data are also considered in relation to the proposed evaluation methodology.
% The chapter concludes by presenting the evaluation metrics used to assess object detection performance. These metrics form the basis for evaluating the models in the experiments presented in the subsequent chapter.
This chapter opens with a formal mathematical formulation and theoretical context for the problem under investigation, specifically addressing how \gls{lupi} may be incorporated into object detection. This initial section establishes the conceptual basis before proceeding to define the role of privileged information within this context.
Subsequently, the chapter presents an overview of the object detection models selected for this study. This is followed by a detailed account of the modifications made to these models to facilitate the construction of the teacher and student networks. An architectural overview is provided to clarify the interrelation between these components and the general configuration adopted.
The following section outlines the implementation details, including the training parameters employed during the experiments outlined in the next chapter. Furthermore, this section also covers the pre-processing and post-processing steps applied uniformly across all models.
The discussion then turns to the datasets employed in this study. Both \gls{uav}-based litter detection datasets and the Pascal VOC 2012 dataset are examined, highlighting the dataset pre-processing techniques required for each. The structure and nature of the data are also considered in relation to the proposed evaluation methodology.

\section{Problem Definition}
\label{sec:4_problem_definition}

To properly investigate the role of learning using privileged information in object detection and assess its feasibility, as per the first objective (\textbf{O1}), it is first necessary to understand the core problem being tackled and the required outputs. As outlined in Section \ref{sec:2_detection}, the object detection problem can be decomposed into two distinct components: localisation and classification.
With respect to the localisation component, the objective is to predict a bounding box (\gls{b}) that encapsulates the object of interest. This bounding box can be formally represented as a set of coordinates:
\begin{equation}
\label{eq:bounding_box}
b = \{b_x, b_y, b_w, b_h\}, \quad \text{where } 0 \leq b_x < W, 0 \leq b_y < H, 0 \leq b_w \leq W, 0 \leq b_h \leq H,
\end{equation}

\noindent where \( b_x \) and \( b_y \) denote the coordinates of the box's centre, while \( b_w \) and \( b_h \) represent the width and height of the box, respectively, constrained by the image dimensions \gls{W} (width) and \gls{H} (height).
In contrast, the classification component aims to assign a categorical label to the detected object. Let \gls{l} denote this predicted class label for a single object of interest:
\begin{equation}
\label{eq:classification_label}
l \in L,
\end{equation}

\noindent where \gls{L} represents the set of all possible class labels defined during the training process. For instance, in the case of \gls{coco}, trained models predict 80 distinct labels, i.e., \gls{L}$ = \{l_1, l_2, \dots, l_{80}\}$, while for Pascal \gls{voc}, the label set is smaller, with \gls{L}$ = \{l_1, l_2, \dots, l_{20}\}$.
Thus, for multi-label object detection, the predicted output for a given image can be defined as a set of detections (\gls{O}), whereby each tuple in the set consists of a bounding box and a class label:
\begin{equation}
\label{eq:multi_label_output}
O = \left\{ (b_1, l_1), (b_2, l_2), \ldots, (b_i, l_i) \right\}_{i=1}^{N}, \quad \text{where } N \in \mathbb{N} .
\end{equation}
Here, \gls{N} represents the total number of detections (or predictions) made for the given image, which is typically determined by the model's output layer and the number of objects detected in the scene.

After establishing the output, the next step is to focus on the predicting function \gls{f_function}, where \( x \) represents the input image fed into the network. In the context of object detection, this prediction can be split into two tasks: predicting the bounding box coordinates through a regression process and predicting the class label through a classification task. Therefore, the prediction function can be expressed as:
\begin{equation}
\label{eq:prediction_function}
f(x) = r(x) \oplus c(x).
\end{equation}

In this formulation, \gls{r_function} the regression component is responsible for estimating the bounding box coordinates, while \gls{c_function} the classification component is responsible for assigning a class label to the image. The operator ``\(\oplus\)'' denotes set element addition, combining the outputs from both the regression and classification components to form the final output set, denoted by \gls{O}.

The object detection task, as formulated in the \gls{lupi} paradigm, incorporates both standard input data (\gls{x}) and privileged information (\gls{x_star}), where the latter is only available during training. This combination enhances the model's performance by providing additional context. Formally, the object detection problem is defined using a training set consisting of triplets, as follows:
\begin{equation} \label{eq:object_detection}
\mathcal D_{\text{train}} = {(x_i, x_i^*, y_i)}_{i=1}^N, \quad x_i \in X , x_i^* \in X^*, y_i \in Y.
\end{equation}
In this formulation, \( X \) represents the space of input images, \( X^* \) denotes the space of privileged information instances, and \( Y \) comprises the space of bounding boxes with their associated class labels. Given a teacher model defined as:
\begin{equation}
f_{\text{teacher}}: X \cup X^* \rightarrow Y,
\end{equation}
which accurately predicts \( y \) based on both \( x \) and \( x^* \), the objective is to develop a student model:
\begin{equation}
f_{\text{student}}: X \rightarrow Y,
\end{equation}
that effectively maps \( X \) to \( Y \) by leveraging not only the intrinsic information in \( X \), but also the knowledge encoded within \gls{teacher}. In other words, during training, \gls{student} learns to map \( X \) to \( Y \) through knowledge distillation from \gls{teacher} and the information contained in the labelled examples from the training set \gls{trainingset}.

\section{Proposed Approach}
\label{sec:4_proposed_approach}

Given the problem definition, the proposed approach aims to integrate \gls{lupi} into object detection. To this end, both the \gls{teacher} and \gls{student} models are implemented as deep neural networks, each consisting of \gls{layers} layers. These models are represented as a series of transformations:
\begin{equation}
f_{\text{teacher}} = f_1^{(t)} \circ f_2^{(t)} \circ \cdots \circ f_{l}^{(t)} \circ 
\left\{
  \begin{array}{l}
    f_r^{(t)} \circ f_{r+1}^{(t)} \circ \cdots \circ f_{r+k}^{(t)} \\
    f_c^{(t)} \circ f_{c+1}^{(t)} \circ \cdots \circ f_{c+m}^{(t)}
  \end{array}
\right\} \circ f_\mathcal{L}^{(t)},
\end{equation}
\begin{equation}
f_{\text{student}} = f_1^{(s)} \circ f_2^{(s)} \circ \cdots \circ f_{l}^{(s)} \circ 
\left\{
  \begin{array}{l}
    f_r^{(s)} \circ f_{r+1}^{(s)} \circ \cdots \circ f_{r+k}^{(s)} \\
    f_c^{(s)} \circ f_{c+1}^{(s)} \circ \cdots \circ f_{c+m}^{(s)}
  \end{array}
\right\} \circ f_\mathcal{L}^{(s)},
\end{equation}
where ``\( \circ \)'' denotes function composition, and \( f_i^{(t)} \) and \( f_i^{(s)} \) represent the \( i \)-th transformation layer in the teacher and student networks, respectively. The \( l \)-th layer of both networks, \( f_l^{(t)} \) and \( f_l^{(s)} \), is constrained to have identical dimensionality, ensuring that the shared latent representation is consistent across both models. This shared representation serves as the foundation for deriving both the regression and classification outputs. Specifically, the regression branch, consisting of layers \( f_r^{(s)}, f_{r+1}^{(s)}, \dots, f_{r+k}^{(s)} \), predicts the bounding box coordinates, while the classification branch, consisting of layers \( f_c^{(s)}, f_{c+1}^{(s)}, \dots, f_{c+m}^{(s)} \), assigns class labels to the detected objects. The results from both branches are then combined at the final layer \( f_\mathcal{L}^{(s)} \) to form the final detection output (\gls{O}).

To enable knowledge transfer between the models, a distance function \( D(f_l^{(t)}, f_l^{(s)}) \) is minimised, thereby aligning the internal representations of the teacher and student at layer \( l \). For each training triplet \( (x_i, x_i^*, y_i) \in \mathcal{D}_{train} \), where \( x_i \) denotes the standard input, \( x_i^* \) denotes the privileged information, and \( y_i \) is the target label, the teacher computes a more expressive latent representation due to access to both \( x_i \) and \( x_i^* \). The student, restricted to \( x_i \), is guided to approximate this representation via distillation.

The student network is optimised using a composite loss function (\gls{student_loss}) that incorporates both supervised learning and representational distillation. The overall loss is defined as:
\begin{equation} \label{eq:lupi_loss_function}
L_{\mathcal{S}} = (1 - \alpha) \cdot L(f_{\text{student}}(x, y)) + \alpha \cdot D(f_i^{(t)}, f_i^{(s)}),
\end{equation}
where \( L(f_{\text{student}}(x, y)) \), represents the standard supervised loss for the student network, which encompasses both regression and classification tasks. The term \( D(f_i^{(t)}, f_i^{(s)}) \) denotes the distillation loss, measuring the difference between the teacher and student networks' internal representations at the \( i \)-th layer. The scalar parameter \( \alpha \in [0, 1] \) controls the balance between standard supervision and distillation. Specifically, \gls{alpha} determines how much the student network relies on the teacher’s internal representation as opposed to learning from labelled data. When \( \alpha = 0 \), the student learns solely from labelled data, while \( \alpha = 1 \) implies that the student depends entirely on the teacher's internal representations for guidance.

To compute the dissimilarity between the latent feature representations of the student and teacher (\gls{D}), the cosine distance function is employed:
\begin{equation} \label{eq:cosine_distance}
D(f_i^{(t)}, f_i^{(s)}) = 1 - \frac{\sum_{j=1}^{d} f_{i,j}^{(t)} f_{i,j}^{(s)}}{\sqrt{\sum_{j=1}^{d} (f_{i,j}^{(t)})^2} \, \sqrt{\sum_{j=1}^{d} (f_{i,j}^{(s)})^2}}.
\end{equation}
Here, \( f_i^{(t)} \) and \( f_i^{(s)} \) represent the feature vectors of the teacher and student networks at layer \( i \), each of length \( d \), with \( j \) indexing the components of the vectors. Here, the cosine distance captures the angular difference between these representations. By encouraging alignment at a shared intermediate layer \( l \), the student can benefit from the privileged information available during training, even though only the input \( x \) is accessible at inference.

Given this general formulation of how we integrated \gls{lupi} within the context of object detection, the next step, in alignment with the stated objectives, is to delve into the nature and role of the privileged information itself.

%  This setup naturally leads to a teacher--student framework. Within this framework, the teacher model undertakes both regression and classification tasks, and can be defined as follows:
% \begin{equation} \label{eq:teacher_network_reg}
% r_t(x, x^*) = \mathcal{T}_{\text{reg}}(\mathcal{T}_{\text{feat}}(x, x^*)),
% \end{equation}

% \begin{equation} \label{eq:teacher_network_cls}
% c_t(x, x^*) = \mathcal{T}_{\text{cls}}(\mathcal{T}_{\text{feat}}(x, x^*)) .
% \end{equation}

% \noindent Here, the regression function \( r_t \) and classification function \( c_t \) for the teacher both receive privileged information as input. The teacher network \gls{teacher} is decomposed into two distinct branches: \( \mathcal{T}_{\text{reg}} \) and \( \mathcal{T}_{\text{cls}} \), responsible for producing the respective outputs. The shared latent representation \( \mathcal{T}_{\text{feat}}(x, x^*) \) refers to the feature map extracted by the teacher’s feature extractor (or backbone), serving as the input to both heads.

% Similarly, the same formulation can be expressed in terms of the student network \gls{student}. However, this network takes only \gls{x} as input, without the privileged information \gls{x_star}. Thus, the regression and classification tasks for the student model can be decomposed as follows:
% \begin{equation} \label{eq:student_network_reg}
% r_s(x) = \mathcal{S}_{\text{reg}}(\mathcal{S}_{\text{feat}}(x)),
% \end{equation}

% \begin{equation} \label{eq:student_network_cls}
% c_s(x) = \mathcal{S}_{\text{cls}}(\mathcal{S}_{\text{feat}}(x)).
% \end{equation}

% \noindent Since the student network processes only the regular input (\gls{x}), it benefits from the rich learning representations generated by the teacher network. To incorporate this knowledge, distillation is employed within the loss function, as described in works such as \cite{lab2wild, lupi, lupi_distillation}. The resulting student network loss function (\gls{student_loss}) is given by:
% \begin{equation} \label{eq:lupi_loss_function}
% L_{\mathcal{S}} = (1 - \alpha) \cdot ( L_{\text{reg}}(x) + L_{\text{cls}}(x)) + \alpha \cdot D(\mathcal{S}_{\text{feat}}(x), \mathcal{T}_{\text{feat}}(x, x^*)).
% \end{equation}

% \noindent In this formulation, the combined losses \( L_{\text{reg}}(x) + L_{\text{cls}}(x) \) represent the standard student network loss, which integrates the regression and classification losses for object detection. The second term introduces the distillation loss, \( D(\mathcal{S}_{\text{feat}}(x), \mathcal{T}_{\text{feat}}(x, x^*)) \), where \gls{D} represents a distance function that penalises the difference between the latent representations of the student and the teacher at the same network layer. The parameter \gls{alpha}, which lies within the interval \( [0, 1] \), controls the degree to which the privileged information impacts the training process of the student network. When \gls{alpha}$=0$, the student relies solely on the ground truth labels, without utilising any privileged information. Conversely, when \gls{alpha}$=1$, the student mirrors the teacher's predictions, disregarding the ground truth labels.

% To compute the distance between the latent representations of the student and teacher for distillation, one can utilise the Cosine distance function \gls{D}, which is defined as follows:

% \begin{equation} \label{eq:cosine_distance}
% D(a, b) = 1 - \frac{\sum_{i=1}^{d} a_i b_i}{\sqrt{\sum_{i=1}^{d} a_i^2} \, \sqrt{\sum_{i=1}^{d} b_i^2}} .
% \end{equation}

% \noindent Here, \( a \) and \( b \) denote the vectors representing the student and teacher, respectively. Furthermore, this distance function quantifies the angular distance between the two vectors in the feature space, offering a measure of similarity or dissimilarity between their latent representations. This facilitates the knowledge transfer from the teacher to the student, thereby improving the student's training process.
% Given this general formulation of how \gls{lupi} can be integrated within the context of object detection, the next step is to address the nature and role of the privileged information itself.

\section{Privileged Information for Object Detection}
\label{sec:4_privileged_information}

Determining the appropriate form for privileged information that can be effectively used for object detection is challenging. Identifying the relevant information that enables detection networks to achieve improved results requires extensive research. Recent studies have delved into similar aspects of this issue, aiming to understand how humans detect objects in a scene \cite{object_detection_philospy}. In \cite{object_detection_philospy}, it was highlighted that \textit{physical reasoning}, such as the \textit{centre of mass}, is a key factor humans rely on when locating objects. This suggests that physical reasoning and vision are intertwined, rather than one process solely depending on the other. The relationship between the two is sufficiently strong that physical reasoning can predict the location of objects more effectively than processes like spatial memory and attention.
In related studies in deep learning, saliency \cite{itti, deepgaze} and depth prediction \cite{depth_anything, dpt_large} outputs were analysed to determine which sources most strongly correlate with object detection \cite{bartolo2024correlationobjectdetectionperformance}. The results showed that saliency prediction had a stronger correlation with the object detection problem \cite{bartolo2024correlationobjectdetectionperformance}. Building on these findings, a preliminary experiment was conducted to assess several renowned saliency methods, including the Itti \cite{itti} and DeepGaze IIE \cite{deepgaze} models, as well as popular depth prediction models such as Depth Anything \cite{depth_anything} and DPT-Large \cite{dpt_large}. Figures \ref{fig:channels} and \ref{fig:preliminary_privileged} illustrate the findings. However, these single-channel privileged information sources did not prove to be particularly successful in improving detection performance.

To further investigate the types of privileged information that could be beneficial in this regard, inspiration was drawn from psychological concepts related to human perception \cite{itti_ior, spotlight}. Specifically, the spotlight principle was investigated in the context of identifying privileged information that could potentially improve detection accuracy \cite{spotlight}. According to \cite{spotlight}, spatially directed attention significantly improves visual perceptual processing. Drawing from this attention spotlight principle in the human visual cortex \cite{spotlight}, this study explored how a similar concept might be applied to object detection, albeit in a different format. The underlying idea is straightforward: \textit{given an image, how can an object detection network be guided to focus on areas containing the objects of interest?}
Although the network already receives guidance through the ground-truth bounding boxes during training, the study considered whether an additional channel could be integrated to streamline and aid the search process. It is within this context that this study proposes a bounding box mask as a potential solution.

\begin{figure}[!htbp]
  \centering
  \begin{tabular}{cc}
    \includegraphics[width=0.48\textwidth, height=5cm]{gray_image.jpg} &
    \includegraphics[width=0.48\textwidth, height=5cm]{normal_image.jpg} \\
    \small (a) & \small (b) \\
  \end{tabular}\\
  \caption{Visual illustration of generated privileged information: (a) a three-channel RGB image from the \gls{soda} dataset \cite{soda_dataset}, and (b) the corresponding generated privileged information represented by a single-channel bounding box mask image.}
  \label{fig:privileged_visual}
\end{figure}

The bounding box mask image proposed in this study serves as a structured form of privileged information, developed to support object detection networks across both localisation and classification tasks. The generated image consists of a black background, representing non-object regions. Over this, bounding boxes are drawn to indicate the locations of objects, thereby addressing the localisation problem. To simultaneously encode class-specific information, each bounding box is filled with a distinct shade of grey corresponding to its class label in order to tackle the classification problem. This approach is visually exemplified in Figure \ref{fig:privileged_visual}.

Moreover, a single-channel grayscale format in the range $[0, 255]$ was used to represent the generated privileged information image. This kept the representation simple, preserving essential structure and meaning, without adding extra computational load. This single-channel format was selected in preference to RGB representations, as it simplifies the data without compromising the richness of the information conveyed. The algorithm used to generate this form of privileged input is presented in Algorithm \ref{alg:boundingBoxMask}. Special attention was given to reducing object occlusion within the privileged mask. To achieve this, a sorting step was introduced before rendering the bounding boxes onto the black mask image. The boxes were ordered in descending size, allowing the larger ones to be drawn first. This approach helped to reduce the likelihood of smaller objects being obscured.

% \begin{algorithm}
%     \caption{Generating Bounding Box Mask (Privileged Information) for Object Detection}
%     \label{alg:boundingBoxMask}
%     \begin{algorithmic}
%         \State \textbf{Input:} \( \text{bounding boxes}, \, \text{labels}, \, \text{image width}, \, \text{image height}, \, \text{num classes} \)
%         \State \textbf{Output:} Bounding Box Mask Image (Single Channel)
        
%         \State 1. Create a black image of size \( (\text{image width}, \text{image height}) \) with pixel intensities all 0
%         \State 2. Load the annotations: bounding boxes, labels for multi-label detection
%         \State 3. Create a distinct grayscale colour distribution in the range of 55 to 255 for each label, ensuring no overlapping colours and dark shades
%         \State 4. Sort the annotations by bounding box area (largest first) to prevent object occlusions
        
%         \For{each annotation \( i \) in the set of annotations}
%             \State 5. Extract the bounding box coordinates \( (b_x, b_y, b_w, b_h) \) and label for the current annotation
%             \State 6. Assign a unique grayscale colour to the current label from the predefined colour distribution
%             \State 7. Draw the bounding box on the image using the assigned grayscale colour
%         \EndFor
        
%         \State 8. Save the bounding box mask as a single-channel grayscale image
%     \end{algorithmic}
% \end{algorithm}
\begin{algorithm}
    \caption{Generating Bounding Box Mask (Privileged Information) for Object Detection}
    \label{alg:boundingBoxMask}
    \begin{algorithmic}
        \State \textbf{Input:} Bounding boxes, labels, image width, height, number of classes
        \State \textbf{Output:} Single-channel grayscale bounding box mask
        
        \State 1. Initialise a blank grayscale image of size \( (\text{width}, \text{height}) \)
        \State 2. Load annotations: bounding boxes and class labels
        \State 3. Assign distinct grayscale values in range $[55, 255]$ to each class label to avoid overlap
        \State 4. Sort annotations by bounding box area (descending order) to reduce occlusion
        
        \For{each annotation \( i \)}
            \State 5. Extract bounding box coordinates \( (b_x, b_y, b_w, b_h) \) and label \( l \)
            \State 6. Retrieve grayscale value corresponding to \( l \)
            \State 7. Fill the box region in the image with this value
        \EndFor
        
        \State 8. Save as a single-channel grayscale mask image
    \end{algorithmic}
\end{algorithm}

%-
In addition to this, polygon or segmentation masks are often regarded in the literature \cite{mask_to_annotation, lupi_od_extra1, lupi_od_extra2} as offering greater precision in object representation. However, in this study, the bounding box mask was preferred over these alternatives for two principal reasons. The first is that the majority of object detection datasets are annotated using bounding boxes rather than polygon formats. Consequently, obtaining a polygon mask from a bounding box would require the use of a segmentation model such as \cite{maskrcnn, deeplabv3, sam, sam2} to predict the polygon mask. Such a prediction cannot be regarded as ground truth and would therefore introduce an additional error surface.

The second reason concerns the control of the error margin. The aim was to improve object detection accuracy without adding unnecessary complexity by introducing shape details that could confuse the model. Preliminary experiments with polygon masks indicated further challenges, particularly in handling overlapping objects. Since these were not represented as squares, identifying the object’s corners and thereby locating its centre proved difficult. Nevertheless, the potential of polygon or segmentation masks as privileged information for object detection merits further investigation in future studies.


\section{Deep Learning-Based Object Detection Architectures}
\label{sec:4_distillation_architectures}
% Model Architectures (model selection), explain (don't have to say anything about yolo, used architectures open source, torchvision footnotes, pretrained models trained on COCO just mention) subsection for each one one page for each, 

A structured methodological framework was devised to investigate the influence of \gls{lupi} in object detection and assess whether it provides measurable improvements in accuracy, without necessitating increasingly complex or computationally demanding architectures, to address the first objective (\textbf{O1}). This framework relied on utilising pre-trained models with few structural adjustments, as briefly hinted in the preceding sections, thereby presenting a methodology capable of accommodating the integration of \gls{lupi} into existing detection pipelines to tackle the second objective (\textbf{O2}).
To ensure scientific rigour and relevance, a selection of renowned models was employed to test this hypothesis--models that feature prominently in object detection literature and are also commonly used in litter detection (see Section \ref{sec:3_litter}). Five architectures were selected for their distinct algorithmic principles and practical relevance: Faster \gls{rcnn} \cite{rcnn}, \gls{ssd} \cite{ssd}, RetinaNet \cite{retinanet}, SSDLite \cite{ssdlite}, and \gls{fcos} \cite{fcos}. All of these models are open-source and publicly available through the \verb|torchvision|\footnote{https://pytorch.org/vision/main/models.html\#object-detection} library, with pre-trained weights sourced from the \gls{coco} dataset. Furthermore, these pre-trained configurations provided a uniform baseline for all subsequent experimental comparisons.

Although the study drew upon the full suite of pre-trained object detection models offered through the \verb|torchvision| \gls{api}, these five were selected particularly for their representational breadth and relevance across differing detection paradigms. Together, these models cover both one-stage and two-stage detection paradigms, as well as lightweight and computationally intensive architectures, providing a balanced basis for assessing the contribution of \gls{lupi}. Their selection ensured that any findings are not confined to the characteristics of a single architecture but instead speak to broader trends observable across different model types.



%--

\section{Learning Using Privileged Information for Object Detection}
\label{sec:4_lupi4od}

% Having defined five key object detection architectures relevant to this study, each based on a distinct detection strategy, and having introduced privileged information as an informative input during training, the next step is to combine these elements. Combining these elements into a single pipeline allows for investigating the \gls{lupi} paradigm within existing object detection frameworks. The approach distinguishes between the privileged information available during training and the limited input used during test time, intending to improve accuracy without increasing model size.

% In this setup, privileged information is only available while the model is being trained and not during inference. In light of this, using a teacher–student dynamic becomes necessary, as was done in similar studies \cite{lab2wild, lupi_classification}. The teacher model is trained on both the RGB images (\gls{x}) and the privileged information (\gls{x_star}), along with the target labels (\gls{y}), as is typical in most object detection approaches. With access to this extra information, the teacher can develop a more refined understanding of the target objects.
% By contrast, the student model receives only the RGB images (\gls{x}) at inference. During training, it has access to both the images (\gls{x}) and the labels (\gls{y}), but not the privileged information. This mirrors the conditions expected during real-world deployment, so the student must learn without relying on data that will not be present at test time.

% The main goal is to improve the student by using the richer knowledge learnt by the teacher. Through distillation, the teacher transfers essential knowledge gained from the privileged information, enabling the student model to achieve a level of performance comparable to that of a model trained with access to additional input--despite the student never having access to that input during inference.
% To achieve this, the object detection architectures mentioned earlier can be adapted. By making minor adjustments to the original design, a teacher–student model can be created using the same network architecture. For instance, a RetinaNet teacher model could be used to train a RetinaNet student model.
Having established the architectures and defined the privileged information for this task, the next stage involved integrating this information into each model through a unified training pipeline. This is done by adapting each architecture to support a teacher–student training setup.
In this pipeline, the teacher model is trained using RGB images (\gls{x}), privileged information (\gls{x_star}), and the corresponding labels (\gls{y}). The student model, sharing the same architecture, is trained only with RGB images and labels, mimicking the input conditions expected at deployment. This setup ensures that the student does not rely on any modality unavailable at test time.
%-

Knowledge transfer between the teacher and student models is performed through distillation, which allows the student to learn from the teacher’s enriched representations derived from bounding box mask privileged information. To ensure methodological consistency and isolate the effect of privileged information, both the teacher and student models employ identical architectures. This approach simplifies the alignment of intermediate features or output logits during training and eliminates architectural differences as a confounding factor. Consequently, any observed improvements in detection performance can be confidently attributed to the incorporation of privileged information rather than variations in model design. For example, a RetinaNet teacher model is exclusively paired with a RetinaNet student model, avoiding cross-architecture training scenarios such as pairing a Faster \gls{rcnn} student with a RetinaNet teacher.


\subsection{Teacher Network}
\label{subsec:4_teacher}

To incorporate privileged information, the teacher network was modified to accept both the RGB image and its corresponding bounding box mask. Since conventional models typically process three-channel RGB inputs, the input layer was adapted to accept a four-channel tensor, combining the RGB data with the single-channel mask. More generally, the architecture was extended to handle inputs with \gls{channels} channels, accommodating additional privileged sources. This modification affected the initial convolutional layer, requiring reinitialisation of its weights. To maintain consistency with pre-trained parameters and ensure stable training, Kaiming Normal initialisation \cite{kaiming} was applied to the adjusted layer.

\subsection{Student Network}
\label{subsec:4_student}

With the teacher network established, the subsequent step involved defining the student network. While the student learns under the supervision of the teacher, it does not receive any privileged information as input. Consequently, the input layer of the student network remained unaltered, allowing the direct use of a standard pre-trained object detection model. To incorporate teacher supervision during training, however, the student’s loss function had to be adjusted to include a distillation term, as described in Equation \ref{eq:lupi_loss_function}. 

To enable meaningful feature-level distillation, this approach required the selection of a shared internal layer from both networks, one that captures semantically rich representations and allows for direct comparison. In this study, the final backbone layer was selected for distillation, in line with existing approaches in this area \cite{lab2wild, lupi_distillation, distillation2}. Specifically, for architectures such as Faster \gls{rcnn}, RetinaNet, and \gls{fcos}, which rely on the ResNet-FPN backbone, this corresponded to the final convolutional layer preceding the \gls{fpn}. The feature layer used for distillation in \gls{ssd}, and SSDLite was taken just before the auxiliary detection heads, corresponding to the final layer of the VGG-16 and MobileNetv3 backbones, respectively.
%-

While previous studies have commonly employed the final backbone layer for knowledge transfer, it is important to acknowledge that this choice may not be optimal across all network architectures. The varying designs of \glspl{fpn} and detection heads introduce complexities that might be better addressed by distillation applied at multiple levels or different points within the model. Intermediate layers, for example, may contain distinct or complementary features that the final backbone layer does not capture, potentially bolstering the knowledge transfer process. Due to practical limitations and to preserve consistency across experiments, this investigation did not evaluate alternative distillation layers or experiment with different architectural points for knowledge transfer. Nonetheless, identifying the most suitable layer or combination of layers for knowledge transfer in each architecture represents a valuable avenue for future investigation. Given the variability in architectural design, their response to distillation may differ, and further research could refine and improve the current approach.

Once the appropriate layer was selected, the corresponding latent feature representations were extracted during training from both the student and the teacher. These outputs were then flattened into vectors, allowing for a similarity comparison using the Cosine Distance (\gls{D}) function. This metric quantifies the angular divergence between the two feature vectors, offering a precise scalar measure of their alignment in feature space. The resulting distance value was then integrated into the student’s total loss function. As outlined in Equation \ref{eq:lupi_loss_function}, an \gls{alpha} parameter controls the influence of this distillation loss, balancing the learning from direct supervision against the soft guidance offered by the teacher.

\subsection{Proposed Architecture}
\label{subsec:4_architecture}
% Grazzi Sinjur Alla, Ahfirli Sinjur Alla, u Ismaghni Sinjur Alla

\begin{figure}[!b]
    \centering
    \includegraphics[width=1\textwidth]{Architecture LUPIv3.png}
    \caption{General architecture for integrating the \gls{lupi} paradigm into any object detection model. The diagram illustrates the incorporation of both RGB images and bounding box masks as inputs to the teacher network, the use of a standard RGB input for the student network, the selection of the final backbone layer for knowledge distillation, and the generation of output predictions from the student model.}
    \label{fig:lupi_architecture}% Resized to 200%
\end{figure}

Bringing together all the elements described in this section, the incorporation of privileged information into any object detection model can be formally represented through the architecture depicted in Figure \ref{fig:lupi_architecture}. This method rests on two core alterations: adapting the teacher network to process both RGB images and the corresponding privileged input and introducing a loss adjustment in the student model to enable distillation. These adjustments are deliberately kept minimal to prevent unnecessary architectural complexity or increased computational demands. The required changes are confined to modifying the input layer of the teacher model and integrating a distillation mechanism during the student training. Moreover, generating the privileged information, such as bounding box masks, remains a simple and direct process.
% \begin{figure}[!htbp] % Overflow since problem with small font
%     \centering
%     \hspace*{-0.1\textwidth}  % Adjust the negative value as needed to slightly overflow
%     \includegraphics[width=1.2\textwidth]{Architecture LUPIv2.png}
%     \caption{Architecture of the object detection models, illustrating the use of the LUPI paradigm with RGB and bounding box mask inputs, the teacher and student networks, the final backbone layer for knowledge distillation, and the output predictions.}
%     \label{fig:lupi_architecture}
% \end{figure}

%-
Training an optimised detection model in this study began with the teacher network, which was configured to learn from a richer four-channel input composed of RGB data and privileged information. Once the teacher was trained, its parameters were frozen, and it served as a guide for the student model. During this training phase, the student processed only the standard RGB images, ensuring no modifications to the existing detection architecture while strictly preventing any data leakage from privileged information into the student model or inference process. Feature representations were drawn from the final layer of the backbone in both networks. Using identical architectures for the teacher and student simplifies implementation, as it guarantees that the resulting feature vectors are dimensionally aligned and directly comparable.


% Glorja lil Missier, lil Iben u lil Ispirtu s-Santu, Kif kien mill Bidu, Issa u ghal Dejjem ta' Dejjem. Amen. Grazzi hafna Mulej

\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{Architecture LUPIv5.png}
\caption{Detailed architecture of the training setup showing the teacher network receiving both RGB images and privileged input channels. The student network processes only RGB data but is trained with additional supervision through knowledge distillation. A baseline RGB-only model is included for comparison.}
\label{fig:lupi_architecture_final}
\end{figure}

The similarity between the latent feature representations—obtained by compressing feature maps into continuous vectors—was measured using a cosine distance function (see Equation \ref{eq:cosine_distance}), producing a distillation loss that captures how closely the student aligns with the teacher. This loss was backpropagated through the student model alongside standard detection losses such as regression and classification, as well as additional losses specific to each architecture, including the \gls{fpn} loss for Faster \gls{rcnn} and the centerness loss for \gls{fcos}. The influence of the teacher’s guidance on the overall training objective was controlled by a tunable parameter, \gls{alpha}, which determines the weight given to the distillation term. Once training was complete, the teacher model was no longer needed, as privileged information, such as bounding box ground truth, cannot be accessed during inference when detections must be produced autonomously. Nevertheless, the student model, having learned under the teacher’s supervision, retains the benefits of this privileged knowledge despite relying solely on standard RGB inputs at test time.


To complement the overview presented in Figure~\ref{fig:lupi_architecture}, Figure~\ref{fig:lupi_architecture_final} offers a more detailed and structured illustration of the training framework. This expanded diagram does not revisit the mathematical details but clarifies the data flow and knowledge distillation process. It highlights that the teacher model receives a richer input, combining standard RGB channels with additional privileged information. Meanwhile, both teacher and student models operate in parallel, with the loss computation (refer to Equation \ref{eq:lupi_loss_function}) abstracted into a simplified and more interpretable visual form.

The baseline model, relying exclusively on standard RGB input, serves as a conventional point of reference within this framework. The emphasis remains on contrasting this baseline with the student model, which, although identical in architecture during inference, achieves improved performance through the additional supervision provided by the teacher during training.


\section{Training Parameters}
\label{sec:4_training_params}
% Glorja lil Missier, lil Iben u lil-Ispirtu s-Santu. Kif kien mill-bidu issa u dejjem, ghal dejjem ta' dejjem. Amen

To evaluate the proposed methodology across the five selected object detection models, a uniform experimental training protocol was employed throughout, in alignment with the second objective (\textbf{O2}). The primary aim was to determine whether the integration of the \gls{lupi} framework could yield improvements in detection accuracy without imposing additional computational demands on the baseline architectures. To ensure that any observed improvements in performance could be attributed solely to the inclusion of \gls{lupi}, the training configuration was intentionally kept simple and consistent. Each model was trained for $100$ epochs using the \gls{adam} optimiser \cite{adam_optimizer}, with a fixed learning rate of \(1 \times 10^{-3}\).
Preliminary tests compared alternative optimisers and learning rates \cite{sgd_optimizer}. Although the \gls{sgd} optimiser was initially considered, \gls{adam} achieved comparable convergence to the same minimum but with greater efficiency. Various learning rates were also explored, including $0.1, 0.01, 0.001,$ \(1 \times 10^{-3}\), and \(1 \times 10^{-4}\). Among these, \(1 \times 10^{-3}\) consistently provided the most stable convergence across the selected models. 
In addition, an early stopping callback was applied based on validation loss, with a patience threshold of eight epochs. A model checkpointing mechanism was also used to retain the weights corresponding to the best-performing epoch, thereby minimising the risk of overfitting.
Moreover, following standard practice when training object detection models for a given application or dataset \cite{yolov12, soda_dataset, fasterrcnn, yolov10}, all models were initialised with pre-trained weights from the \gls{coco} dataset. The detection heads were subsequently adapted to match the number of target classes present in the specific dataset used during training.

\section{Pre-processing Steps}
\label{sec:4_preprocessing}
% Sliema ghalik Marija, bil-Grazzja mimlija, Imbierek il frott tal guf tieghek Gesu', Qaddisa Marija Omm Alla, Itlob ghalina l-mindibin, issa u fis siegha tal-mewt taghna. Amen

In addition to the training parameters, a consistent set of preprocessing steps was applied across all experiments to maintain comparability of results. These transformations applied to the input images of the detection networks include image normalisation, resizing, and per-channel standardisation, each of which is outlined in detail below.

\subsection{Min-Max Normalisation}
\label{subsec:4_normalisation}

The initial step involves normalising the image pixels to a uniform scale. This is done to ensure equal weighting across all image inputs and channels. Both the RGB inputs and the additional privileged information channels are scaled using Min-Max normalisation \cite{min_max_normalisation}, as described below:

\begin{equation} \label{eq:min_max}
I_{\text{norm}} = \frac{I - I_{\text{min}}}{I_{\text{max}} - I_{\text{min}}} .
\end{equation}

\noindent Here, \gls{image} refers to the original image, \gls{imin} and \gls{imax} are the minimum and maximum pixel intensity values computed across the entire dataset for each respective channel. The inputs are scaled to lie within the range $[0, 1]$, thereby normalising the input space for both the teacher and student networks. Crucially, normalisation is performed separately for the RGB images and the privileged information channels to maintain the distinct statistical properties of each modality. 

\subsection{Image Resizing}
\label{subsec:4_resizing}

Following normalisation, all images are resized to a fixed resolution of $800 \times 800$ pixels. This resizing step allows for the formation of input tensors with consistent dimensions, a prerequisite for batch training in \gls{cnn}. The resolution of $800$ pixels was selected as a compromise across the models employed in this study.
Although models such as \gls{ssd} and SSDLite typically operate on smaller input resolutions (e.g., $300 \times 300$ or $320 \times 320$), detectors like Faster \gls{rcnn}, RetinaNet, and \gls{fcos} adopt a minimum resize of $800$ pixels. The choice of a larger image resolution also contributes to preserving fine-grained features, which is especially relevant for detecting small-scale objects that might otherwise lose distinctive characteristics if downsampled excessively.

\subsection{Channel-Wise Standardisation}
\label{subsec:4_standardisation}

The final step in the pre-processing pipeline involves statistical normalisation by subtracting the channel-wise mean and dividing by the standard deviation. These parameters are computed over the training dataset and are applied individually to each channel, including both RGB and privileged input. This transformation ensures zero-mean and unit-variance inputs, which helps stabilise training and accelerate convergence \cite{min_max_normalisation}. This standardisation is carried out automatically by the model during training, based on mean and standard deviation values calculated from the specified dataset. The computed mean and standard deviation values for each training dataset and input channel are summarised in Table~\ref{tab:channel_stats}, with the corresponding datasets explored in Section~\ref{sec:4_datasets}.

\begin{table}[ht]
    \centering
    \begin{tabular}{llcc}
        \toprule
        \textbf{Dataset} & \textbf{Channel} & \textbf{Mean} & \textbf{Standard Deviation} \\
        \midrule
        \multirow{4}{*}{\textbf{SODA Litter (1-Metre Altitude)}} 
            & Red               & 0.434 & 0.261 \\
            & Green             & 0.406 & 0.249 \\
            & Blue              & 0.320 & 0.238 \\
            & Bounding Box Mask & 0.144 & 0.135 \\
        \midrule
        \multirow{4}{*}{\textbf{SODA Litter (All Altitudes)}} 
            & Red               & 0.467 & 0.255 \\
            & Green             & 0.430 & 0.240 \\
            & Blue              & 0.357 & 0.233 \\
            & Bounding Box Mask & 0.021 & 0.129 \\
        \midrule
        \multirow{4}{*}{\textbf{Pascal VOC 2012}} 
            & Red               & 0.452 & 0.275 \\
            & Green             & 0.431 & 0.273 \\
            & Blue              & 0.399 & 0.284 \\
            & Bounding Box Mask & 0.142 & 0.216 \\
        \bottomrule
    \end{tabular}
    \caption{Channel-wise mean and standard deviation values for the Red, Green, Blue, and Bounding Box Mask channels, computed over the training sets of the Pascal VOC 2012 and SODA Litter datasets.}
    \label{tab:channel_stats}
\end{table}

Overall, these pre-processing transformations were kept deliberately minimal, consistent, and aligned with the general standards in object detection research to ensure that the influence of the \gls{lupi} methodology could be fairly and transparently evaluated without extraneous confounding factors.

\section{Post-processing Steps}
\label{sec:4_postprocessing}
% Missierna li into fis Smewwiet, Jitqaddes Ismek, tigi Saltnatek, Ikun dak li tried into, hekk fis Semma Hekk da fl-art. Hobzna ta' kuljum ghatina illum, Ahfrilna dnubietna Bhalma Nahfru dak li Huwa Hati' Ghalina. La ddahalniex fit tigrib izda ehlisna mid deni. Amen

In addition to the pre-processing techniques applied to the input images, a consistent post-processing step was introduced to ensure uniformity across all five selected models. This step employed non-maximum suppression \cite{nms}, a widely used method for refining object detection results by eliminating redundant predictions.
The suppression process ranks all predicted bounding boxes according to their confidence scores. When two or more boxes significantly overlap, only the one with the highest score is retained. In this study, the significant overlap was defined using an \gls{iou} threshold of $0.5$. Any box exceeding this level of overlap with a higher-scoring prediction was removed by the \gls{nms}.
Applying this method consistently was essential for a fair evaluation. Some of the chosen models already had \gls{nms} integrated internally, while others required an external implementation. By enforcing a common post-processing step, the outputs became directly comparable, regardless of any differences in model architecture.

\section{Datasets}
\label{sec:4_datasets}
% definition of dataset - check old thesis
% litter detection for uav hard problem, 
% uav based dataset highlighted in lit review chosen, but HAIDA annotations could not be downloaded
% mention why they were selected available etc. . ., mention roboflow
% images for each one

The object detection problem, which is regarded as a supervised learning task, necessitates the use of datasets for both training and validating detection models. These datasets typically consist of image sequences annotated with ground truth bounding boxes and class labels, specifying the type and location of each object \cite{pascal-voc-2012, coco}.
To evaluate the proposed methodological architecture in alignment with the third objective (\textbf{O3}), \gls{uav}-based litter detection was chosen as the target problem to assess the feasibility of the proposed approach. This problem presents a considerable challenge due to the need to detect small objects in varied terrain and under differing altitudes \cite{soda_dataset}. Moreover, the complexity of this task makes it a suitable benchmark for evaluating both the generalisability and precision of the proposed framework.

Currently, only four \gls{uav}-based litter detection datasets are available (see Table \ref{tab:lit_review}): \gls{bdw} \cite{bdwdataset}, UAVVaste \cite{uavvaste}, HAIDA \cite{haida}, and \gls{soda} \cite{soda_dataset}. Among these datasets, \gls{bdw}\footnote{https://universe.roboflow.com/bottles-in-the-wild}, UAVVaste\footnote{https://universe.roboflow.com/mcast/uavvaste-avcle}, and \gls{soda}\footnote{https://universe.roboflow.com/soda-dataset/} are accessible through the Roboflow dataset annotation platform. Although the HAIDA dataset is publicly available on Github\footnote{https://github.com/LiaoSteve/HAIDA-Trash-Dataset-High-Resolution-Aerial-image}, it could not be used due to inaccessible annotations. As a result, the evaluation focused on the three datasets available through Roboflow. In addition, the well-established Pascal VOC 2012 dataset \cite{pascal-voc-2012}, also accessible on Roboflow\footnote{https://public.roboflow.com/object-detection/pascal-voc-2012/1}, was included to give a broader perspective. It was chosen to test the proposed approach for multi-label detection across a larger variety of classes.


\subsection{SODA: Small Objects at Different Altitudes}
\label{subsec:4_soda}

As discussed in Subsection \ref{subsec:3_sodadataset}, \gls{soda} serves as a representative \gls{uav}-based litter detection dataset. It includes six litter classes and contains image subsets captured at a range of altitudes, which reflects more realistic operating conditions for \gls{uav}-based detection.
However, as shown in Table \ref{tab:soda_annotation_summary}, one of the key issues with this dataset is class imbalance. Certain categories, such as \textit{drink can} and \textit{clear plastic bottle}, contain significantly more annotations than others. There is also an uneven distribution of images across altitudes, with noticeably fewer samples at higher elevations.
Despite these limitations, \gls{soda} remains a valuable dataset in this area of research. Among the three datasets selected in this study, \gls{soda} covers the widest range of altitudes.

\begin{table}[ht]
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \renewcommand{\arraystretch}{2}%1.5
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
        \hline
        \textbf{\gls{agl} (m)} & \textbf{Total Images} & \textbf{Total Annotations} & \textbf{Drink Can} & \textbf{Clear Plastic Bottle} & \textbf{Glass Bottle} & \textbf{Other Plastic Bottle} & \textbf{Drink Carton} & \textbf{Glass Jar} \\
        \hline \hline
        1  & 452 & 712  & 256 & 180 & 96  & 71  & 54  & 55 \\
        \hline
        5  & 202 & 1839 & 837 & 429 & 183 & 152 & 136 & 102 \\
        \hline
        10 & 50  & 1138 & 491 & 268 & 128 & 106 & 88  & 57 \\
        \hline
        15 & 35  & 956  & 434 & 232 & 93  & 87  & 67  & 43 \\
        \hline
        20 & 30  & 763  & 324 & 199 & 68  & 79  & 57  & 36 \\
        \hline
        25 & 30  & 709  & 327 & 174 & 46  & 79  & 57  & 26 \\
        \hline
        30 & 30  & 602  & 270 & 149 & 36  & 82  & 55  & 10 \\
        \hline \hline
        \textbf{Total (Incl. 1m)} & \textbf{829} & \textbf{6719} & \textbf{2939} & \textbf{1631} & \textbf{650} & \textbf{656} & \textbf{514} & \textbf{329} \\
        \hline
        \textbf{Total (Excl. 1m)} & \textbf{377} & \textbf{6007} & \textbf{2683} & \textbf{1451} & \textbf{554} & \textbf{585} & \textbf{460} & \textbf{274} \\
        \hline
    \end{tabular}
    \renewcommand{\arraystretch}{1}%1.5
    \end{adjustbox}
    \caption{Summary of annotated objects across different altitudes \gls{agl} in the \gls{soda} dataset. (Source: \cite{soda_dataset})}
    \label{tab:soda_annotation_summary}
\end{table}

In the original publications that introduced the SODA dataset and used it for training detection models \cite{soda_dataset, detect_litter}, the authors adopted a tiling technique \cite{tiling, sahi_detection}. This method aimed to improve the detection of small objects, particularly in images taken at higher altitudes, by magnifying specific regions. In \cite{detect_litter}, they applied a 5$\times$5 tiling strategy, resizing each tile to 640$\times$640 pixels to match the input dimensions required by the \gls{yolo}v8 \cite{yolov8} network employed in their study.

\begin{figure}[!htbp]
  \centering

  % Full-width image
  \includegraphics[width=0.98\textwidth]{tiling1.jpg} \\
  \small (a)

  \vspace{1em}

  % Second row of two images
  \begin{tabular}{cc}
    \includegraphics[width=0.48\textwidth, height=4.5cm]{tiling2.png} &
    \includegraphics[width=0.48\textwidth, height=4.5cm]{tiling3.png} \\
    \small (b) & \small (c) \\
  \end{tabular}

  \vspace{1em}

  % Third row of two images
  \begin{tabular}{cc}
    \includegraphics[width=0.48\textwidth, height=4.5cm]{tiling4.png} &
    \includegraphics[width=0.48\textwidth, height=4.5cm]{tiling5.png} \\
    \small (d) & \small (e) \\
  \end{tabular}

  \caption{Visual illustration highlighting the need for tiling. (a) Annotated image of litter captured at 15 metres altitude from the SODA dataset \cite{soda_dataset}; (b) 5$\times$5 tiling of the original image, with each tile resized to 640$\times$640 pixels; (c) original image resized directly to 640$\times$640 pixels; (d) the 13\textsuperscript{th} tile from the tiling approach, resized; (e) the corresponding region extracted from the non-tiled image.}

  \label{fig:tiling_examples}
\end{figure}

To understand why such an approach becomes necessary, visual exploration of the data proves useful. Figure \ref{fig:tiling_examples} illustrates this clearly. Subfigure (a) presents an annotated image captured at 15 metres altitude, which is half the maximum altitude in the dataset. If the entire image is resized directly to 640$\times$640 pixels without tiling, as shown in subfigure (c), the result is a significant loss in feature detail. This version represents the full image as it would be input to the detection network in one pass.
Applying tiling, shown in subfigure (b), divides the original image into smaller sections. Taking the 13\textsuperscript{th} tile--resized and displayed in subfigure (d)--as an example, it shows litter with greater clarity and scale. This makes detection of litter from \gls{uav} footage easier. A side-by-side comparison with the corresponding region from the non-tiled image (subfigure (e)) highlights the reduction in granularity that occurs without tiling.
However, this improved resolution comes at the cost of longer inference times \cite{tiling}, since the model must process several images rather than one. As a result, selecting appropriate tiling parameters involves balancing detection accuracy with computational efficiency.

% Dataset Statistics
\begin{figure}[!b]
  \centering
  \begin{tabular}{c}
    \includegraphics[width=0.6\textwidth]{image_distribution1.png} \\
    \small (a) \\
    \addlinespace[1em]
    \includegraphics[width=0.6\textwidth]{image_distribution2.png} \\
    \small (b) \\
  \end{tabular}
  \caption{\gls{soda} dataset image distribution: (a) before 3$\times$3 tiling, and (b) after 3$\times$3 tiling.}
  \label{fig:soda_image_distribution}
\end{figure}
For this study, a 3$\times$3 grid was used to tile the \gls{soda} dataset. This choice is supported by the experimental findings discussed in Section \ref{sec:5_tiling_exp}. Additionally, a resize dimension of 800 pixels was applied, based on the rationale presented in Subsection \ref{subsec:4_resizing}. No data augmentation techniques were introduced to maintain simplicity and ensure that any observed improvements could be attributed directly to the proposed methodology. As a result, the total number of images and annotations increased substantially. The original version of the \gls{soda} dataset contained 829 images. After tiling, this figure rose to 7,461. The image distribution across the category split is presented in Figure~\ref{fig:soda_image_distribution}. Notably, despite the application of these pre-processing steps, the original ratio between the training, validation, and testing sets remains consistent before and after tiling.

\begin{figure}[!b]
  \centering
  \begin{tabular}{c}
    \includegraphics[width=0.6\textwidth]{annotation_heatmap1.png} \\
    \small (a) \\
    \addlinespace[1em]
    \includegraphics[width=0.6\textwidth]{annotation_heatmap2.png} \\
    \small (b) \\
  \end{tabular}
  \caption{\gls{soda} dataset annotation heatmap: (a) before 3$\times$3 tiling, and (b) after 3$\times$3 tiling.}
  \label{fig:soda_annotation_heatmap}
\end{figure}

Additionally, with the use of tiling, the spatial distribution of object annotations changed significantly. As the number of images increased, the objects became larger and more defined, helping to mitigate the annotation bias present in the original \gls{soda} dataset. This is illustrated in Figure~\ref{fig:soda_annotation_heatmap}. In subfigure (a), the heatmap before tiling shows a strong centre bias. After tiling, shown in subfigure (b), this bias is less pronounced, with a more even distribution across the heatmap, though some focus on the centre remains. The pixel intensities also increased due to the larger number of images and annotations.

\begin{figure}[!ht]
  \centering
  \begin{tabular}{c}
    \includegraphics[width=1\textwidth]{annotation_distribution1.png} \\
    \small (a) \\
    \addlinespace[1em]
    \includegraphics[width=1\textwidth]{annotation_distribution2.png} \\
    \small (b) \\
  \end{tabular}
  \caption{\gls{soda} dataset annotation distribution per category and split: (a) before 3$\times$3 tiling, and (b) after 3$\times$3 tiling.}
  \label{fig:soda_annotation_distribution}
\end{figure}

Tiling unintentionally worsened the class imbalance. As images were split into smaller sections, some object bounding boxes were also divided. This was especially the case for objects near the edges or those covering a larger area. Common categories like \textit{drink can} and \textit{clear plastic bottle}, which were already more frequent, appeared even more often after tiling. This shift is reflected in the category distribution shown in Figure \ref{fig:soda_annotation_distribution}.
Meanwhile, less frequent classes such as \textit{glass jar} and \textit{drink carton} showed a marginal increase in representation. Interestingly, \textit{glass bottle} surpassed Other \textit{plastic bottle} in the number of annotations, reversing their relative frequencies before tiling.

% \clearpage

\begin{figure}[!ht]% Values taken from TIling Experiment which was more reliable
  \centering
  \begin{tabular}{c}
    \includegraphics[width=1\textwidth]{object_size_distribution1.png} \\
    \small (a) \\
    \addlinespace[1em]
    \includegraphics[width=1\textwidth]{object_size_distribution2.png} \\
    \small (b) \\
  \end{tabular}
  \caption{\gls{soda} dataset object size distribution: (a) before 3$\times$3 tiling, and (b) after 3$\times$3 tiling.}
  \label{fig:soda_object_size_distribution}
\end{figure}

The increase in annotations due to tiling has had a significant impact on the overall distribution of object sizes. Using the object size classifications from the \gls{coco} dataset \cite{coco}, where objects are defined as small (less than 32$\times$32 pixels), medium (ranging from 32$\times$32 to 96$\times$96 pixels), and large (greater than 96$\times$96 pixels), a clear shift in object size distribution can be observed. As shown in Figure~\ref{fig:soda_object_size_distribution}, while the total number of objects has increased, the proportion of small objects has decreased, with a corresponding rise in both medium and large objects. This shift in distribution suggests an improvement in the dataset’s size balance, which should improve the detection of previously smaller objects. With the larger objects now more prominent, the detection models can benefit from improved size constraints, making these objects easier to detect.



In summary, tiling has proven to be quite beneficial in improving the detection of objects in the \gls{soda} dataset, particularly for smaller objects. While it helped reduce some of the bias in annotations, it also introduced new challenges, especially regarding class imbalance. This tiling setup has been utilised to magnify and address the issue of small litter detection for the chosen case study, which will be used in the experiments discussed in Section \ref{sec:5_within_dataset_exp}.


\subsection{BDW: Bottle Detection in the Wild}
\label{subsec:4_bdw}

The \gls{bdw} dataset \cite{bdwdataset}, mentioned in Subsection \ref{subsec:3_bdw}, contains 25,407 images taken at altitudes between 10 and 30 metres, but it focuses only on detecting bottles. For this study, this dataset will be used solely for evaluation, as it only covers the representation of bottles, thus limiting the generalisability of the results. Therefore, only the testing set, consisting of 5,078 images, will be used to evaluate the models. Although the dataset claims to include images captured at higher altitudes, the objects in these images, as shown in Figure \ref{fig:bdw_samples}, appear relatively large and clear. This suggests that an image-splitting technique may have been applied during data collection.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.32\textwidth]{bdw1.jpg}
  \hfill
  \includegraphics[width=0.32\textwidth]{bdw2.jpg}
  \hfill
  \includegraphics[width=0.32\textwidth]{bdw3.jpg}
  \caption{Sample images from the test subset of the \gls{bdw} dataset, illustrating the general nature of the data. (Source: \cite{bdwdataset})}
  \label{fig:bdw_samples}
\end{figure}


\subsection{UAVVaste}
\label{subsec:4_uavvaste}

UAVVaste \cite{uavvaste}, the final publicly available \gls{uav}-based litter detection dataset chosen for this study, and as discussed in Subsection \ref{subsec:3_uavvaste}, contains 772 images captured at unspecified low altitudes, with various forms of litter annotated under a single waste category. In this study, the dataset will be used exclusively for evaluation, as its focus on low-altitude imagery limits generalisation. Only the testing set, containing 77 images, will be used to evaluate the trained models. A visual inspection of the dataset, as shown in Figure \ref{fig:uavvaste_samples}, suggests the images were likely captured at altitudes between 5 and 10 metres \gls{agl}.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.48\textwidth]{uavvaste1.jpg}
  \hfill
  \includegraphics[width=0.48\textwidth]{uavvaste2.jpg}
  \caption{Sample images from the test subset of the UAVVaste dataset, illustrating the general nature of the data. (Source: \cite{uavvaste})}
  \label{fig:uavvaste_samples}
\end{figure}


\subsection{Pascal Visual Object Classes}
\label{subsec:4_pascal_voc}

\begin{figure}[!b]% Place Figure at Bottom of Page
    \centering
    \includegraphics[width=1\textwidth]{pascalvoc2012images.jpg}
    \caption{Sample images from the Pascal VOC 2012 dataset, showcasing the diversity in object categories included in the dataset. (Source: \cite{pascal-voc-2012})}
    \label{fig:pascal_voc}% Resized to 200%
\end{figure}

Pascal \gls{voc} is a widely used benchmark dataset in object detection, featuring 20 object categories including people, animals, vehicles, and various household items \cite{pascal-voc-2012}. Figure \ref{fig:pascal_voc} shows sample images that illustrate the diversity of the dataset. The dataset includes images annotated with object class labels, bounding boxes, and pixel-level segmentation masks, enabling its use across multiple computer vision tasks. In this study, the Pascal \gls{voc} 2012 dataset is used for both training and validation to test the proposed methodology beyond the context of \gls{uav}-based litter detection, per the fourth objective (\textbf{O4}). This helps assess whether the approach remains effective across a broader set of object classes. 

The dataset utilised in this study, sourced from Roboflow, contains 17,112 images in total. Of these, 13,690 are assigned to the training set, and 3,422 are designated for validation. However, the Pascal VOC 2012 dataset also suffers from class imbalance, as shown in Figure~\ref{fig:class_distribution_pascal_voc}. There is a disproportionately high number of instances labelled as \textit{person} compared to other categories. While the remaining classes show smaller gaps among themselves, class imbalance is still present, with \textit{bus} being the least represented category.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{annotation_distribution_pascal.png}
    \caption{Class distribution of the Pascal VOC 2012 dataset, highlighting the imbalance across different categories, with the number of objects per class from both the training and validation splits.}
    \label{fig:class_distribution_pascal_voc}% Resized to 200%
\end{figure}


\section{Conclusion}
\label{sec:4_conclusion}
% Grazzi Sinjur Alla, Ahfirli Sinjur Alla u Ismaghni Sinjur Alla. Grazzi, Amen.

% This chapter has provided a structured framework for understanding the integration of \gls{lupi} in object detection, beginning with a formal mathematical formulation and the theoretical context surrounding the problem. It has established the conceptual basis for incorporating privileged information and defined its role within this context. The discussion then turned to the object detection models selected for this study, detailing their structural components and architectural configurations while outlining the modifications made to facilitate the construction of teacher and student networks. The implementation details, including training parameters and the consistent application of pre-processing and post-processing steps, were also covered. Furthermore, the chapter examined the datasets used in this research, highlighting the necessary pre-processing steps involved and analysing the structure of the data in relation to the proposed evaluation methodology. Finally, the chapter concluded with the presentation of the evaluation metrics that will be used to assess model performance, providing a foundation for the experiments in the following chapter.
This chapter has provided a structured framework for understanding the integration of \gls{lupi} in object detection, beginning with a formal mathematical formulation and the theoretical context surrounding the problem. It has established the conceptual basis for incorporating privileged information and defined its role within this context. The discussion then turned to the object detection models selected for this study, outlining the modifications made to facilitate the construction of teacher and student networks. The implementation details, including training parameters and the consistent application of pre-processing and post-processing steps, were also covered. Furthermore, the chapter examined the datasets used in this research, highlighting the necessary pre-processing steps involved and analysing the structure of the data in relation to the proposed evaluation methodology.

%--