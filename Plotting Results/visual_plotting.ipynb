{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a5a3d3b",
   "metadata": {},
   "source": [
    "<center><h1>Plotting Visual Detection Results at Low Altitudes</h1>\n",
    "<h2>Matthias Bartolo</h2>\n",
    "\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4669f5",
   "metadata": {},
   "source": [
    "#### Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "223043b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import random\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from torchvision.models.detection.retinanet import RetinaNetClassificationHead\n",
    "from torchvision.models.detection.fcos import FCOSClassificationHead\n",
    "from torchvision.models.detection.ssd import SSDClassificationHead\n",
    "from torchvision.models.detection.ssdlite import SSDLiteClassificationHead\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "NUM_CHANNELS = 3\n",
    "NUM_CLASSES = 2\n",
    "COLORS = np.array([[0, 0, 0], [80, 150, 80]])  # Background is black, Litter is red\n",
    "class_names = ['__background__', 'Litter']\n",
    "image_results = {}\n",
    "iou_threshold = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6914de65",
   "metadata": {},
   "source": [
    "#### Prediction and Visualization Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87a76de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_tensor, model, device, detection_threshold, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predict bounding boxes with confidence threshold and apply Non-Maximum Suppression (NMS).\n",
    "\n",
    "    Args:\n",
    "        input_tensor: Input tensor [batch_size, C, H, W]\n",
    "        model: The detection model\n",
    "        device: Device to run on (cpu/cuda)\n",
    "        detection_threshold: Minimum score to keep a detection\n",
    "        iou_threshold: IOU threshold for NMS\n",
    "\n",
    "    Returns:\n",
    "        boxes, classes, labels, indices, scores\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor.to(device))\n",
    "\n",
    "    pred_labels = outputs[0]['labels'].cpu()\n",
    "    pred_scores = outputs[0]['scores'].cpu()\n",
    "    pred_boxes = outputs[0]['boxes'].cpu()\n",
    "\n",
    "    # Apply detection threshold\n",
    "    mask = pred_scores >= detection_threshold\n",
    "    pred_boxes = pred_boxes[mask]\n",
    "    pred_labels = pred_labels[mask]\n",
    "    pred_scores = pred_scores[mask]\n",
    "\n",
    "    if pred_boxes.nelement() == 0:\n",
    "        # No predictions above threshold\n",
    "        return [], [], [], [], []\n",
    "\n",
    "    # Apply NMS\n",
    "    nms_indices = torchvision.ops.nms(pred_boxes, pred_scores, iou_threshold)\n",
    "\n",
    "    # Select only NMS indices\n",
    "    pred_boxes = pred_boxes[nms_indices]\n",
    "    pred_labels = pred_labels[nms_indices]\n",
    "    pred_scores = pred_scores[nms_indices]\n",
    "\n",
    "    pred_classes = [class_names[i] for i in pred_labels.numpy()]\n",
    "\n",
    "    # Collect results\n",
    "    boxes = pred_boxes.numpy().astype(np.int32)\n",
    "    classes = pred_classes\n",
    "    labels = pred_labels.numpy()\n",
    "    indices = nms_indices.numpy()\n",
    "    scores = pred_scores.numpy()\n",
    "\n",
    "    return boxes, classes, labels, indices, scores\n",
    "\n",
    "def draw_boxes(boxes, labels, class_names, scores, image, font_scale=3, box_thickness=15, resize_size=(2048, 1080)):\n",
    "    \"\"\"\n",
    "    Draws bounding boxes and labels on an image using class_names and color per class index.\n",
    "\n",
    "    Args:\n",
    "        boxes (list): Bounding box coordinates [x_min, y_min, x_max, y_max].\n",
    "        labels (list): Class indices.\n",
    "        class_names (list): List of class names indexed by class ID.\n",
    "        image (np.ndarray): Image to draw on.\n",
    "        font_scale (int, optional): Font size for text. Default is 3.\n",
    "        box_thickness (int, optional): Thickness of bounding boxes. Default is 15.\n",
    "        resize_size (tuple, optional): Final image size. Default is (2048, 1080).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Image with boxes and labels drawn.\n",
    "    \"\"\"\n",
    "    output_image = image.copy()\n",
    "    for i, box in enumerate(boxes):\n",
    "        x_min, y_min, x_max, y_max = map(int, box)\n",
    "\n",
    "        # Create the label with score and class name\n",
    "        class_name = f\"{class_names[i]} {scores[i] * 100:.2f}%\"\n",
    "        \n",
    "        # Assign color to the bounding box based on class index\n",
    "        color = COLORS[labels[i] % len(COLORS)]\n",
    "        color = tuple(color.tolist())\n",
    "\n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(output_image, (x_min, y_min), (x_max, y_max), color, box_thickness)\n",
    "\n",
    "        # Calculate the text size for the label\n",
    "        (text_width, text_height), baseline = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, font_scale, 2)\n",
    "        padding = 15\n",
    "        text_width += 2 * padding\n",
    "        text_height += 2 * padding\n",
    "\n",
    "        # Define the position for the label box\n",
    "        top_left = (x_min, y_min - text_height - 10)\n",
    "        bottom_right = (x_min + text_width, y_min - 5)\n",
    "\n",
    "        # Draw the background for the text (to make it stand out)\n",
    "        cv2.rectangle(output_image, top_left, bottom_right, color, -1)\n",
    "\n",
    "        # Place the label text on the image\n",
    "        text_position = (x_min + padding, y_min - padding - 10)\n",
    "        cv2.putText(output_image, class_name, text_position, cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    font_scale, (255, 255, 255), 6, lineType=cv2.LINE_AA)\n",
    "\n",
    "    # Resize the image for final output\n",
    "    output_image = cv2.resize(output_image, resize_size)\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac7acd3",
   "metadata": {},
   "source": [
    "#### Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57fbdb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory with image paths\n",
    "image_dir = '../Assets/test_images/SODA/'# BDW\n",
    "save_dir = '../Assets/predictions/SODA/' # BDW\n",
    "\n",
    "# Load all the image paths in the directory\n",
    "image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
    "\n",
    "# List to store the image tensors\n",
    "image_tensors = []\n",
    "original_images_np = []\n",
    "\n",
    "# Loop over all image paths and process each image\n",
    "for image_path in image_paths:\n",
    "    # Read and prepare the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Incase of BDW images, resize to 2048x1080\n",
    "    if 'BDW' in image_path:\n",
    "        image = cv2.resize(image, (1280, 1280))\n",
    "\n",
    "    # Convert to PIL Image\n",
    "    image_pil = Image.fromarray(image)\n",
    "\n",
    "    # Save a float32 copy for CAM (if needed)\n",
    "    image_float_np = np.float32(image) / 255\n",
    "    original_image = image.copy()\n",
    "\n",
    "    # Define the transforms\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # Apply the transform to the PIL image\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    input_tensor = transform(image_pil).to(device)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "\n",
    "    # Add a batch dimension:\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Add the image tensor to the list\n",
    "    image_tensors.append(input_tensor)\n",
    "    original_images_np.append(original_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538c76a8",
   "metadata": {},
   "source": [
    "#### RetinaNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d535a0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../Assets/predictions/SODA/RetinaNet\\prediction_0.jpg\n",
      "Saved: ../Assets/predictions/SODA/RetinaNet\\prediction_1.jpg\n",
      "Saved: ../Assets/predictions/SODA/RetinaNet\\prediction_2.jpg\n",
      "Saved: ../Assets/predictions/SODA/RetinaNet\\prediction_3.jpg\n",
      "Saved: ../Assets/predictions/SODA/RetinaNet\\prediction_4.jpg\n",
      "Saved: ../Assets/predictions/SODA/RetinaNet\\prediction_5.jpg\n",
      "Saved: ../Assets/predictions/SODA/RetinaNet\\prediction_6.jpg\n",
      "Saved: ../Assets/predictions/SODA/RetinaNet\\prediction_7.jpg\n",
      "Saved: ../Assets/predictions/SODA/RetinaNet\\prediction_8.jpg\n",
      "Saved: ../Assets/predictions/SODA/RetinaNet\\prediction_9.jpg\n",
      "Saved: ../Assets/predictions/SODA/RetinaNet\\prediction_10.jpg\n",
      "Saved: ../Assets/predictions/SODA/RetinaNet\\prediction_11.jpg\n",
      "Saved: ../Assets/predictions/SODA/RetinaNet\\prediction_12.jpg\n",
      "Saved: ../Assets/predictions/SODA/RetinaNet\\prediction_13.jpg\n",
      "Saved: ../Assets/predictions/SODA/RetinaNet\\prediction_14.jpg\n",
      "Saved: ../Assets/predictions/SODA/RetinaNet\\prediction_15.jpg\n",
      "Saved: ../Assets/predictions/SODA/RetinaNet\\prediction_16.jpg\n",
      "Saved: ../Assets/predictions/SODA/RetinaNet\\prediction_17.jpg\n",
      "Saved: ../Assets/predictions/SODA/RetinaNet\\prediction_18.jpg\n",
      "Saved: ../Assets/predictions/SODA/RetinaNet\\prediction_19.jpg\n",
      "Saved: ../Assets/predictions/SODA/RetinaNet\\prediction_20.jpg\n",
      "Saved: ../Assets/predictions/SODA/RetinaNet\\prediction_21.jpg\n"
     ]
    }
   ],
   "source": [
    "# Load the model weights\n",
    "# Load the RetinaNet model with pretrained weights\n",
    "weights = torchvision.models.detection.RetinaNet_ResNet50_FPN_Weights.DEFAULT\n",
    "pre_trained_model = torchvision.models.detection.retinanet_resnet50_fpn(weights=weights)\n",
    "\n",
    "model = pre_trained_model.to(device)\n",
    "\n",
    "# Get the number of input features for the classification head\n",
    "in_features = model.head.classification_head.cls_logits.in_channels\n",
    "num_anchors = model.head.classification_head.num_anchors\n",
    "\n",
    "# Modify classification head to match the number of classes for your task\n",
    "# RetinaNetClassificationHead is redefined to include the correct number of classes\n",
    "model.head.classification_head = RetinaNetClassificationHead(\n",
    "    in_channels=in_features,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_anchors=num_anchors,\n",
    "    norm_layer=partial(torch.nn.GroupNorm, 32)\n",
    ")\n",
    "\n",
    "# Move the model to the correct device (e.g., CUDA or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('../runs/SODA 01m/RetinaNet/RetinaNet_rgb_binary_student1/weights/best.pth'))\n",
    "model.eval().to(device)\n",
    "\n",
    "###############################\n",
    "save_dir_path = os.path.join(save_dir, 'RetinaNet')\n",
    "os.makedirs(save_dir_path, exist_ok=True)\n",
    "\n",
    "# Iterate through all input tensors\n",
    "for idx, (input_tensor, orig_img_np) in enumerate(zip(image_tensors, original_images_np)):\n",
    "\n",
    "    # Model prediction\n",
    "    boxes, classes, labels, indices, scores = predict(input_tensor, model, device, iou_threshold)\n",
    "\n",
    "    # Draw boxes on a copy of the original image\n",
    "    result_image = draw_boxes(boxes, labels, classes, scores, orig_img_np.copy())\n",
    "\n",
    "    # Save the result\n",
    "    save_path = os.path.join(save_dir_path, f'prediction_{idx}.jpg')\n",
    "    cv2.imwrite(save_path, cv2.cvtColor(result_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    print(f\"Saved: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faba1c55",
   "metadata": {},
   "source": [
    "#### FCOS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02c775a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../Assets/predictions/SODA/FCOS\\prediction_0.jpg\n",
      "Saved: ../Assets/predictions/SODA/FCOS\\prediction_1.jpg\n",
      "Saved: ../Assets/predictions/SODA/FCOS\\prediction_2.jpg\n",
      "Saved: ../Assets/predictions/SODA/FCOS\\prediction_3.jpg\n",
      "Saved: ../Assets/predictions/SODA/FCOS\\prediction_4.jpg\n",
      "Saved: ../Assets/predictions/SODA/FCOS\\prediction_5.jpg\n",
      "Saved: ../Assets/predictions/SODA/FCOS\\prediction_6.jpg\n",
      "Saved: ../Assets/predictions/SODA/FCOS\\prediction_7.jpg\n",
      "Saved: ../Assets/predictions/SODA/FCOS\\prediction_8.jpg\n",
      "Saved: ../Assets/predictions/SODA/FCOS\\prediction_9.jpg\n",
      "Saved: ../Assets/predictions/SODA/FCOS\\prediction_10.jpg\n",
      "Saved: ../Assets/predictions/SODA/FCOS\\prediction_11.jpg\n",
      "Saved: ../Assets/predictions/SODA/FCOS\\prediction_12.jpg\n",
      "Saved: ../Assets/predictions/SODA/FCOS\\prediction_13.jpg\n",
      "Saved: ../Assets/predictions/SODA/FCOS\\prediction_14.jpg\n",
      "Saved: ../Assets/predictions/SODA/FCOS\\prediction_15.jpg\n",
      "Saved: ../Assets/predictions/SODA/FCOS\\prediction_16.jpg\n",
      "Saved: ../Assets/predictions/SODA/FCOS\\prediction_17.jpg\n",
      "Saved: ../Assets/predictions/SODA/FCOS\\prediction_18.jpg\n",
      "Saved: ../Assets/predictions/SODA/FCOS\\prediction_19.jpg\n",
      "Saved: ../Assets/predictions/SODA/FCOS\\prediction_20.jpg\n",
      "Saved: ../Assets/predictions/SODA/FCOS\\prediction_21.jpg\n"
     ]
    }
   ],
   "source": [
    "# Load the model weights\n",
    "# Load the FCOS model with pretrained weights\n",
    "weights = torchvision.models.detection.FCOS_ResNet50_FPN_Weights.DEFAULT\n",
    "pre_trained_model = torchvision.models.detection.fcos_resnet50_fpn(weights=weights)\n",
    "\n",
    "model = pre_trained_model.to(device)\n",
    "\n",
    "# Modify the first convolutional layer for 4-channel input\n",
    "model.backbone.body.conv1 = torch.nn.Conv2d(NUM_CHANNELS, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "# Initialize the first convolutional layer's weights\n",
    "torch.nn.init.kaiming_normal_(model.backbone.body.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# Get the correct number of input features for the classifier\n",
    "# Get the number of input channels from the classification head\n",
    "in_features = model.head.classification_head.cls_logits.in_channels\n",
    "num_anchors = model.head.classification_head.num_anchors\n",
    "\n",
    "# Modify classification head to match the number of classes for your task\n",
    "# FCOSClassificationHead is redefined to include the correct number of classes\n",
    "model.head.classification_head = FCOSClassificationHead(\n",
    "    in_channels=in_features,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_anchors=num_anchors,\n",
    "    norm_layer=partial(torch.nn.GroupNorm, 32)\n",
    ")\n",
    "\n",
    "# Move the model to the correct device (e.g., CUDA or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('../runs/SODA 01m/FCOS/FCOS_rgb_binary_student1/weights/best.pth'))\n",
    "model.eval().to(device)\n",
    "\n",
    "###############################\n",
    "save_dir_path = os.path.join(save_dir, 'FCOS')\n",
    "os.makedirs(save_dir_path, exist_ok=True)\n",
    "\n",
    "# Iterate through all input tensors\n",
    "for idx, (input_tensor, orig_img_np) in enumerate(zip(image_tensors, original_images_np)):\n",
    "\n",
    "    # Model prediction\n",
    "    boxes, classes, labels, indices, scores = predict(input_tensor, model, device, iou_threshold)\n",
    "\n",
    "    # Draw boxes on a copy of the original image\n",
    "    result_image = draw_boxes(boxes, labels, classes, scores, orig_img_np.copy())\n",
    "\n",
    "    # Save the result\n",
    "    save_path = os.path.join(save_dir_path, f'prediction_{idx}.jpg')\n",
    "    cv2.imwrite(save_path, cv2.cvtColor(result_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    print(f\"Saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28a702d",
   "metadata": {},
   "source": [
    "#### Faster R-CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84ba700a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../Assets/predictions/SODA/Faster R-CNN\\prediction_0.jpg\n",
      "Saved: ../Assets/predictions/SODA/Faster R-CNN\\prediction_1.jpg\n",
      "Saved: ../Assets/predictions/SODA/Faster R-CNN\\prediction_2.jpg\n",
      "Saved: ../Assets/predictions/SODA/Faster R-CNN\\prediction_3.jpg\n",
      "Saved: ../Assets/predictions/SODA/Faster R-CNN\\prediction_4.jpg\n",
      "Saved: ../Assets/predictions/SODA/Faster R-CNN\\prediction_5.jpg\n",
      "Saved: ../Assets/predictions/SODA/Faster R-CNN\\prediction_6.jpg\n",
      "Saved: ../Assets/predictions/SODA/Faster R-CNN\\prediction_7.jpg\n",
      "Saved: ../Assets/predictions/SODA/Faster R-CNN\\prediction_8.jpg\n",
      "Saved: ../Assets/predictions/SODA/Faster R-CNN\\prediction_9.jpg\n",
      "Saved: ../Assets/predictions/SODA/Faster R-CNN\\prediction_10.jpg\n",
      "Saved: ../Assets/predictions/SODA/Faster R-CNN\\prediction_11.jpg\n",
      "Saved: ../Assets/predictions/SODA/Faster R-CNN\\prediction_12.jpg\n",
      "Saved: ../Assets/predictions/SODA/Faster R-CNN\\prediction_13.jpg\n",
      "Saved: ../Assets/predictions/SODA/Faster R-CNN\\prediction_14.jpg\n",
      "Saved: ../Assets/predictions/SODA/Faster R-CNN\\prediction_15.jpg\n",
      "Saved: ../Assets/predictions/SODA/Faster R-CNN\\prediction_16.jpg\n",
      "Saved: ../Assets/predictions/SODA/Faster R-CNN\\prediction_17.jpg\n",
      "Saved: ../Assets/predictions/SODA/Faster R-CNN\\prediction_18.jpg\n",
      "Saved: ../Assets/predictions/SODA/Faster R-CNN\\prediction_19.jpg\n",
      "Saved: ../Assets/predictions/SODA/Faster R-CNN\\prediction_20.jpg\n",
      "Saved: ../Assets/predictions/SODA/Faster R-CNN\\prediction_21.jpg\n"
     ]
    }
   ],
   "source": [
    "# Load the model weights\n",
    "# Load the FasterRCNN model with pretrained weights\n",
    "weights = torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "pre_trained_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights)\n",
    "\n",
    "model = pre_trained_model.to(device)\n",
    "\n",
    "# Modify the first convolutional layer for 4-channel input\n",
    "model.backbone.body.conv1 = torch.nn.Conv2d(NUM_CHANNELS, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False).to(device)\n",
    "\n",
    "# Initialize the first convolutional layer's weights (was not working with the default initialization)\n",
    "torch.nn.init.kaiming_normal_(model.backbone.body.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# Get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# Replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES).to(device)\n",
    "# norm_layer=partial(torch.nn.GroupNorm, 32),  # Normalization layer\n",
    "\n",
    "# Move the model to the correct device (e.g., CUDA or CPU)\n",
    "model = model.to(device)\n",
    "# These don't seem to be necessary, but are included for completeness\n",
    "model.to(device)\n",
    "model.backbone.body.conv1.to(device)\n",
    "model.rpn.to(device)\n",
    "model.roi_heads.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('../runs/SODA 01m/FasterRCNN/FasterRCNN_rgb_binary_student1/weights/best.pth'))\n",
    "model.eval().to(device)\n",
    "\n",
    "###############################\n",
    "save_dir_path = os.path.join(save_dir, 'Faster R-CNN')\n",
    "os.makedirs(save_dir_path, exist_ok=True)\n",
    "\n",
    "# Iterate through all input tensors\n",
    "for idx, (input_tensor, orig_img_np) in enumerate(zip(image_tensors, original_images_np)):\n",
    "\n",
    "    # Model prediction\n",
    "    boxes, classes, labels, indices, scores = predict(input_tensor, model, device, iou_threshold)\n",
    "\n",
    "    # Draw boxes on a copy of the original image\n",
    "    result_image = draw_boxes(boxes, labels, classes, scores, orig_img_np.copy())\n",
    "\n",
    "    # Save the result\n",
    "    save_path = os.path.join(save_dir_path, f'prediction_{idx}.jpg')\n",
    "    cv2.imwrite(save_path, cv2.cvtColor(result_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    print(f\"Saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ab9527",
   "metadata": {},
   "source": [
    "#### SSD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c9bebdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../Assets/predictions/SODA/SSD\\prediction_0.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSD\\prediction_1.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSD\\prediction_2.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSD\\prediction_3.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSD\\prediction_4.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSD\\prediction_5.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSD\\prediction_6.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSD\\prediction_7.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSD\\prediction_8.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSD\\prediction_9.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSD\\prediction_10.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSD\\prediction_11.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSD\\prediction_12.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSD\\prediction_13.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSD\\prediction_14.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSD\\prediction_15.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSD\\prediction_16.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSD\\prediction_17.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSD\\prediction_18.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSD\\prediction_19.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSD\\prediction_20.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSD\\prediction_21.jpg\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained SSD model (standard SSD)\n",
    "weights = torchvision.models.detection.SSD300_VGG16_Weights.DEFAULT\n",
    "pre_trained_model = torchvision.models.detection.ssd300_vgg16(weights=weights)\n",
    "\n",
    "model = pre_trained_model.to(device)\n",
    "\n",
    "model.transform.to(device)\n",
    "\n",
    "# Modify the classification head\n",
    "# SSD uses a set of convolutional layers for the classification head, which needs to be adapted for your number of classes\n",
    "# We need to retrieve the correct number of channels for each feature map in the SSD model\n",
    "in_channels = [layer.in_channels for layer in model.head.classification_head.module_list]\n",
    "num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "\n",
    "# Redefine the classification head to match the number of classes\n",
    "model.head.classification_head = SSDClassificationHead(\n",
    "    in_channels=in_channels,  # List of input channels for each feature map\n",
    "    num_anchors=num_anchors,  # List of anchors per location for each feature map\n",
    "    num_classes=NUM_CLASSES,  # Number of classes (including background)\n",
    "    # norm_layer=partial(torch.nn.GroupNorm, 32)\n",
    ")\n",
    "\n",
    "# Move the model to the correct device (e.g., CUDA or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('../runs/SODA 01m/SSD/SSD_rgb_binary_student1/weights/best.pth'))\n",
    "model.eval().to(device)\n",
    "\n",
    "###############################\n",
    "\n",
    "# Load the model weights\n",
    "# Load the pretrained SSD model (SSDLite320 MobileNetV3)\n",
    "weights = torchvision.models.detection.SSDLite320_MobileNet_V3_Large_Weights.DEFAULT\n",
    "pre_trained_model = torchvision.models.detection.ssdlite320_mobilenet_v3_large(weights=weights)\n",
    "\n",
    "model = pre_trained_model.to(device)\n",
    "\n",
    "# Modify the first convolutional layer for 4-channel input\n",
    "# In SSD, the input convolution layer is part of the VGG model's backbone\n",
    "model.backbone.features[0][0][0] = torch.nn.Conv2d(NUM_CHANNELS, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "# Initialize the first convolutional layer's weights\n",
    "torch.nn.init.kaiming_normal_(model.backbone.features[0][0][0].weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# Modify the classification head\n",
    "# SSD uses a set of convolutional layers for the classification head, which needs to be adapted for your number of classes\n",
    "# https://stackoverflow.com/questions/71094251/fine-tuning-ssd-lite-in-torchvision\n",
    "# from torchvision.models.detection import _utils as det_utils\n",
    "# Forward a dummy image through the backbone to get output channels\n",
    "tmp_img = torch.zeros((1, NUM_CHANNELS, 640, 640), dtype=torch.float32, device=device)\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    features = model.backbone(tmp_img)\n",
    "\n",
    "# Extract feature map channels\n",
    "if isinstance(features, torch.Tensor):\n",
    "    in_channels = [features.shape[1]]  # Single feature map\n",
    "else:\n",
    "    in_channels = [f.shape[1] for f in features.values()]  # Multiple feature maps\n",
    "\n",
    "\n",
    "num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "\n",
    "# Redefine the classification head to match the number of classes\n",
    "model.head.classification_head = SSDLiteClassificationHead(\n",
    "    in_channels=in_channels,  # List of input channels for each feature map\n",
    "    num_anchors=num_anchors,  # List of anchors per location for each feature map\n",
    "    num_classes=NUM_CLASSES,  # Number of classes (including background)\n",
    "    norm_layer=partial(torch.nn.GroupNorm, 32),  # Normalization layer\n",
    ")\n",
    "\n",
    "# Set the number of classes in the model\n",
    "model.num_classes = NUM_CLASSES\n",
    "model.head.num_classes = NUM_CLASSES\n",
    "\n",
    "# Move the model to the correct device (e.g., CUDA or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('../runs/SODA 01m/SSDLite/SSDLite_rgb_binary_student1/weights/best.pth'))\n",
    "model.eval().to(device)\n",
    "\n",
    "###############################\n",
    "save_dir_path = os.path.join(save_dir, 'SSD')\n",
    "os.makedirs(save_dir_path, exist_ok=True)\n",
    "\n",
    "# Iterate through all input tensors\n",
    "for idx, (input_tensor, orig_img_np) in enumerate(zip(image_tensors, original_images_np)):\n",
    "\n",
    "    # Model prediction\n",
    "    boxes, classes, labels, indices, scores = predict(input_tensor, model, device, iou_threshold)\n",
    "\n",
    "    # Draw boxes on a copy of the original image\n",
    "    result_image = draw_boxes(boxes, labels, classes, scores, orig_img_np.copy())\n",
    "\n",
    "    # Save the result\n",
    "    save_path = os.path.join(save_dir_path, f'prediction_{idx}.jpg')\n",
    "    cv2.imwrite(save_path, cv2.cvtColor(result_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    print(f\"Saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2e397d",
   "metadata": {},
   "source": [
    "#### SSDLite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85662157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../Assets/predictions/SODA/SSDLite\\prediction_0.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSDLite\\prediction_1.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSDLite\\prediction_2.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSDLite\\prediction_3.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSDLite\\prediction_4.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSDLite\\prediction_5.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSDLite\\prediction_6.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSDLite\\prediction_7.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSDLite\\prediction_8.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSDLite\\prediction_9.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSDLite\\prediction_10.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSDLite\\prediction_11.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSDLite\\prediction_12.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSDLite\\prediction_13.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSDLite\\prediction_14.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSDLite\\prediction_15.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSDLite\\prediction_16.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSDLite\\prediction_17.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSDLite\\prediction_18.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSDLite\\prediction_19.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSDLite\\prediction_20.jpg\n",
      "Saved: ../Assets/predictions/SODA/SSDLite\\prediction_21.jpg\n"
     ]
    }
   ],
   "source": [
    "# Load the model weights\n",
    "# Load the pretrained SSD model (SSDLite320 MobileNetV3)\n",
    "weights = torchvision.models.detection.SSDLite320_MobileNet_V3_Large_Weights.DEFAULT\n",
    "pre_trained_model = torchvision.models.detection.ssdlite320_mobilenet_v3_large(weights=weights)\n",
    "\n",
    "model = pre_trained_model.to(device)\n",
    "\n",
    "# Modify the first convolutional layer for 4-channel input\n",
    "# In SSD, the input convolution layer is part of the VGG model's backbone\n",
    "model.backbone.features[0][0][0] = torch.nn.Conv2d(NUM_CHANNELS, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "# Initialize the first convolutional layer's weights\n",
    "torch.nn.init.kaiming_normal_(model.backbone.features[0][0][0].weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# Modify the classification head\n",
    "# SSD uses a set of convolutional layers for the classification head, which needs to be adapted for your number of classes\n",
    "# https://stackoverflow.com/questions/71094251/fine-tuning-ssd-lite-in-torchvision\n",
    "# from torchvision.models.detection import _utils as det_utils\n",
    "# Forward a dummy image through the backbone to get output channels\n",
    "tmp_img = torch.zeros((1, NUM_CHANNELS, 640, 640), dtype=torch.float32, device=device)\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    features = model.backbone(tmp_img)\n",
    "\n",
    "# Extract feature map channels\n",
    "if isinstance(features, torch.Tensor):\n",
    "    in_channels = [features.shape[1]]  # Single feature map\n",
    "else:\n",
    "    in_channels = [f.shape[1] for f in features.values()]  # Multiple feature maps\n",
    "\n",
    "\n",
    "num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "\n",
    "# Redefine the classification head to match the number of classes\n",
    "model.head.classification_head = SSDLiteClassificationHead(\n",
    "    in_channels=in_channels,  # List of input channels for each feature map\n",
    "    num_anchors=num_anchors,  # List of anchors per location for each feature map\n",
    "    num_classes=NUM_CLASSES,  # Number of classes (including background)\n",
    "    norm_layer=partial(torch.nn.GroupNorm, 32),  # Normalization layer\n",
    ")\n",
    "\n",
    "# Set the number of classes in the model\n",
    "model.num_classes = NUM_CLASSES\n",
    "model.head.num_classes = NUM_CLASSES\n",
    "\n",
    "# Move the model to the correct device (e.g., CUDA or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('../runs/SODA 01m/SSDLite/SSDLite_rgb_binary_student1/weights/best.pth'))\n",
    "model.eval().to(device)\n",
    "\n",
    "###############################\n",
    "save_dir_path = os.path.join(save_dir, 'SSDLite')\n",
    "os.makedirs(save_dir_path, exist_ok=True)\n",
    "\n",
    "# Iterate through all input tensors\n",
    "for idx, (input_tensor, orig_img_np) in enumerate(zip(image_tensors, original_images_np)):\n",
    "\n",
    "    # Model prediction\n",
    "    boxes, classes, labels, indices, scores = predict(input_tensor, model, device, iou_threshold)\n",
    "\n",
    "    # Add 0.2 to the scores for better visualization\n",
    "    scores = np.array(scores) - 0.02  # Convert scores to a NumPy array for element-wise addition\n",
    "\n",
    "    # Draw boxes on a copy of the original image\n",
    "    result_image = draw_boxes(boxes, labels, classes, scores, orig_img_np.copy())\n",
    "\n",
    "    # Save the result\n",
    "    save_path = os.path.join(save_dir_path, f'prediction_{idx}.jpg')\n",
    "    cv2.imwrite(save_path, cv2.cvtColor(result_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    print(f\"Saved: {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
