{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a5a3d3b",
   "metadata": {},
   "source": [
    "<center><h1>Plotting Visual Detection Results at High Altitudes</h1>\n",
    "<h2>Matthias Bartolo</h2>\n",
    "\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4669f5",
   "metadata": {},
   "source": [
    "#### Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "223043b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import random\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from torchvision.models.detection.retinanet import RetinaNetClassificationHead\n",
    "from torchvision.models.detection.fcos import FCOSClassificationHead\n",
    "from torchvision.models.detection.ssd import SSDClassificationHead\n",
    "from torchvision.models.detection.ssdlite import SSDLiteClassificationHead\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "NUM_CHANNELS = 3\n",
    "\n",
    "\"\"\"\n",
    "For Binary Detection:\n",
    "\"\"\"\n",
    "COLORS = np.array([[0, 0, 0], [80, 150, 80]])  # Background is black, Litter is red\n",
    "class_names = ['__background__', 'Litter']\n",
    "\n",
    "\"\"\"\n",
    "For Multi-Class Detection:\n",
    "\"\"\"\n",
    "# COLORS = np.array([\n",
    "#     [0, 0, 0],           # Background\n",
    "#     [80, 150, 80],       # Clear Plastic Bottle\n",
    "#     [150, 50, 50],       # Drink Can\n",
    "#     [160, 100, 40],      # Drink Carton\n",
    "#     [60, 60, 150],       # Glass Bottle\n",
    "#     [180, 100, 50],      # Glass Jar\n",
    "#     [100, 200, 150],     # Other Plastic Bottle\n",
    "# ])\n",
    "\n",
    "# class_names = [\n",
    "#     \"__background__\",\n",
    "#     \"Clear Plastic Bottle\",\n",
    "#     \"Drink Can\",\n",
    "#     \"Drink Carton\",\n",
    "#     \"Glass Bottle\",\n",
    "#     \"Glass Jar\",\n",
    "#     \"Other Plastic Bottle\",\n",
    "# ]\n",
    "\n",
    "NUM_CLASSES = len(class_names)\n",
    "\n",
    "MODEL_NAME = 'SODA_Dataset_Tiled_Single' # 'SODA_Dataset_Tiled_Multi'\n",
    "\n",
    "image_results = {}\n",
    "iou_threshold = 0.5\n",
    "tile_cols = 5#7 # For beautiful printing only\n",
    "tile_rows = 5#7 # ''\n",
    "\n",
    "# Apply the transform to the PIL image\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6914de65",
   "metadata": {},
   "source": [
    "#### Prediction and Visualization Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87a76de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_image, model, device, detection_threshold, iou_threshold=0.5, tile_rows=tile_cols, tile_cols=tile_rows):\n",
    "    \"\"\"\n",
    "    Apply object detection over a tiled 5x5 grid of the image, recombine results and apply NMS.\n",
    "\n",
    "    Args:\n",
    "        input_image (np.ndarray or PIL.Image): The full input image (H, W, C)\n",
    "        model: Object detection model\n",
    "        device: torch.device\n",
    "        detection_threshold (float): Confidence threshold\n",
    "        iou_threshold (float): IOU for NMS\n",
    "        tile_rows (int): Number of vertical tiles\n",
    "        tile_cols (int): Number of horizontal tiles\n",
    "\n",
    "    Returns:\n",
    "        boxes, classes, labels, indices, scores\n",
    "    \"\"\"\n",
    "    if isinstance(input_image, Image.Image):\n",
    "        input_image = np.array(input_image)\n",
    "\n",
    "    H, W, _ = input_image.shape\n",
    "    tile_h, tile_w = H // tile_rows, W // tile_cols\n",
    "\n",
    "    all_boxes, all_scores, all_labels = [], [], []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for row in range(tile_rows):\n",
    "            for col in range(tile_cols):\n",
    "                # Extract tile\n",
    "                y1, y2 = row * tile_h, (row + 1) * tile_h\n",
    "                x1, x2 = col * tile_w, (col + 1) * tile_w\n",
    "                tile = input_image[y1:y2, x1:x2, :]\n",
    "\n",
    "                # Resize tile to model input size\n",
    "                # tile = cv2.resize(tile, (800, 800))\n",
    "\n",
    "                # Prepare input\n",
    "                tile_tensor = torchvision.transforms.ToTensor()(tile).unsqueeze(0).to(device)\n",
    "\n",
    "                # Inference\n",
    "                outputs = model(tile_tensor)\n",
    "\n",
    "                # Filter by threshold\n",
    "                scores = outputs[0]['scores'].cpu()\n",
    "                keep = scores >= detection_threshold\n",
    "\n",
    "                boxes = outputs[0]['boxes'][keep].cpu().numpy()\n",
    "                labels = outputs[0]['labels'][keep].cpu().numpy()\n",
    "                scores = scores[keep].numpy()\n",
    "\n",
    "                # Shift boxes to full image coordinates\n",
    "                if boxes.size > 0:\n",
    "                    boxes[:, [0, 2]] += x1  # shift x\n",
    "                    boxes[:, [1, 3]] += y1  # shift y\n",
    "                    all_boxes.append(boxes)\n",
    "                    all_labels.append(labels)\n",
    "                    all_scores.append(scores)\n",
    "\n",
    "    if not all_boxes:\n",
    "        return [], [], [], [], []\n",
    "\n",
    "    # Concatenate all tile predictions\n",
    "    all_boxes = np.vstack(all_boxes)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_scores = np.concatenate(all_scores)\n",
    "\n",
    "    # Torch NMS expects tensors\n",
    "    boxes_tensor = torch.tensor(all_boxes, dtype=torch.float32)\n",
    "    scores_tensor = torch.tensor(all_scores)\n",
    "    labels_tensor = torch.tensor(all_labels)\n",
    "\n",
    "    # Run NMS on entire set\n",
    "    keep = torchvision.ops.nms(boxes_tensor, scores_tensor, iou_threshold)\n",
    "\n",
    "    final_boxes = boxes_tensor[keep].numpy().astype(np.int32)\n",
    "    final_scores = scores_tensor[keep].numpy()\n",
    "    final_labels = labels_tensor[keep].numpy()\n",
    "    final_classes = [class_names[i] for i in final_labels]\n",
    "    final_indices = keep.numpy()\n",
    "\n",
    "    return final_boxes, final_classes, final_labels, final_indices, final_scores\n",
    "\n",
    "def draw_boxes(boxes, labels, class_names, scores, image, font_scale=3, box_thickness=15, resize_size=(2048, 1080)):\n",
    "    \"\"\"\n",
    "    Draws bounding boxes and labels on an image using class_names and color per class index.\n",
    "\n",
    "    Args:\n",
    "        boxes (list): Bounding box coordinates [x_min, y_min, x_max, y_max].\n",
    "        labels (list): Class indices.\n",
    "        class_names (list): List of class names indexed by class ID.\n",
    "        image (np.ndarray): Image to draw on.\n",
    "        font_scale (int, optional): Font size for text. Default is 3.\n",
    "        box_thickness (int, optional): Thickness of bounding boxes. Default is 15.\n",
    "        resize_size (tuple, optional): Final image size. Default is (2048, 1080).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Image with boxes and labels drawn.\n",
    "    \"\"\"\n",
    "    output_image = image.copy()\n",
    "    for i, box in enumerate(boxes):\n",
    "        x_min, y_min, x_max, y_max = map(int, box)\n",
    "\n",
    "        # Create the label with score and class name\n",
    "        class_name = f\"{class_names[i]} {scores[i] * 100:.2f}%\"\n",
    "        \n",
    "        # Assign color to the bounding box based on class index\n",
    "        color = COLORS[labels[i] % len(COLORS)]\n",
    "        color = tuple(color.tolist())\n",
    "\n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(output_image, (x_min, y_min), (x_max, y_max), color, box_thickness)\n",
    "\n",
    "        # Calculate the text size for the label\n",
    "        (text_width, text_height), baseline = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, font_scale, 2)\n",
    "        padding = 15\n",
    "        text_width += 2 * padding\n",
    "        text_height += 2 * padding\n",
    "\n",
    "        # Define the position for the label box\n",
    "        top_left = (x_min, y_min - text_height - 10)\n",
    "        bottom_right = (x_min + text_width, y_min - 5)\n",
    "\n",
    "        # Draw the background for the text (to make it stand out)\n",
    "        cv2.rectangle(output_image, top_left, bottom_right, color, -1)\n",
    "\n",
    "        # Place the label text on the image\n",
    "        text_position = (x_min + padding, y_min - padding - 10)\n",
    "        cv2.putText(output_image, class_name, text_position, cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    font_scale, (255, 255, 255), 6, lineType=cv2.LINE_AA)\n",
    "\n",
    "    # Resize the image for final output\n",
    "    output_image = cv2.resize(output_image, resize_size)\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac7acd3",
   "metadata": {},
   "source": [
    "#### Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57fbdb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory with image paths\n",
    "image_dir = '../Assets/test_images/SODA High Altitude/'# UAVVaste\n",
    "save_dir = '../Assets/predictions/SODA High Altitude/' # UAVVaste\n",
    "\n",
    "# Load all the image paths in the directory\n",
    "image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
    "\n",
    "# List to store the image tensors\n",
    "image_tensors = []\n",
    "original_images_np = []\n",
    "\n",
    "# Loop over all image paths and process each image\n",
    "for image_path in image_paths:\n",
    "    # Read and prepare the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert to PIL Image\n",
    "    image_pil = Image.fromarray(image)\n",
    "\n",
    "    # Save a float32 copy for CAM (if needed)\n",
    "    image_float_np = np.float32(image) / 255\n",
    "    original_image = image.copy()\n",
    "\n",
    "    # Add the image tensor to the list\n",
    "    image_tensors.append(image_pil)\n",
    "    original_images_np.append(original_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538c76a8",
   "metadata": {},
   "source": [
    "#### RetinaNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d535a0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../Assets/predictions/SODA High Altitude/RetinaNet\\prediction_0.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/RetinaNet\\prediction_1.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/RetinaNet\\prediction_2.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/RetinaNet\\prediction_3.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/RetinaNet\\prediction_4.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/RetinaNet\\prediction_5.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/RetinaNet\\prediction_6.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/RetinaNet\\prediction_7.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/RetinaNet\\prediction_8.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/RetinaNet\\prediction_9.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/RetinaNet\\prediction_10.jpg\n"
     ]
    }
   ],
   "source": [
    "# Load the model weights\n",
    "# Load the RetinaNet model with pretrained weights\n",
    "weights = torchvision.models.detection.RetinaNet_ResNet50_FPN_Weights.DEFAULT\n",
    "pre_trained_model = torchvision.models.detection.retinanet_resnet50_fpn(weights=weights)\n",
    "\n",
    "model = pre_trained_model.to(device)\n",
    "\n",
    "# Get the number of input features for the classification head\n",
    "in_features = model.head.classification_head.cls_logits.in_channels\n",
    "num_anchors = model.head.classification_head.num_anchors\n",
    "\n",
    "# Modify classification head to match the number of classes for your task\n",
    "# RetinaNetClassificationHead is redefined to include the correct number of classes\n",
    "model.head.classification_head = RetinaNetClassificationHead(\n",
    "    in_channels=in_features,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_anchors=num_anchors,\n",
    "    norm_layer=partial(torch.nn.GroupNorm, 32)\n",
    ")\n",
    "\n",
    "# Move the model to the correct device (e.g., CUDA or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(f'../runs/RetinaNet/RetinaNet_{MODEL_NAME}_student1/weights/best.pth'))\n",
    "model.eval().to(device)\n",
    "\n",
    "###############################\n",
    "save_dir_path = os.path.join(save_dir, 'RetinaNet')\n",
    "os.makedirs(save_dir_path, exist_ok=True)\n",
    "\n",
    "# Iterate through all input tensors\n",
    "for idx, (input_tensor, orig_img_np) in enumerate(zip(image_tensors, original_images_np)):\n",
    "\n",
    "    # Model prediction\n",
    "    boxes, classes, labels, indices, scores = predict(input_tensor, model, device, iou_threshold)\n",
    "\n",
    "    # Draw boxes on a copy of the original image\n",
    "    result_image = draw_boxes(boxes, labels, classes, scores, orig_img_np.copy())\n",
    "\n",
    "    # Save the result\n",
    "    save_path = os.path.join(save_dir_path, f'prediction_{idx}.jpg')\n",
    "    cv2.imwrite(save_path, cv2.cvtColor(result_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    print(f\"Saved: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faba1c55",
   "metadata": {},
   "source": [
    "#### FCOS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02c775a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../Assets/predictions/SODA High Altitude/FCOS\\prediction_0.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/FCOS\\prediction_1.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/FCOS\\prediction_2.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/FCOS\\prediction_3.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/FCOS\\prediction_4.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/FCOS\\prediction_5.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/FCOS\\prediction_6.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/FCOS\\prediction_7.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/FCOS\\prediction_8.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/FCOS\\prediction_9.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/FCOS\\prediction_10.jpg\n"
     ]
    }
   ],
   "source": [
    "# Load the model weights\n",
    "# Load the FCOS model with pretrained weights\n",
    "weights = torchvision.models.detection.FCOS_ResNet50_FPN_Weights.DEFAULT\n",
    "pre_trained_model = torchvision.models.detection.fcos_resnet50_fpn(weights=weights)\n",
    "\n",
    "model = pre_trained_model.to(device)\n",
    "\n",
    "# Modify the first convolutional layer for 4-channel input\n",
    "model.backbone.body.conv1 = torch.nn.Conv2d(NUM_CHANNELS, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "# Initialize the first convolutional layer's weights\n",
    "torch.nn.init.kaiming_normal_(model.backbone.body.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# Get the correct number of input features for the classifier\n",
    "# Get the number of input channels from the classification head\n",
    "in_features = model.head.classification_head.cls_logits.in_channels\n",
    "num_anchors = model.head.classification_head.num_anchors\n",
    "\n",
    "# Modify classification head to match the number of classes for your task\n",
    "# FCOSClassificationHead is redefined to include the correct number of classes\n",
    "model.head.classification_head = FCOSClassificationHead(\n",
    "    in_channels=in_features,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_anchors=num_anchors,\n",
    "    norm_layer=partial(torch.nn.GroupNorm, 32)\n",
    ")\n",
    "\n",
    "# Move the model to the correct device (e.g., CUDA or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(f'../runs/FCOS/FCOS_{MODEL_NAME}_student1/weights/best.pth'))\n",
    "model.eval().to(device)\n",
    "\n",
    "###############################\n",
    "save_dir_path = os.path.join(save_dir, 'FCOS')\n",
    "os.makedirs(save_dir_path, exist_ok=True)\n",
    "\n",
    "# Iterate through all input tensors\n",
    "for idx, (input_tensor, orig_img_np) in enumerate(zip(image_tensors, original_images_np)):\n",
    "\n",
    "    # Model prediction\n",
    "    boxes, classes, labels, indices, scores = predict(input_tensor, model, device, iou_threshold)\n",
    "\n",
    "    # Draw boxes on a copy of the original image\n",
    "    result_image = draw_boxes(boxes, labels, classes, scores, orig_img_np.copy())\n",
    "\n",
    "    # Save the result\n",
    "    save_path = os.path.join(save_dir_path, f'prediction_{idx}.jpg')\n",
    "    cv2.imwrite(save_path, cv2.cvtColor(result_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    print(f\"Saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28a702d",
   "metadata": {},
   "source": [
    "#### Faster R-CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84ba700a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../Assets/predictions/SODA High Altitude/Faster R-CNN\\prediction_0.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/Faster R-CNN\\prediction_1.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/Faster R-CNN\\prediction_2.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/Faster R-CNN\\prediction_3.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/Faster R-CNN\\prediction_4.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/Faster R-CNN\\prediction_5.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/Faster R-CNN\\prediction_6.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/Faster R-CNN\\prediction_7.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/Faster R-CNN\\prediction_8.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/Faster R-CNN\\prediction_9.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/Faster R-CNN\\prediction_10.jpg\n"
     ]
    }
   ],
   "source": [
    "# Load the model weights\n",
    "# Load the FasterRCNN model with pretrained weights\n",
    "weights = torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "pre_trained_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights)\n",
    "\n",
    "model = pre_trained_model.to(device)\n",
    "\n",
    "# Modify the first convolutional layer for 4-channel input\n",
    "model.backbone.body.conv1 = torch.nn.Conv2d(NUM_CHANNELS, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False).to(device)\n",
    "\n",
    "# Initialize the first convolutional layer's weights (was not working with the default initialization)\n",
    "torch.nn.init.kaiming_normal_(model.backbone.body.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# Get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# Replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES).to(device)\n",
    "# norm_layer=partial(torch.nn.GroupNorm, 32),  # Normalization layer\n",
    "\n",
    "# Move the model to the correct device (e.g., CUDA or CPU)\n",
    "model = model.to(device)\n",
    "# These don't seem to be necessary, but are included for completeness\n",
    "model.to(device)\n",
    "model.backbone.body.conv1.to(device)\n",
    "model.rpn.to(device)\n",
    "model.roi_heads.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(f'../runs/FasterRCNN/FasterRCNN_{MODEL_NAME}_student1/weights/best.pth'))\n",
    "model.eval().to(device)\n",
    "\n",
    "###############################\n",
    "save_dir_path = os.path.join(save_dir, 'Faster R-CNN')\n",
    "os.makedirs(save_dir_path, exist_ok=True)\n",
    "\n",
    "# Iterate through all input tensors\n",
    "for idx, (input_tensor, orig_img_np) in enumerate(zip(image_tensors, original_images_np)):\n",
    "\n",
    "    # Model prediction\n",
    "    boxes, classes, labels, indices, scores = predict(input_tensor, model, device, iou_threshold)\n",
    "\n",
    "    # Draw boxes on a copy of the original image\n",
    "    result_image = draw_boxes(boxes, labels, classes, scores, orig_img_np.copy())\n",
    "\n",
    "    # Save the result\n",
    "    save_path = os.path.join(save_dir_path, f'prediction_{idx}.jpg')\n",
    "    cv2.imwrite(save_path, cv2.cvtColor(result_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    print(f\"Saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ab9527",
   "metadata": {},
   "source": [
    "#### SSD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c9bebdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../Assets/predictions/SODA High Altitude/SSD\\prediction_0.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/SSD\\prediction_1.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/SSD\\prediction_2.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/SSD\\prediction_3.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/SSD\\prediction_4.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/SSD\\prediction_5.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/SSD\\prediction_6.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/SSD\\prediction_7.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/SSD\\prediction_8.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/SSD\\prediction_9.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/SSD\\prediction_10.jpg\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained SSD model (standard SSD)\n",
    "weights = torchvision.models.detection.SSD300_VGG16_Weights.DEFAULT\n",
    "pre_trained_model = torchvision.models.detection.ssd300_vgg16(weights=weights)\n",
    "\n",
    "model = pre_trained_model.to(device)\n",
    "\n",
    "model.transform.to(device)\n",
    "\n",
    "# Modify the classification head\n",
    "# SSD uses a set of convolutional layers for the classification head, which needs to be adapted for your number of classes\n",
    "# We need to retrieve the correct number of channels for each feature map in the SSD model\n",
    "in_channels = [layer.in_channels for layer in model.head.classification_head.module_list]\n",
    "num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "\n",
    "# Redefine the classification head to match the number of classes\n",
    "model.head.classification_head = SSDClassificationHead(\n",
    "    in_channels=in_channels,  # List of input channels for each feature map\n",
    "    num_anchors=num_anchors,  # List of anchors per location for each feature map\n",
    "    num_classes=NUM_CLASSES,  # Number of classes (including background)\n",
    "    # norm_layer=partial(torch.nn.GroupNorm, 32)\n",
    ")\n",
    "\n",
    "# Move the model to the correct device (e.g., CUDA or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(f'../runs/SSD/SSD_{MODEL_NAME}_student1/weights/best.pth'))\n",
    "model.eval().to(device)\n",
    "\n",
    "###############################\n",
    "\n",
    "# Load the model weights\n",
    "# Load the pretrained SSD model (SSDLite320 MobileNetV3)\n",
    "weights = torchvision.models.detection.SSDLite320_MobileNet_V3_Large_Weights.DEFAULT\n",
    "pre_trained_model = torchvision.models.detection.ssdlite320_mobilenet_v3_large(weights=weights)\n",
    "\n",
    "model = pre_trained_model.to(device)\n",
    "\n",
    "# Modify the first convolutional layer for 4-channel input\n",
    "# In SSD, the input convolution layer is part of the VGG model's backbone\n",
    "model.backbone.features[0][0][0] = torch.nn.Conv2d(NUM_CHANNELS, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "# Initialize the first convolutional layer's weights\n",
    "torch.nn.init.kaiming_normal_(model.backbone.features[0][0][0].weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# Modify the classification head\n",
    "# SSD uses a set of convolutional layers for the classification head, which needs to be adapted for your number of classes\n",
    "# https://stackoverflow.com/questions/71094251/fine-tuning-ssd-lite-in-torchvision\n",
    "# from torchvision.models.detection import _utils as det_utils\n",
    "# Forward a dummy image through the backbone to get output channels\n",
    "tmp_img = torch.zeros((1, NUM_CHANNELS, 640, 640), dtype=torch.float32, device=device)\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    features = model.backbone(tmp_img)\n",
    "\n",
    "# Extract feature map channels\n",
    "if isinstance(features, torch.Tensor):\n",
    "    in_channels = [features.shape[1]]  # Single feature map\n",
    "else:\n",
    "    in_channels = [f.shape[1] for f in features.values()]  # Multiple feature maps\n",
    "\n",
    "\n",
    "num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "\n",
    "# Redefine the classification head to match the number of classes\n",
    "model.head.classification_head = SSDLiteClassificationHead(\n",
    "    in_channels=in_channels,  # List of input channels for each feature map\n",
    "    num_anchors=num_anchors,  # List of anchors per location for each feature map\n",
    "    num_classes=NUM_CLASSES,  # Number of classes (including background)\n",
    "    norm_layer=partial(torch.nn.GroupNorm, 32),  # Normalization layer\n",
    ")\n",
    "\n",
    "# Set the number of classes in the model\n",
    "model.num_classes = NUM_CLASSES\n",
    "model.head.num_classes = NUM_CLASSES\n",
    "\n",
    "# Move the model to the correct device (e.g., CUDA or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(f'../runs/SSDLite/SSDLite_{MODEL_NAME}_student1/weights/best.pth'))\n",
    "model.eval().to(device)\n",
    "\n",
    "###############################\n",
    "save_dir_path = os.path.join(save_dir, 'SSD')\n",
    "os.makedirs(save_dir_path, exist_ok=True)\n",
    "\n",
    "# Iterate through all input tensors\n",
    "for idx, (input_tensor, orig_img_np) in enumerate(zip(image_tensors, original_images_np)):\n",
    "\n",
    "    # Model prediction\n",
    "    boxes, classes, labels, indices, scores = predict(input_tensor, model, device, iou_threshold)\n",
    "\n",
    "    # Draw boxes on a copy of the original image\n",
    "    result_image = draw_boxes(boxes, labels, classes, scores, orig_img_np.copy())\n",
    "\n",
    "    # Save the result\n",
    "    save_path = os.path.join(save_dir_path, f'prediction_{idx}.jpg')\n",
    "    cv2.imwrite(save_path, cv2.cvtColor(result_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    print(f\"Saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2e397d",
   "metadata": {},
   "source": [
    "#### SSDLite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85662157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../Assets/predictions/SODA High Altitude/SSDLite\\prediction_0.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/SSDLite\\prediction_1.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/SSDLite\\prediction_2.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/SSDLite\\prediction_3.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/SSDLite\\prediction_4.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/SSDLite\\prediction_5.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/SSDLite\\prediction_6.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/SSDLite\\prediction_7.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/SSDLite\\prediction_8.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/SSDLite\\prediction_9.jpg\n",
      "Saved: ../Assets/predictions/SODA High Altitude/SSDLite\\prediction_10.jpg\n"
     ]
    }
   ],
   "source": [
    "# Load the model weights\n",
    "# Load the pretrained SSD model (SSDLite320 MobileNetV3)\n",
    "weights = torchvision.models.detection.SSDLite320_MobileNet_V3_Large_Weights.DEFAULT\n",
    "pre_trained_model = torchvision.models.detection.ssdlite320_mobilenet_v3_large(weights=weights)\n",
    "\n",
    "model = pre_trained_model.to(device)\n",
    "\n",
    "# Modify the first convolutional layer for 4-channel input\n",
    "# In SSD, the input convolution layer is part of the VGG model's backbone\n",
    "model.backbone.features[0][0][0] = torch.nn.Conv2d(NUM_CHANNELS, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "# Initialize the first convolutional layer's weights\n",
    "torch.nn.init.kaiming_normal_(model.backbone.features[0][0][0].weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# Modify the classification head\n",
    "# SSD uses a set of convolutional layers for the classification head, which needs to be adapted for your number of classes\n",
    "# https://stackoverflow.com/questions/71094251/fine-tuning-ssd-lite-in-torchvision\n",
    "# from torchvision.models.detection import _utils as det_utils\n",
    "# Forward a dummy image through the backbone to get output channels\n",
    "tmp_img = torch.zeros((1, NUM_CHANNELS, 640, 640), dtype=torch.float32, device=device)\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    features = model.backbone(tmp_img)\n",
    "\n",
    "# Extract feature map channels\n",
    "if isinstance(features, torch.Tensor):\n",
    "    in_channels = [features.shape[1]]  # Single feature map\n",
    "else:\n",
    "    in_channels = [f.shape[1] for f in features.values()]  # Multiple feature maps\n",
    "\n",
    "\n",
    "num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "\n",
    "# Redefine the classification head to match the number of classes\n",
    "model.head.classification_head = SSDLiteClassificationHead(\n",
    "    in_channels=in_channels,  # List of input channels for each feature map\n",
    "    num_anchors=num_anchors,  # List of anchors per location for each feature map\n",
    "    num_classes=NUM_CLASSES,  # Number of classes (including background)\n",
    "    norm_layer=partial(torch.nn.GroupNorm, 32),  # Normalization layer\n",
    ")\n",
    "\n",
    "# Set the number of classes in the model\n",
    "model.num_classes = NUM_CLASSES\n",
    "model.head.num_classes = NUM_CLASSES\n",
    "\n",
    "# Move the model to the correct device (e.g., CUDA or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(f'../runs/SSDLite/SSDLite_{MODEL_NAME}_student1/weights/best.pth'))\n",
    "model.eval().to(device)\n",
    "\n",
    "###############################\n",
    "save_dir_path = os.path.join(save_dir, 'SSDLite')\n",
    "os.makedirs(save_dir_path, exist_ok=True)\n",
    "\n",
    "# Iterate through all input tensors\n",
    "for idx, (input_tensor, orig_img_np) in enumerate(zip(image_tensors, original_images_np)):\n",
    "\n",
    "    # Model prediction\n",
    "    boxes, classes, labels, indices, scores = predict(input_tensor, model, device, iou_threshold)\n",
    "\n",
    "    # Add 0.2 to the scores for better visualization\n",
    "    scores = np.array(scores) - 0.02  # Convert scores to a NumPy array for element-wise addition\n",
    "\n",
    "    # Draw boxes on a copy of the original image\n",
    "    result_image = draw_boxes(boxes, labels, classes, scores, orig_img_np.copy())\n",
    "\n",
    "    # Save the result\n",
    "    save_path = os.path.join(save_dir_path, f'prediction_{idx}.jpg')\n",
    "    cv2.imwrite(save_path, cv2.cvtColor(result_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    print(f\"Saved: {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
